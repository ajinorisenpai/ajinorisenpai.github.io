{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"about Sakai Soichi \u60c5\u5831\u5b66\u79d1\u3067\u6df1\u5c64\u5b66\u7fd2\u306e\u7814\u7a76\u3092\u3057\u3066\u3044\u308b\u9662\u751f \u5b50\u4f9b\u5411\u3051\u306e\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\uff06\u96fb\u5b50\u5de5\u4f5c\u6559\u5ba4\u3092\u958b\u3044\u3066\u3044\u308b \u7af6\u6280\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u597d\u304d \u8208\u5473 \u30e1\u30c7\u30a3\u30a2\u30a2\u30fc\u30c8 (\u7279\u306b\u4eba\u5de5\u751f\u547d) \u30b2\u30fc\u30df\u30d5\u30a3\u30b1\u30fc\u30b7\u30e7\u30f3\u306b\u3088\u308b\u6559\u80b2 (\u7d44\u307f\u5408\u308f\u305b\u30b2\u30fc\u30e0\u7406\u8ad6\u306e\u6559\u80b2\u3078\u306e\u5229\u7528) \u30b3\u30f3\u30d4\u30e5\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u5168\u822c (\u7279\u306b\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3068\u30c7\u30fc\u30bf\u69cb\u9020\uff0c\u6a5f\u68b0\u5b66\u7fd2\uff0c\u4f4e\u30ec\u30a4\u30e4\u6280\u8853\u306a\u3069) \u5236\u4f5c\u7269 & \u30b9\u30ad\u30eb\u30bb\u30c3\u30c8 web\u77e5\u8b58 \u8679\u8272\u30aa\u30f3\u30e9\u30a4\u30f3\u7b97\u6570\u6559\u5ba4 \u5b50\u4f9b\u5411\u3051\u6559\u6750\u30b5\u30a4\u30c8\u306e\u5236\u4f5c\u3092\u4f01\u753b\uff0c\u5236\u4f5c\uff0c\u8ca9\u58f2\uff0e \u8a18\u4e8b\u306e\u57f7\u7b46\u306f \u672a\u6765\u5948\u7dd2\u7f8e\u5148\u751f \u306b\u884c\u3063\u3066\u3082\u3089\u3063\u305f\uff0e wordpress, bootstrap4, html5, css3, javascript, php \u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u529b \u5b66\u5185\u3067\u7af6\u6280\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u52c9\u5f37\u4f1a\u3092\u6bce\u9031\u6728\u66dc\u306b\u4e3b\u50ac\uff0cICPC\u3067NTT\u30b3\u30df\u30e5\u30cb\u30b1\u30fc\u30b7\u30e7\u30f3\u30ba\u8cde\u53d7\u8cde \u7af6\u6280\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u3067Atcoder\u6c34\u8272\u306e\u80fd\u529b \u8a08\u7b97\u91cf\u306b\u95a2\u3059\u308b\u611f\u899a\u304c\u4f53\u306b\u67d3\u307f\u3064\u3044\u3066\u304a\u308a\u3001\u8907\u96d1\u306a\u51e6\u7406\u3067\u3082\u82e6\u3082\u306a\u304f\u5b9f\u88c5\u51fa\u6765\u308b \u6df1\u3055\u512a\u5148\u63a2\u7d22\u3084\u5e45\u512a\u5148\u63a2\u7d22\u3001\u9806\u5217\u306e\u5168\u5217\u6319\u3084\u30d1\u30bf\u30fc\u30f3\u306e\u5168\u5217\u6319\u306a\u3069\u304c\u3067\u304d\u308b\u3002\u305d\u3053\u304b\u3089\u52d5\u7684\u8a08\u753b\u6cd5\u3084\u30e1\u30e2\u5316\u518d\u5e30\u306a\u3069\u306e\u8a08\u7b97\u91cf\u6539\u5584\u306b\u3064\u306a\u3052\u308b\u3053\u3068\u3082\u591a\u5c11\u51fa\u6765\u308b\u3002 \u8caa\u6b32\u30fbDP\u30fb\u3057\u3083\u304f\u3068\u308a\u6cd5\u30fb\u4e8c\u5206\u63a2\u7d22\u306a\u3069\u306e\u8a08\u7b97\u91cf\u3092\u6539\u5584\u3059\u308b\u30c6\u30af\u30cb\u30c3\u30af\u3092\u3042\u308b\u7a0b\u5ea6\u4f7f\u3044\u5206\u3051\u308b\u3053\u3068\u304c\u51fa\u6765\u308b\u3002 \u7d2f\u7a4d\u548c\u3084UnionFind(\u7af6\u30d7\u30ed\u5916\u3067\u306fDisjoint Set)\u306a\u3069\u306e\u30c7\u30fc\u30bf\u69cb\u9020\u3092\u4f7f\u3044\u3053\u306a\u3059\u3053\u3068\u304c\u51fa\u6765\u308b\u3002 \u30c0\u30a4\u30af\u30b9\u30c8\u30e9\u6cd5\u3084\u30ef\u30fc\u30b7\u30e3\u30eb\u30d5\u30ed\u30a4\u30c9\u6cd5\u3001\u30af\u30e9\u30b9\u30ab\u30eb\u6cd5\u306a\u3069\u306e\u3001\u57fa\u672c\u7684\u306a\u30b0\u30e9\u30d5\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u304c\u6271\u3048\u308b\u3002\u6728\u69cb\u9020\u3084\u30b0\u30e9\u30d5\u69cb\u9020\u306b\u5bfe\u3057\u3066\u9069\u5207\u306b\u51e6\u7406\u3092\u884c\u3046\u3053\u3068\u304c\u51fa\u6765\u308b\u3002 \u4e00\u5b9a\u4ee5\u4e0a\u306e\u6570\u5b66\u306b\u95a2\u3059\u308b\u7d20\u990a\u304c\u3042\u308b\u3002\u7d20\u6570\u306a\u3069\u306e\u6027\u8cea\u3084\u3001\u305d\u308c\u3092\u5229\u7528\u3057\u305f\u7d20\u6570\u5224\u5b9a\u3084\u5217\u6319\u3001\u7d04\u6570\u306e\u5217\u6319\u7b49\u3001\u6700\u5c0f\u516c\u500d\u6570\u3084\u6700\u5927\u516c\u7d04\u6570\u3001\u7d44\u307f\u5408\u308f\u305b\u306e\u8a08\u7b97\u306a\u3069\u3001\u7af6\u30d7\u30ed\u306b\u3042\u308a\u304c\u3061\u306a\u5178\u578b\u6570\u5b66\u554f\u984c\u306b\u5bfe\u51e6\u3067\u304d\u308b\u3002 \u76f4\u5927\u6c0f\u306e\u30d6\u30ed\u30b0 \u3088\u308a\u5f15\u7528 \u6a5f\u68b0\u5b66\u7fd2 \u6df1\u5c64\u5b66\u7fd2 CNN\u306e\u5b66\u7fd2\u904e\u7a0b\u306e\u53ef\u8996\u5316 pytorch, CNN, VAE, LSTM \u753b\u50cf\u8a8d\u8b58 \u6f2b\u753b\u753b\u50cf\u4e2d\u306e\u9854\u306e\u5411\u304d\u8a8d\u8b58 hog\u7279\u5fb4\u91cf, SVM \u81ea\u7136\u8a00\u8a9e\u51e6\u7406 \u30cd\u30c3\u30c8\u30aa\u30fc\u30af\u30b7\u30e7\u30f3\u306e\u5546\u54c1\u8aac\u660e\u6587\u304b\u3089\u4fa1\u683c\u4e88\u6e2c mercari-price-suggestion-challenge TF-IDF, \u30c8\u30d4\u30c3\u30af\u30e2\u30c7\u30eb(LDA) \u30bd\u30d5\u30c8\u30a6\u30a7\u30a2\u5236\u4f5c \u30d7\u30ed\u30d5\u30a3\u30fc\u30eb\u4ea4\u63db\u30a2\u30d7\u30ea HACK U 2017 4\u4eba\u30c1\u30fc\u30e0\u3067\u5236\u4f5c\uff0c\u30d5\u30ed\u30f3\u30c8\u30a8\u30f3\u30c9\u5168\u822c\u3068\u7aef\u672b\u306e\u885d\u7a81\u8a8d\u8b58\u3092\u62c5\u5f53 Swift, Unity, C# \u30cf\u30ed\u30a6\u30a3\u30f3 \u30d0\u30fc\u30c1\u30e3\u30eb\u4eee\u88c5 kinect\u3092\u4f7f\u3063\u305f\u4eee\u88c5\u4f53\u9a13\u304c\u3067\u304d\u308b\u4f5c\u54c1\uff0e \u6885\u7530Loft\u3067\u5c55\u793a\uff0c\u4e8c\u4eba\u30c1\u30fc\u30e0\u3067\u4f01\u753b\u3068\u5b9f\u88c5\u306b\u5206\u304b\u308c\u5b9f\u88c5\u5168\u822c\u62c5\u5f53 Processing, Kinect v2 \u4f4e\u30ec\u30a4\u30e4\u77e5\u8b58 \u30b3\u30f3\u30d1\u30a4\u30e9\u5236\u4f5c c, x86, docker SDL2.0\u3067\u30b2\u30fc\u30e0\u5236\u4f5c C++, SDL2.0, OpenGL, GLSL CPU\u5236\u4f5c CPU\u306e\u5275\u308a\u65b9\u3092\u53c2\u8003\u306b\uff0c\u6c4e\u7528\u30ed\u30b8\u30c3\u30afIC\u30674bitCPU\u5236\u4f5c \u96fb\u5b50\u5de5\u4f5c\u6559\u5ba4 Arduino \u8da3\u5473 \u4f5c\u66f2 Ableton(DTM\u30bd\u30d5\u30c8), \u30a2\u30ca\u30ed\u30b0\u30b7\u30f3\u30bb, \u30b8\u30e3\u30ba\u7406\u8ad6, \u30d4\u30a2\u30ce \u9023\u7d61\u5148","title":"about"},{"location":"#about","text":"Sakai Soichi \u60c5\u5831\u5b66\u79d1\u3067\u6df1\u5c64\u5b66\u7fd2\u306e\u7814\u7a76\u3092\u3057\u3066\u3044\u308b\u9662\u751f \u5b50\u4f9b\u5411\u3051\u306e\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\uff06\u96fb\u5b50\u5de5\u4f5c\u6559\u5ba4\u3092\u958b\u3044\u3066\u3044\u308b \u7af6\u6280\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u597d\u304d","title":"about"},{"location":"#_1","text":"\u30e1\u30c7\u30a3\u30a2\u30a2\u30fc\u30c8 (\u7279\u306b\u4eba\u5de5\u751f\u547d) \u30b2\u30fc\u30df\u30d5\u30a3\u30b1\u30fc\u30b7\u30e7\u30f3\u306b\u3088\u308b\u6559\u80b2 (\u7d44\u307f\u5408\u308f\u305b\u30b2\u30fc\u30e0\u7406\u8ad6\u306e\u6559\u80b2\u3078\u306e\u5229\u7528) \u30b3\u30f3\u30d4\u30e5\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u5168\u822c (\u7279\u306b\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3068\u30c7\u30fc\u30bf\u69cb\u9020\uff0c\u6a5f\u68b0\u5b66\u7fd2\uff0c\u4f4e\u30ec\u30a4\u30e4\u6280\u8853\u306a\u3069)","title":"\u8208\u5473"},{"location":"#_2","text":"","title":"\u5236\u4f5c\u7269 &amp; \u30b9\u30ad\u30eb\u30bb\u30c3\u30c8"},{"location":"#web","text":"","title":"web\u77e5\u8b58"},{"location":"#_3","text":"\u5b50\u4f9b\u5411\u3051\u6559\u6750\u30b5\u30a4\u30c8\u306e\u5236\u4f5c\u3092\u4f01\u753b\uff0c\u5236\u4f5c\uff0c\u8ca9\u58f2\uff0e \u8a18\u4e8b\u306e\u57f7\u7b46\u306f \u672a\u6765\u5948\u7dd2\u7f8e\u5148\u751f \u306b\u884c\u3063\u3066\u3082\u3089\u3063\u305f\uff0e wordpress, bootstrap4, html5, css3, javascript, php","title":"\u8679\u8272\u30aa\u30f3\u30e9\u30a4\u30f3\u7b97\u6570\u6559\u5ba4"},{"location":"#_4","text":"\u5b66\u5185\u3067\u7af6\u6280\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u52c9\u5f37\u4f1a\u3092\u6bce\u9031\u6728\u66dc\u306b\u4e3b\u50ac\uff0cICPC\u3067NTT\u30b3\u30df\u30e5\u30cb\u30b1\u30fc\u30b7\u30e7\u30f3\u30ba\u8cde\u53d7\u8cde \u7af6\u6280\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u3067Atcoder\u6c34\u8272\u306e\u80fd\u529b \u8a08\u7b97\u91cf\u306b\u95a2\u3059\u308b\u611f\u899a\u304c\u4f53\u306b\u67d3\u307f\u3064\u3044\u3066\u304a\u308a\u3001\u8907\u96d1\u306a\u51e6\u7406\u3067\u3082\u82e6\u3082\u306a\u304f\u5b9f\u88c5\u51fa\u6765\u308b \u6df1\u3055\u512a\u5148\u63a2\u7d22\u3084\u5e45\u512a\u5148\u63a2\u7d22\u3001\u9806\u5217\u306e\u5168\u5217\u6319\u3084\u30d1\u30bf\u30fc\u30f3\u306e\u5168\u5217\u6319\u306a\u3069\u304c\u3067\u304d\u308b\u3002\u305d\u3053\u304b\u3089\u52d5\u7684\u8a08\u753b\u6cd5\u3084\u30e1\u30e2\u5316\u518d\u5e30\u306a\u3069\u306e\u8a08\u7b97\u91cf\u6539\u5584\u306b\u3064\u306a\u3052\u308b\u3053\u3068\u3082\u591a\u5c11\u51fa\u6765\u308b\u3002 \u8caa\u6b32\u30fbDP\u30fb\u3057\u3083\u304f\u3068\u308a\u6cd5\u30fb\u4e8c\u5206\u63a2\u7d22\u306a\u3069\u306e\u8a08\u7b97\u91cf\u3092\u6539\u5584\u3059\u308b\u30c6\u30af\u30cb\u30c3\u30af\u3092\u3042\u308b\u7a0b\u5ea6\u4f7f\u3044\u5206\u3051\u308b\u3053\u3068\u304c\u51fa\u6765\u308b\u3002 \u7d2f\u7a4d\u548c\u3084UnionFind(\u7af6\u30d7\u30ed\u5916\u3067\u306fDisjoint Set)\u306a\u3069\u306e\u30c7\u30fc\u30bf\u69cb\u9020\u3092\u4f7f\u3044\u3053\u306a\u3059\u3053\u3068\u304c\u51fa\u6765\u308b\u3002 \u30c0\u30a4\u30af\u30b9\u30c8\u30e9\u6cd5\u3084\u30ef\u30fc\u30b7\u30e3\u30eb\u30d5\u30ed\u30a4\u30c9\u6cd5\u3001\u30af\u30e9\u30b9\u30ab\u30eb\u6cd5\u306a\u3069\u306e\u3001\u57fa\u672c\u7684\u306a\u30b0\u30e9\u30d5\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u304c\u6271\u3048\u308b\u3002\u6728\u69cb\u9020\u3084\u30b0\u30e9\u30d5\u69cb\u9020\u306b\u5bfe\u3057\u3066\u9069\u5207\u306b\u51e6\u7406\u3092\u884c\u3046\u3053\u3068\u304c\u51fa\u6765\u308b\u3002 \u4e00\u5b9a\u4ee5\u4e0a\u306e\u6570\u5b66\u306b\u95a2\u3059\u308b\u7d20\u990a\u304c\u3042\u308b\u3002\u7d20\u6570\u306a\u3069\u306e\u6027\u8cea\u3084\u3001\u305d\u308c\u3092\u5229\u7528\u3057\u305f\u7d20\u6570\u5224\u5b9a\u3084\u5217\u6319\u3001\u7d04\u6570\u306e\u5217\u6319\u7b49\u3001\u6700\u5c0f\u516c\u500d\u6570\u3084\u6700\u5927\u516c\u7d04\u6570\u3001\u7d44\u307f\u5408\u308f\u305b\u306e\u8a08\u7b97\u306a\u3069\u3001\u7af6\u30d7\u30ed\u306b\u3042\u308a\u304c\u3061\u306a\u5178\u578b\u6570\u5b66\u554f\u984c\u306b\u5bfe\u51e6\u3067\u304d\u308b\u3002 \u76f4\u5927\u6c0f\u306e\u30d6\u30ed\u30b0 \u3088\u308a\u5f15\u7528","title":"\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u529b"},{"location":"#_5","text":"","title":"\u6a5f\u68b0\u5b66\u7fd2"},{"location":"#_6","text":"","title":"\u6df1\u5c64\u5b66\u7fd2"},{"location":"#cnn","text":"pytorch, CNN, VAE, LSTM","title":"CNN\u306e\u5b66\u7fd2\u904e\u7a0b\u306e\u53ef\u8996\u5316"},{"location":"#_7","text":"","title":"\u753b\u50cf\u8a8d\u8b58"},{"location":"#_8","text":"hog\u7279\u5fb4\u91cf, SVM","title":"\u6f2b\u753b\u753b\u50cf\u4e2d\u306e\u9854\u306e\u5411\u304d\u8a8d\u8b58"},{"location":"#_9","text":"","title":"\u81ea\u7136\u8a00\u8a9e\u51e6\u7406"},{"location":"#mercari-price-suggestion-challenge","text":"TF-IDF, \u30c8\u30d4\u30c3\u30af\u30e2\u30c7\u30eb(LDA)","title":"\u30cd\u30c3\u30c8\u30aa\u30fc\u30af\u30b7\u30e7\u30f3\u306e\u5546\u54c1\u8aac\u660e\u6587\u304b\u3089\u4fa1\u683c\u4e88\u6e2c mercari-price-suggestion-challenge"},{"location":"#_10","text":"","title":"\u30bd\u30d5\u30c8\u30a6\u30a7\u30a2\u5236\u4f5c"},{"location":"#hack-u-2017","text":"4\u4eba\u30c1\u30fc\u30e0\u3067\u5236\u4f5c\uff0c\u30d5\u30ed\u30f3\u30c8\u30a8\u30f3\u30c9\u5168\u822c\u3068\u7aef\u672b\u306e\u885d\u7a81\u8a8d\u8b58\u3092\u62c5\u5f53 Swift, Unity, C#","title":"\u30d7\u30ed\u30d5\u30a3\u30fc\u30eb\u4ea4\u63db\u30a2\u30d7\u30ea HACK U 2017"},{"location":"#_11","text":"kinect\u3092\u4f7f\u3063\u305f\u4eee\u88c5\u4f53\u9a13\u304c\u3067\u304d\u308b\u4f5c\u54c1\uff0e \u6885\u7530Loft\u3067\u5c55\u793a\uff0c\u4e8c\u4eba\u30c1\u30fc\u30e0\u3067\u4f01\u753b\u3068\u5b9f\u88c5\u306b\u5206\u304b\u308c\u5b9f\u88c5\u5168\u822c\u62c5\u5f53 Processing, Kinect v2","title":"\u30cf\u30ed\u30a6\u30a3\u30f3 \u30d0\u30fc\u30c1\u30e3\u30eb\u4eee\u88c5"},{"location":"#_12","text":"","title":"\u4f4e\u30ec\u30a4\u30e4\u77e5\u8b58"},{"location":"#_13","text":"c, x86, docker","title":"\u30b3\u30f3\u30d1\u30a4\u30e9\u5236\u4f5c"},{"location":"#sdl20","text":"C++, SDL2.0, OpenGL, GLSL","title":"SDL2.0\u3067\u30b2\u30fc\u30e0\u5236\u4f5c"},{"location":"#cpu","text":"CPU\u306e\u5275\u308a\u65b9\u3092\u53c2\u8003\u306b\uff0c\u6c4e\u7528\u30ed\u30b8\u30c3\u30afIC\u30674bitCPU\u5236\u4f5c","title":"CPU\u5236\u4f5c"},{"location":"#_14","text":"Arduino","title":"\u96fb\u5b50\u5de5\u4f5c\u6559\u5ba4"},{"location":"#_15","text":"","title":"\u8da3\u5473"},{"location":"#_16","text":"Ableton(DTM\u30bd\u30d5\u30c8), \u30a2\u30ca\u30ed\u30b0\u30b7\u30f3\u30bb, \u30b8\u30e3\u30ba\u7406\u8ad6, \u30d4\u30a2\u30ce","title":"\u4f5c\u66f2"},{"location":"#_17","text":"","title":"\u9023\u7d61\u5148"},{"location":"DL/survey/Visual_Interpretability/","text":"Visual_Interpretability VisualInterpretability for Deep Learning: a Survey Quan-shi ZHANG, Song-chun ZHU Abstract This paper reviews recent studies in understanding neural-network representations and learning neural networks with interpretable/disentangled middlelayer representations. Although deep neural networks have exhibited superior performance in various tasks, the interpretability is always the Achilles\u2019 heel of deep neural networks. \u672c\u7a3f\u3067\u306f\u3001\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u8868\u73fe\u3092\u7406\u89e3\u3059\u308b\u305f\u3081\u306e\u6700\u8fd1\u306e\u7814\u7a76\u3092\u89e3\u8aac\u3059\u308b\uff0e\u307e\u305f\uff0c\u89e3\u91c8\u53ef\u80fd\u306a\u7d61\u307e\u3063\u3066\u3044\u306a\u3044\u4e2d\u5c64\u8868\u73fe\u3092\u7528\u3044\u3066\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u5b66\u3076\uff0e \u30c7\u30a3\u30fc\u30d7\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306f\u69d8\u3005\u306a\u30bf\u30b9\u30af\u306b\u304a\u3044\u3066\u512a\u308c\u305f\u6027\u80fd\u3092\u767a\u63ee\u3057\u305f\u304c\u3001\u89e3\u91c8\u6027\u306f\u5e38\u306b\u30c7\u30a3\u30fc\u30d7\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u5f31\u70b9\u3067\u3042\u308b\u3002 At present, deep neural networks obtain high discrimination power at the cost of low interpretability of their black-box representations. \u73fe\u5728\u3001\u30c7\u30a3\u30fc\u30d7\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306f\u3001\u30d6\u30e9\u30c3\u30af\u30dc\u30c3\u30af\u30b9\u306a\u8868\u73fe\u304b\u3089\u304f\u308b\u89e3\u91c8\u6027\u306e\u4f4e\u3055\u3092\u72a0\u7272\u306b\u3057\u3066\u3001\u9ad8\u3044\u8b58\u5225\u529b\u3092\u5f97\u3066\u3044\u308b\u3002 We believe that high model interpretability may help people to break several bottlenecks of deep learning, e.g. learning from very few annotations, learning via human-computer communications at the semantic level, and semantically debugging network representations. \u79c1\u305f\u3061\u306f\u89e3\u91c8\u80fd\u529b\u306e\u9ad8\u3044\u30e2\u30c7\u30eb\u306f\u3001\u4eba\u3005\u304c\u6df1\u3044\u5b66\u7fd2\u306e\u3044\u304f\u3064\u304b\u306e\u30dc\u30c8\u30eb\u30cd\u30c3\u30af\u3092\u89e3\u6d88\u3059\u308b\u306e\u306b\u5f79\u7acb\u3064\u304b\u3082\u3057\u308c\u306a\u3044\u3068\u8003\u3048\u3066\u3044\u308b\u3002 \u975e\u5e38\u306b\u5c11\u6570\u306e\u6ce8\u91c8\u304b\u3089\u306e\u5b66\u7fd2\u3001\u610f\u5473\u30ec\u30d9\u30eb\u3067\u306e\u4eba\u9593\u3068\u6a5f\u68b0\u306e\u4f1a\u8a71\u304b\u3089\u306e\u5b66\u7fd2\u3001\u304a\u3088\u3073\u610f\u5473\u8ad6\u7684\u306a\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u8868\u73fe\u306e\u30c7\u30d0\u30c3\u30b0\u3002 We focus on convolutional neural networks (CNNs), and we revisit the visualization of CNN representations, methods of diagnosing representations of pre-trained CNNs, approaches for disentangling pre-trained CNN representations, learning of CNNs with disentangled representations, and middle-to-end learning based on model interpretability. Finally, we discuss prospective trends in explainable artificial intelligence. \u6211\u3005\u306f\u3001\u7573\u307f\u8fbc\u307f\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\uff08CNN\uff09\u306b\u7126\u70b9\u3092\u5f53\u3066\u3001CNN\u8868\u73fe\u306e\u8996\u899a\u5316\u3001\u4e8b\u524d\u306b\u8a13\u7df4\u3055\u308c\u305fCNN\u306e\u8868\u73fe\u3092\u8a3a\u65ad\u3059\u308b\u65b9\u6cd5\u3001\u4e8b\u524d\u306b\u8a13\u7df4\u3055\u308c\u305fCNN\u8868\u73fe\u306e\u89e3\u304d\u65b9\u3001\u5206\u89e3\u8868\u73fe\u3092\u6301\u3064CNN\u306e\u5b66\u7fd2\u3001 \u30e2\u30c7\u30eb\u306e\u89e3\u91c8\u80fd\u529b\u306b\u57fa\u3065\u3044\u3066\u5b66\u7fd2\u3092\u7d42\u4e86\u3059\u308b\u3002 \u6700\u5f8c\u306b\u3001\u8aac\u660e\u53ef\u80fd\u306a\u4eba\u5de5\u77e5\u80fd\u306e\u5c06\u6765\u306e\u52d5\u5411\u306b\u3064\u3044\u3066\u8b70\u8ad6\u3059\u308b\u3002 Introduction Convolutional neural networks (CNNs) [LeCun et al., 1998a; Krizhevsky et al., 2012; He et al., 2016; Huang et al., 2017] have achieved superior performance in many visual tasks, such as object classification and detection. However, the endto-end learning strategy makes CNN representations a black box. Except for the final network output, it is difficult for people to understand the logic of CNN predictions hidden inside the network. \u7573\u307f\u8fbc\u307f\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\uff08CNN\uff09[LeCun\u3089\u30011998a; Krizhevsky\u3089\u30012012; He\u3089\u30012016; Huang\u3089\u30012017]\u306f\u3001\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u306e\u5206\u985e\u3084\u691c\u51fa\u306a\u3069\u3001\u591a\u304f\u306e\u30d3\u30b8\u30e5\u30a2\u30eb\u30bf\u30b9\u30af\u3067\u512a\u308c\u305f\u30d1\u30d5\u30a9\u30fc\u30de\u30f3\u30b9\u3092\u9054\u6210\u3057\u3066\u3044\u307e\u3059\u3002 \u3057\u304b\u3057\u306a\u304c\u3089\uff0cend-to-end\u306e\u5b66\u7fd2\u6226\u7565\u306f\u3001CNN\u306e\u8868\u73fe\u3092\u30d6\u30e9\u30c3\u30af\u30dc\u30c3\u30af\u30b9\u5316\u3057\u307e\u3059\uff0e \u6700\u7d42\u7684\u306a\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u51fa\u529b\u3092\u9664\u3044\u3066\u3001\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u5185\u306b\u96a0\u308c\u3066\u3044\u308bCNN\u306e\u4e88\u6e2c\u306e\u30ed\u30b8\u30c3\u30af\u3092\u4eba\u3005\u304c\u7406\u89e3\u3059\u308b\u3053\u3068\u306f\u56f0\u96e3\u3067\u3059\u3002 LeCun Y, Bottou L, Bengio Y, et al., 1998a. Gradient-based learning applied to document recognition. Proc IEEE, 86(11):2278-2324. https://doi.org/10.1109/5.726791 Krizhevsky A, Sutskever I, Hinton GE, 2012. Imagenet classification with deep convolutional neural networks. NIPS, p.1097-1105. He K, Zhang X, Ren S, et al., 2016. Deep residual learning for image recognition. Proc IEEE Conf on Computer Vision and Pattern Recognition, p.770-778. Huang G, Liu Z, Weinberger KQ, et al., 2017. Densely connected convolutional networks. Proc IEEE Conf on Computer Vision and Pattern Recognition, p.4700-4708. In recent years, a growing number of researchers have realized that high model interpretability is of significant value in both theory and practice and have developed models with interpretable knowledge representations. \u8fd1\u5e74\u3001\u591a\u304f\u306e\u7814\u7a76\u8005\u304c\u9ad8\u3044\u30e2\u30c7\u30eb\u306e\u89e3\u91c8\u80fd\u529b\u306f\u7406\u8ad6\u3068\u5b9f\u8df5\u306e\u4e21\u9762\u3067\u91cd\u8981\u306a\u4fa1\u5024\u3092\u6301\u3064\u3053\u3068\u3092\u7406\u89e3\u3057\uff0c\u89e3\u91c8\u53ef\u80fd\u306a\u77e5\u8b58\u8868\u73fe\u3092\u6301\u3064\u30e2\u30c7\u30eb\u3092\u958b\u767a\u3057\u3066\u3044\u307e\u3059\uff0e In this paper, we conduct a survey of current studies in understanding neural-network representations and learning neural networks with interpretable/disentangled representations. We can roughly define the scope of the review into the following six research directions. \u672c\u7a3f\u3067\u306f\u3001\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u8868\u73fe\u3092\u7406\u89e3\u3059\u308b\u305f\u3081\u306e\u73fe\u72b6\u306e\u7814\u7a76\u3068\u3001\u89e3\u91c8\u53ef\u80fd/\u89e3\u304d\u307b\u3050\u3055\u308c\u305f\u8868\u73fe\u3092\u7528\u3044\u305f\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u5b66\u7fd2\u306b\u3064\u3044\u3066\u8abf\u67fb\u3059\u308b\u3002 \u79c1\u305f\u3061\u306f\u30ec\u30d3\u30e5\u30fc\u306e\u7bc4\u56f2\u3092\u5927\u307e\u304b\u306b\u4ee5\u4e0b\u306e6\u3064\u306e\u7814\u7a76\u65b9\u5411\u306b\u5b9a\u7fa9\u3057\u307e\u3057\u305f\u3002 Visualization of CNN representations in intermediate network layers. These methods mainly synthesize the image that maximizes the score of a given unit in a pretrained CNN or invert feature maps of a conv-layer back to the input image. Please see Section 2 for detailed discussion. \u4e2d\u9593\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u5c64\u306b\u304a\u3051\u308bCNN\u8868\u73fe\u306e\u8996\u899a\u5316 \u3053\u308c\u3089\u306e\u65b9\u6cd5\u306f\u3001\u4e3b\u306b\u3001\u4e88\u3081\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u3055\u308c\u305fCNN\u5185\u306e\u6240\u4e0e\u306e\u30e6\u30cb\u30c3\u30c8\u306e\u30b9\u30b3\u30a2\u3092\u6700\u5927\u5316\u3059\u308b\u753b\u50cf\u3092\u5408\u6210\u3059\u308b\u304b\u3001\u307e\u305f\u306fconv-layer\u306e\u7279\u5fb4\u30de\u30c3\u30d7\u3092\u5165\u529b\u753b\u50cf\u307e\u3067\u623b\u3057\u307e\u3059\u3002 \u8a73\u7d30\u306a\u8aac\u660e\u306f\u30bb\u30af\u30b7\u30e7\u30f32\u3092\u53c2\u7167\u3057\u3066\u304f\u3060\u3055\u3044\u3002 > 2. Diagnosis of CNN representations. Related studies may either diagnose a CNN\u2019s feature space for different object categories or discover potential representation flaws in conv-layers. Please see Section 3 for details. CNN\u8868\u73fe\u306e\u8a3a\u65ad\uff0e \u95a2\u9023\u3059\u308b\u7814\u7a76\u3067\u306f\u3001\u7570\u306a\u308b\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u30ab\u30c6\u30b4\u30ea\u306eCNN\u306e\u7279\u5fb4\u7a7a\u9593\u3092\u8a3a\u65ad\u3059\u308b\u304b\u3001conv-layers\u306e\u6f5c\u5728\u7684\u306a\u8868\u73fe\u6b20\u9665\u3092\u767a\u898b\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u3002 \u8a73\u7d30\u306f\u30bb\u30af\u30b7\u30e7\u30f33\u3092\u53c2\u7167\u3057\u3066\u304f\u3060\u3055\u3044\u3002 Disentanglement of \u201cthe mixture of patterns\u201d encoded in each filter of CNNs. These studies mainly disentangle complex representations in conv-layers and transform network representations into interpretable graphs. Please see Section 4 for details. CNN\u306e\u5404\u30d5\u30a3\u30eb\u30bf\u3067\u7b26\u53f7\u5316\u3055\u308c\u305f \"\u30d1\u30bf\u30fc\u30f3\u306e\u6df7\u5408\"\u3092\u7d10\u89e3\u304f. \u3053\u308c\u3089\u306e\u7814\u7a76\u306f\u4e3b\u306bconv-layer\u306e\u8907\u96d1\u306a\u8868\u73fe\u3092\u89e3\u304d\u3001\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u8868\u73fe\u3092\u89e3\u91c8\u53ef\u80fd\u306a\u30b0\u30e9\u30d5\u306b\u5909\u63db\u3059\u308b\u3002 \u8a73\u7d30\u306f\u30bb\u30af\u30b7\u30e7\u30f34\u3092\u53c2\u7167\u3057\u3066\u304f\u3060\u3055\u3044\u3002 Building explainable models. We discuss interpretable CNNs [Zhang et al., 2017c], capsule networks [Sabour et al., 2017], interpretable R-CNNs [Wu et al., 2017], and the InfoGAN [Chen et al., 2016] in Section 5. \u8aac\u660e\u53ef\u80fd\u306a\u30e2\u30c7\u30eb\u3092\u69cb\u7bc9\u3059\u308b\u3002 \u6211\u3005\u306f\u3001\u89e3\u91c8\u53ef\u80fd\u306aCNN [Zhang\u3089\u30012017c]\u3001\u30ab\u30d7\u30bb\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af[Sabour\u3089\u30012017]\u3001\u89e3\u91c8\u53ef\u80fd\u306aR-CNN [Wu\u3089\u30012017]\u3001\u304a\u3088\u3073InfoGAN [Chen et al\u3002\u30012016] Semantic-level middle-to-end learning via humancomputer interaction. A clear semantic disentanglement of CNN representations may further enable \u201cmiddle-toend\u201d learning of neural networks with weak supervision. Section 7 introduces methods to learn new models via human-computer interactions [Zhang et al., 2017b] and active question-answering with very limited human supervision [Zhang et al., 2017a]. \u4eba\u9593\u3068\u30b3\u30f3\u30d4\u30e5\u30fc\u30bf\u3068\u306e\u30a4\u30f3\u30bf\u30e9\u30af\u30b7\u30e7\u30f3\u306b\u3088\u308b\u610f\u5473\u30ec\u30d9\u30eb\u306e\u4e2d\u9593\u7684\u306a\u5b66\u7fd2\u3002 CNN\u8868\u73fe\u306e\u660e\u78ba\u306a\u610f\u5473\u89e3\u6790\u306f\u3001\u5f31\u3044\u76e3\u7763\u4e0b\u3067\u306e\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u300c\u4e2d\u9593\u304b\u3089\u5148\u3078\u306e\u300d\u5b66\u7fd2\u3092\u3055\u3089\u306b\u53ef\u80fd\u306b\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u3002 \u7b2c7\u7bc0\u3067\u306f\u3001\u4eba\u9593\u3068\u30b3\u30f3\u30d4\u30e5\u30fc\u30bf\u306e\u76f8\u4e92\u4f5c\u7528[Zhang et al\u3002\u30012017b]\u3092\u4ecb\u3057\u3066\u65b0\u3057\u3044\u30e2\u30c7\u30eb\u3092\u5b66\u7fd2\u3059\u308b\u65b9\u6cd5\u3068\u3001\u975e\u5e38\u306b\u9650\u5b9a\u3055\u308c\u305f\u4eba\u9593\u306e\u76e3\u7763[Zhang et al\u3002\u30012017a]\u3092\u7528\u3044\u305f\u80fd\u52d5\u7684\u8cea\u554f\u5fdc\u7b54\u65b9\u6cd5\u3092\u7d39\u4ecb\u3059\u308b\u3002 Among all the above, the visualization of CNN representations is the most direct way to explore network representations. The network visualization also provides a technical foundation for many approaches to diagnosing CNN representations. \u4e0a\u8a18\u306e\u3059\u3079\u3066\u306e\u4e2d\u3067\u3001CNN\u8868\u73fe\u306e\u8996\u899a\u5316\u306f\u3001\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u8868\u73fe\u3092\u63a2\u7d22\u3059\u308b\u6700\u3082\u76f4\u63a5\u7684\u306a\u65b9\u6cd5\u3067\u3059\u3002 \u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u8996\u899a\u5316\u306f\u307e\u305f\u3001CNN\u8868\u73fe\u3092\u8a3a\u65ad\u3059\u308b\u305f\u3081\u306e\u591a\u304f\u306e\u30a2\u30d7\u30ed\u30fc\u30c1\u306e\u305f\u3081\u306e\u6280\u8853\u7684\u57fa\u790e\u3092\u63d0\u4f9b\u3059\u308b\u3002 The disentanglement of feature representations of a pre-trained CNN and the learning of explainable network representations present bigger challenges to state-of-the-art algorithms. \u4e8b\u524d\u306b\u8a13\u7df4\u3055\u308c\u305fCNN\u306e\u7279\u5fb4\u8868\u73fe\u306e\u89e3\u6790\uff0c\u304a\u3088\u3073\u8aac\u660e\u53ef\u80fd\u306a\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u8868\u73fe\u306e\u5b66\u7fd2\u306f\u3001\u6700\u5148\u7aef\u306e\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306b\u5bfe\u3059\u308b\u3088\u308a\u5927\u304d\u306a\u8ab2\u984c\u3092\u63d0\u793a\u3059\u308b\u3002 Finally, explainable or disentangled network representations are also the starting point for weakly-supervised middle-to-end learning. Values of model interpretability: The clear semantics in high conv-layers can help people trust a network\u2019s prediction. \u6700\u5f8c\u306b\u3001\u8aac\u660e\u53ef\u80fd\u306a\u307e\u305f\u306f\u89e3\u6790\u3055\u308c\u305f\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u8868\u73fe\u306f\u3001\u5f31\u304f\u76e3\u7763\u3055\u308c\u308b\u4e2d\u9593\u304b\u3089\u7d42\u308f\u308a\u307e\u3067\u306e\u5b66\u7fd2\u306e\u51fa\u767a\u70b9\u3067\u3082\u3042\u308a\u307e\u3059\u3002 \u30e2\u30c7\u30eb\u306e\u89e3\u91c8\u80fd\u529b\u306e\u30ec\u30d9\u30eb\uff1ahigh conv-layers\u3067\u306e\u660e\u78ba\u306a\u610f\u5473\u306f\u3001\u4eba\u3005\u304c\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u4e88\u6e2c\u3092\u4fe1\u983c\u3059\u308b\u306e\u306b\u5f79\u7acb\u3061\u307e\u3059\u3002 As discussed in [Zhang et al., 2018b], considering dataset and representation bias, a high accuracy on testing images still cannot ensure that a CNN will encode correct representations. For example, a CNN may use an unreliable context\u2014eye features\u2014to identify the \u201clipstick\u201d attribute of a face image. [Zhang et al\u3002\u30012018b]\u3067\u8ad6\u3058\u3089\u308c\u3066\u3044\u308b\u3088\u3046\u306b\u3001\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3068\u8868\u73fe\u30d0\u30a4\u30a2\u30b9\u3092\u8003\u616e\u3059\u308b\u3068\u3001\u30c6\u30b9\u30c8\u753b\u50cf\u306e\u7cbe\u5ea6\u304c\u9ad8\u3044\u3053\u3068\u306f\u3001CNN\u304c\u6b63\u3057\u3044\u8868\u73fe\u3092\u7b26\u53f7\u5316\u3059\u308b\u3053\u3068\u3092\u4fdd\u8a3c\u3067\u304d\u307e\u305b\u3093\u3002 \u4f8b\u3048\u3070\u3001CNN\u306f\u3001\u9854\u753b\u50cf\u306e\u300c\u53e3\u7d05\u300d\u5c5e\u6027\u3092\u8b58\u5225\u3059\u308b\u305f\u3081\u306b\uff0c\u4fe1\u983c\u3067\u304d\u306a\u3044 context-eye features-to \u3092\u4f7f\u7528\u3059\u308b\u3060\u308d\u3046\uff0e Zhang Q, Cao R, Shi F, et al., 2018b. Interpreting CNN knowledge via an explanatory graph. Proc 32nd AAAI Conf on Arti\ufb01cial Intelligence, p.2124-2132. Therefore, people usually cannot fully trust a network unless a CNN can semantically or visually explain its logic, e.g. what patterns are used for prediction. \u3057\u305f\u304c\u3063\u3066\u3001CNN\u304c\u305d\u306e\u8ad6\u7406\u3092\u610f\u5473\u7684\u306b\u307e\u305f\u306f\u8996\u899a\u7684\u306b\u8aac\u660e\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u306a\u3044\u9650\u308a\u3001\u4eba\u3005\u306f\u901a\u5e38\u3001\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u5b8c\u5168\u306b\u4fe1\u983c\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u306a\u3044\u3002\u4f8b\u3048\u3070\u3069\u306e\u30d1\u30bf\u30fc\u30f3\u304c\u4e88\u6e2c\u306b\u4f7f\u7528\u3055\u308c\u308b\u304b\u3082\uff0e In addition, the middle-to-end learning or debugging of neural networks based on the explainable or disentangled network representations may also significantly reduce the requirement for human annotation. Furthermore, based on semantic representations of networks, it is possible to merge multiple CNNs into a universal network (i.e. a network encoding generic knowledge representations for different tasks) at the semantic level in the future. \u3055\u3089\u306b\u3001\u8aac\u660e\u53ef\u80fd\u306a\u307e\u305f\u306f\u7d10\u89e3\u304b\u308c\u305f\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u8868\u73fe\u306b\u57fa\u3065\u3044\u305f\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306emiddle-to-end\u306e\u5b66\u7fd2\u307e\u305f\u306f\u30c7\u30d0\u30c3\u30b0\u306f\u3001\u4eba\u9593\u306b\u3088\u308b\u30a2\u30ce\u30c6\u30fc\u30b7\u30e7\u30f3\u306e\u5fc5\u8981\u3092\u5927\u5e45\u306b\u524a\u6e1b\u3059\u308b\u53ef\u80fd\u6027\u304c\u3042\u308a\u307e\u3059\u3002 \u3055\u3089\u306b\u3001\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u610f\u5473\u8868\u73fe\u306b\u57fa\u3065\u3044\u3066\u3001\u5c06\u6765\u306e\u610f\u5473\u30ec\u30d9\u30eb\u3067\u8907\u6570\u306eCNN\u3092\u30e6\u30cb\u30d0\u30fc\u30b5\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\uff08\u3059\u306a\u308f\u3061\u3001\u7570\u306a\u308b\u30bf\u30b9\u30af\u306e\u305f\u3081\u306e\u6c4e\u7528\u77e5\u8b58\u8868\u73fe\u3092\u7b26\u53f7\u5316\u3059\u308b\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\uff09\u306b\u30de\u30fc\u30b8\u3059\u308b\u3053\u3068\u304c\u53ef\u80fd\u3067\u3042\u308b\u3002 In the following sections, we review the above research directions and discuss the potential future of technical developments. \u4ee5\u4e0b\u306e\u30bb\u30af\u30b7\u30e7\u30f3\u3067\u306f\u3001\u4e0a\u8a18\u306e\u7814\u7a76\u306e\u65b9\u5411\u6027\u3092\u898b\u76f4\u3057\u3001\u5c06\u6765\u306e\u6280\u8853\u958b\u767a\u306e\u5c06\u6765\u306b\u3064\u3044\u3066\u8b70\u8ad6\u3059\u308b\u3002 2 Visualization of CNN representations Visualization of filters in a CNN is the most direct way of exploring visual patterns hidden inside a neural unit. Different types of visualization methods have been developed for network visualization. CNN\u306b\u304a\u3051\u308b\u30d5\u30a3\u30eb\u30bf\u306e\u8996\u899a\u5316\u306f\u3001\u795e\u7d4c\u30e6\u30cb\u30c3\u30c8\u5185\u306b\u96a0\u3055\u308c\u305f\u8996\u899a\u30d1\u30bf\u30fc\u30f3\u3092\u63a2\u7d22\u3059\u308b\u6700\u3082\u76f4\u63a5\u7684\u306a\u65b9\u6cd5\u3067\u3042\u308b\u3002 \u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u8996\u899a\u5316\u306e\u305f\u3081\u306e\u7570\u306a\u308b\u30bf\u30a4\u30d7\u306e\u8996\u899a\u5316\u65b9\u6cd5\u304c\u958b\u767a\u3055\u308c\u3066\u3044\u308b\u3002 First, gradient-based methods [Zeiler and Fergus, 2014; Mahendran and Vedaldi, 2015; Simonyan et al., 2013; Springenberg et al., 2015] are the mainstream of network visualization. These methods mainly compute gradients of the score of a given CNN unit w.r.t. the input image. They use the gradients to estimate the image appearance that maximizes the unit score. [Olah et al., 2017] has provided a toolbox of existing techniques to visualize patterns encoded in different conv-layers of a pre-trained CNN. \u307e\u305a\u3001gradient-based\u306e\u65b9\u6cd5[Zeiler and Fergus\u30012014; Mahendran and Vedaldi\u30012015; Simonyan et al\u3002\u30012013; Springenberg et al\u3002\u30012015]\u306f\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u53ef\u8996\u5316\u306e\u4e3b\u6d41\u3067\u3042\u308b\u3002 \u3053\u308c\u3089\u306e\u65b9\u6cd5\u306f\u3001\u4e3b\u306b\u3001\u4e0e\u3048\u3089\u308c\u305fCNN\u30e6\u30cb\u30c3\u30c8\u306e\u30b9\u30b3\u30a2\u306e\u52fe\u914d\u3092\u8a08\u7b97\u3059\u308b\u3002 \u5165\u529b\u753b\u50cf\u306b\u95a2\u3057\u3066\u3002 \u5f7c\u3089\u306f\u3001\u5358\u4f4d\u30b9\u30b3\u30a2\u3092\u6700\u5927\u306b\u3059\u308b\u753b\u50cf\u306e\u72b6\u614b\u3092\u63a8\u5b9a\u3059\u308b\u305f\u3081\u306b\u52fe\u914d\u3092\u4f7f\u7528\u3059\u308b\u3002 [Olah et al\u3002\u30012017]\u306f\u3001\u4e8b\u524d\u306b\u8a13\u7df4\u3055\u308c\u305fCNN\u306e\u3055\u307e\u3056\u307e\u306aconv-layers\u3067\u30b3\u30fc\u30c9\u5316\u3055\u308c\u305f\u30d1\u30bf\u30fc\u30f3\u3092\u8996\u899a\u5316\u3059\u308b\u305f\u3081\u306e\u65e2\u5b58\u306e\u30c6\u30af\u30cb\u30c3\u30af\u306e\u30c4\u30fc\u30eb\u30dc\u30c3\u30af\u30b9\u3092\u63d0\u4f9b\u3057\u3066\u3044\u307e\u3059\u3002 Zeiler MD, Fergus R, 2014. Visualizing and understanding convolutional networks. European Conf on Computer Vision, p.818-833. https://doi.org/10.1007/978-3-319-10590-1_53 Mahendran A, Vedaldi A, 2015. Understanding deep image representations by inverting them. Proc IEEE Conf on Computer Visionand Pattern Recognition, p.5188-5196. https://doi.org/10.1109/CVPR.2015.7299155 Simonyan K, Vedaldi A, Zisserman A, 2013. Deep inside convolutional networks: visualising image classi\ufb01cation models and saliency maps. http://arxiv.org/abs/1312.6034 Springenberg JT, Dosovitskiy A, Brox T, et al., 2015. Striving for simplicity: the all convolutional net. Inte Conf on Learning Representations, p.1-14. Olah C, Mordvintsev A, Schubert L, 2017. Feature visualization. Distill. https://doi.org/10.23915/distill.00007 Second, the up-convolutional net [Dosovitskiy and Brox, 2016] is another typical technique to visualize CNN representations. The up-convolutional net inverts CNN feature maps to images. We can regard up-convolutional nets as a tool that indirectly illustrates the image appearance corresponding to a feature map, although compared to gradient-based methods, up-convolutional nets cannot mathematically ensure that the visualization result exactly reflects actual representations in the CNN. \u6b21\u306b\u3001up-convolutional net[Dosovitskiy and Brox\u30012016]\u306f\u3001CNN\u8868\u73fe\u3092\u8996\u899a\u5316\u3059\u308b\u3082\u30461\u3064\u306e\u5178\u578b\u7684\u306a\u624b\u6cd5\u3067\u3059\u3002 up-convolutional net\u306f\u3001CNN\u306e\u7279\u5fb4\u30de\u30c3\u30d7\u3092\u753b\u50cf\u306b\u8ee2\u5316\u3057\u307e\u3059\u3002 up-convolutional nets\u306f\u52fe\u914d\u30d9\u30fc\u30b9\u306e\u65b9\u6cd5\u3068\u6bd4\u8f03\u3057\u305f\u3068\u304d\u3001feature map\u306b\u5bfe\u5fdc\u3059\u308b\u753b\u50cf\u306e\u72b6\u614b\u3092\u9593\u63a5\u7684\u306b\u793a\u3059\u305f\u3081\u306e\u30c4\u30fc\u30eb\u3068\u3057\u3066\u8003\u3048\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002 \u7573\u307f\u8fbc\u307f\u30cd\u30c3\u30c8\u306f\u3001\u8996\u899a\u5316\u7d50\u679c\u304cCNN\u306e\u5b9f\u969b\u306e\u8868\u73fe\u3092\u6b63\u78ba\u306b\u53cd\u6620\u3059\u308b\u3053\u3068\u3092\u6570\u5b66\u7684\u306b\u4fdd\u8a3c\u3067\u304d\u307e\u305b\u3093\u3002 Similarly, [Nguyen et al., 2017] has further introduced an additional prior, which controls the semantic meaning of the synthesized image, to the adversarial generative network. \u540c\u69d8\u306b\u3001[Nguyen et al\u3002\u30012017]\u306f\u3055\u3089\u306b\u3001\u5408\u6210\u753b\u50cf\u306e\u610f\u5473\u7684\u610f\u5473\u3092\u6575\u5bfe\u7684\u306a\u751f\u6210\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306b\u5236\u5fa1\u3059\u308b\u8ffd\u52a0\u306e\u5148\u884c\u6280\u8853\u3092\u5c0e\u5165\u3057\u305f\u3002 We can use CNN feature maps as the prior for visualization. In addition, [Zhou et al., 2015] has proposed a method to accurately compute the image-resolution receptive field of neural activations in a feature map. The actual receptive field of neural activation is smaller than the theoretical receptive field computed using the filter size. \u6211\u3005\u306f\u3001\u8996\u899a\u5316\u306e\u305f\u3081\u306b\u4e8b\u524d\u306bCNN\u7279\u5fb4\u30de\u30c3\u30d7\u3092\u4f7f\u7528\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u3002 \u3055\u3089\u306b\u3001[Zhou et al\u3002\u30012015]\u306f\u3001\u7279\u5fb4\u30de\u30c3\u30d7\u5185\u306e\u795e\u7d4c\u6d3b\u52d5\u306e\u753b\u50cf\u89e3\u50cf\u5ea6\u53d7\u5bb9\u91ce\u3092\u6b63\u78ba\u306b\u8a08\u7b97\u3059\u308b\u65b9\u6cd5\u3092\u63d0\u6848\u3057\u3066\u3044\u308b\u3002 \u795e\u7d4c\u6d3b\u6027\u5316\u306e\u5b9f\u969b\u306e\u53d7\u5bb9\u91ce\u306f\u3001\u30d5\u30a3\u30eb\u30bf\u30fc\u30b5\u30a4\u30ba\u3092\u7528\u3044\u3066\u8a08\u7b97\u3055\u308c\u305f\u7406\u8ad6\u53d7\u5bb9\u91ce\u3088\u308a\u3082\u5c0f\u3055\u3044\u3002 The accurate estimation of the receptive field helps people to understand the representation of a filter. \u53d7\u5bb9\u91ce\u306e\u6b63\u78ba\u306a\u63a8\u5b9a\u306f\u3001\u4eba\u3005\u304c\u30d5\u30a3\u30eb\u30bf\u306e\u8868\u73fe\u3092\u7406\u89e3\u3059\u308b\u306e\u306b\u5f79\u7acb\u3061\u307e\u3059\u3002 3 Diagnosis of CNN representations Some methods go beyond the visualization of CNNs and diagnose CNN representations to obtain insight understanding of features encoded in a CNN. We roughly divide all relevant research into the following five directions. \u3044\u304f\u3064\u304b\u306e\u65b9\u6cd5\u306f\u3001CNN\u306e\u8996\u899a\u5316\u3092\u8d85\u3048\u3001CNN\u8868\u73fe\u3092\u8a3a\u65ad\u3057\u3066\u3001CNN\u3067\u30b3\u30fc\u30c9\u5316\u3055\u308c\u305f\u7279\u5fb4\u306e\u7406\u89e3\u3092\u5f97\u308b\u3002 \u95a2\u9023\u3059\u308b\u3059\u3079\u3066\u306e\u7814\u7a76\u3092\u4ee5\u4e0b\u306e5\u3064\u306e\u65b9\u5411\u306b\u5927\u307e\u304b\u306b\u5206\u3051\u307e\u3059\u3002 Studies in the first direction analyze CNN features from a global view. [Szegedy et al., 2014] has explored semantic meanings of each filter. [Yosinski et al., 2014] has analyzed the transferability of filter representations in intermediate conv-layers. [Lu, 2015; Aubry and Russell, 2015] have computed feature distributions of different categories/attributes in the feature space of a pre-trained CNN. \u7b2c1\u306e\u65b9\u5411\u306e\u7814\u7a76\u306f\u3001\u30b0\u30ed\u30fc\u30d0\u30eb\u306a\u8996\u70b9\u304b\u3089CNN\u306e\u7279\u5fb4\u3092\u5206\u6790\u3059\u308b\u3002 [Szegedy et al\u3002\u30012014]\u306f\u3001\u5404\u30d5\u30a3\u30eb\u30bf\u306e\u610f\u5473\u8ad6\u7684\u610f\u5473\u3092\u63a2\u6c42\u3057\u3066\u3044\u308b\u3002 [Yosinski et al\u3002\u30012014]\u306f\u3001\u4e2d\u9593\u30b3\u30f3\u30d0\u30fc\u30b8\u30e7\u30f3\u5c64\u306b\u304a\u3051\u308b\u30d5\u30a3\u30eb\u30bf\u8868\u73fe\u306e\u4f1d\u9054\u53ef\u80fd\u6027\u3092\u5206\u6790\u3057\u305f\u3002 [Lu\u30012015; Aubry and Russell\u30012015]\u306f\u3001\u4e8b\u524d\u306b\u8a13\u7df4\u3055\u308c\u305fCNN\u306e\u7279\u5fb4\u7a7a\u9593\u306b\u304a\u3051\u308b\u7570\u306a\u308b\u30ab\u30c6\u30b4\u30ea/\u5c5e\u6027\u306e\u7279\u5fb4\u5206\u5e03\u3092\u8a08\u7b97\u3057\u305f\u3002 The second research direction extracts image regions that directly contribute the network output for a label/attribute to explain CNN representations of the label/attribute. This is similar to the visualization of CNNs. Methods of [Fong and Vedaldi, 2017; Selvaraju et al., 2017] have been proposed to propagate gradients of feature maps w.r.t. the final loss back to the image plane to estimate the image regions. The LIME model proposed in [Ribeiro et al., 2016] extracts image regions that are highly sensitive to the network output. \u7b2c2\u306e\u7814\u7a76\u65b9\u5411\u306f\u3001\u30e9\u30d9\u30eb/\u5c5e\u6027\u306eCNN\u8868\u73fe\u3092\u8aac\u660e\u3059\u308b\u305f\u3081\u306b\u30e9\u30d9\u30eb/\u5c5e\u6027\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u51fa\u529b\u306b\u76f4\u63a5\u5bc4\u4e0e\u3059\u308b\u753b\u50cf\u9818\u57df\u3092\u62bd\u51fa\u3059\u308b\u3002 \u3053\u308c\u306f\u3001CNN\u306e\u8996\u899a\u5316\u306b\u4f3c\u3066\u3044\u307e\u3059\u3002 [Fong and Vedaldi\u30012017; Selvaraju\u3089\u30012017] \u7279\u5fb4\u30de\u30c3\u30d7\u306e\u52fe\u914d\u3092\u4f1d\u64ad\u3059\u308b\u3053\u3068\u304c\u63d0\u6848\u3055\u308c\u3066\u3044\u308b\u3002 \u753b\u50cf\u9818\u57df\u3092\u63a8\u5b9a\u3059\u308b\u305f\u3081\u306b\u6700\u7d42\u7684\u306a\u640d\u5931\u3092\u753b\u50cf\u5e73\u9762\u306b\u623b\u3059\u3002 [Ribeiro et al\u3002\u30012016]\u3067\u63d0\u6848\u3055\u308c\u305fLIME\u30e2\u30c7\u30eb\u306f\u3001\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u51fa\u529b\u306b\u5bfe\u3057\u3066\u975e\u5e38\u306b\u654f\u611f\u306a\u753b\u50cf\u9818\u57df\u3092\u62bd\u51fa\u3059\u308b\u3002 Studies of [Zintgraf et al., 2017; Kindermans et al., 2017; Kumar et al., 2017] have invented methods to visualize areas in the input image that contribute the most to the decision-making process of the CNN. [Wang et al., 2017; Goyal et al., 2016] have tried to interpret the logic for visual question-answering encoded in neural networks. [Zintgraf et al\u3002\u30012017; Kindermans\u3089\u30012017; Kumar et al\u3002\u30012017]\u306f\u3001CNN\u306e\u610f\u601d\u6c7a\u5b9a\u30d7\u30ed\u30bb\u30b9\u306b\u6700\u3082\u5bc4\u4e0e\u3059\u308b\u5165\u529b\u753b\u50cf\u5185\u306e\u9818\u57df\u3092\u8996\u899a\u5316\u3059\u308b\u65b9\u6cd5\u3092\u767a\u660e\u3057\u305f\u3002 [Wang\u3089\u30012017; Goyal et al\u3002\u30012016]\u306f\u3001\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306b\u7b26\u53f7\u5316\u3055\u308c\u305f\u8996\u899a\u7684\u8cea\u554f\u5fdc\u7b54\u306e\u8ad6\u7406\u3092\u89e3\u91c8\u3057\u3088\u3046\u3068\u8a66\u307f\u305f\u3002 These studies list important objects (or regions of interests) detected from the images and crucial words in questions as the explanation of output answers. \u3053\u308c\u3089\u306e\u7814\u7a76\u306f\u3001\u753b\u50cf\u304b\u3089\u691c\u51fa\u3055\u308c\u305f\u91cd\u8981\u306a\u5bfe\u8c61\uff08\u307e\u305f\u306f\u95a2\u5fc3\u9818\u57df\uff09\u3068\u3001\u51fa\u529b\u56de\u7b54\u306e\u8aac\u660e\u3068\u3057\u3066\u306e\u8cea\u554f\u306e\u91cd\u8981\u306a\u5358\u8a9e\u3092\u30ea\u30b9\u30c8\u3057\u307e\u3059\u3002 The estimation of vulnerable points in the feature space of a CNN is also a popular direction for diagnosing network representations. CNN\u306e\u7279\u5fb4\u7a7a\u9593\u306b\u304a\u3051\u308b\u8106\u5f31\u306a\u70b9\u306e\u63a8\u5b9a\u306f\u3001\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u8868\u73fe\u3092\u8a3a\u65ad\u3059\u308b\u305f\u3081\u306e\u4e00\u822c\u7684\u306a\u65b9\u5411\u3067\u3082\u3042\u308b\u3002 Approaches of [Su et al., 2017; Koh and Liang, 2017; Szegedy et al., 2014] have been developed to compute adversarial samples for a CNN. I.e. these studies aim to estimate the minimum noisy perturbation of the input image that can change the final prediction. [Su\u3089\u30012017; Koh and Liang\u30012017; Szegedy et al\u3002\u30012014] \u306f\u3001CNN\u306e\u6575\u5bfe\u7684\u30b5\u30f3\u30d7\u30eb\u3092\u8a08\u7b97\u3059\u308b\u305f\u3081\u306b\u958b\u767a\u3055\u308c\u307e\u3057\u305f\u3002 \u3059\u306a\u308f\u3061\u3002 \u3053\u308c\u3089\u306e\u7814\u7a76\u306f\u3001\u5165\u529b\u753b\u50cf\u4e2d\u306b\u3069\u306e\u3088\u3046\u306aminumum noisy perturbation\u3092\u4e0e\u3048\u308b\u3068\u6700\u7d42\u7684\u306a\u4e88\u6e2c\u3092\u5909\u3048\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u304b\u63a8\u5b9a\u3059\u308b\u3053\u3068\u3092\u76ee\u7684\u3068\u3057\u3066\u3044\u308b\u3002 In particular, influence functions proposed in [Koh and Liang, 2017] can be used to compute adversarial samples. The influence function can also provide plausible ways to create training samples to attack the learning of CNNs, fix the training set, and further debug representations of a CNN. \u7279\u306b\u3001[Koh and Liang\u30012017]\u3067\u63d0\u6848\u3055\u308c\u305f\u5f71\u97ff\u95a2\u6570\u3092\u4f7f\u7528\u3057\u3066\u3001\u6575\u5bfe\u7684\u30b5\u30f3\u30d7\u30eb\u3092\u8a08\u7b97\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u3002 \u5f71\u97ff\u95a2\u6570\u306f\u3001CNN\u306e\u5b66\u7fd2\u3092\u653b\u6483\u3057\u3001\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u30bb\u30c3\u30c8\u3092\u4fee\u6b63\u3057\u3001\u3055\u3089\u306bCNN\u306e\u8868\u73fe\u3092\u30c7\u30d0\u30c3\u30b0\u3059\u308b\u305f\u3081\u306e\u8a13\u7df4\u30b5\u30f3\u30d7\u30eb\u3092\u4f5c\u6210\u3059\u308b\u305f\u3081\u306e\u3082\u3063\u3068\u3082\u3089\u3057\u3044\u65b9\u6cd5\u3092\u63d0\u4f9b\u3059\u308b\u3053\u3068\u3082\u3067\u304d\u308b\u3002 The fourth research direction is to refine network representations based on the analysis of network feature spaces. \u7b2c4\u306e\u7814\u7a76\u65b9\u5411\u306f\u3001\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u7279\u5fb4\u7a7a\u9593\u306e\u5206\u6790\u306b\u57fa\u3065\u3044\u3066\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u8868\u73fe\u3092\u6539\u826f\u3059\u308b\u3053\u3068\u3067\u3042\u308b\u3002 Given a CNN pre-trained for object classification, [Lakkaraju et al., 2017] has proposed a method to discover knowledge blind spots (unknown patterns) of the CNN in a weakly-supervised manner. \u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u5206\u985e\u306e\u305f\u3081\u306b\u4e8b\u524d\u306b\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u3055\u308c\u305fCNN\u304c\u4e0e\u3048\u3089\u308c\u305f\u5834\u5408\u3001[Lakkaraju et al\u3002\u30012017]\u306fCNN\u306e\u77e5\u8b58\u306e\u76f2\u70b9\uff08\u672a\u77e5\u306e\u30d1\u30bf\u30fc\u30f3\uff09\u3092\u5f31\u304f\u76e3\u8996\u3057\u305f\u65b9\u6cd5\u3067\u767a\u898b\u3059\u308b\u65b9\u6cd5\u3092\u63d0\u6848\u3057\u3066\u3044\u308b\u3002 This method grouped all sample points in the entire feature space of a CNN into thousands of pseudo-categories. It assumed that a well learned CNN would use the sub-space of each pseudo-category to exclusively represent a subset of a specific object class. \u3053\u306e\u65b9\u6cd5\u306f\u3001CNN\u306e\u7279\u5fb4\u7a7a\u9593\u5168\u4f53\u306e\u3059\u3079\u3066\u306e\u30b5\u30f3\u30d7\u30eb\u70b9\u3092\u6570\u5343\u306e\u64ec\u4f3c\u30ab\u30c6\u30b4\u30ea\u306b\u5206\u985e\u3059\u308b\u3002 \u3088\u304f\u5b66\u7fd2\u3055\u308c\u305fCNN\u306f\u3001\u7279\u5b9a\u306e\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u30af\u30e9\u30b9\u306e\u30b5\u30d6\u30bb\u30c3\u30c8\u3092\u6392\u4ed6\u7684\u306b\u8868\u3059\u305f\u3081\u306b\u5404\u7591\u4f3c\u30ab\u30c6\u30b4\u30ea\u306e\u90e8\u5206\u7a7a\u9593\u3092\u4f7f\u7528\u3059\u308b\u3068\u3044\u3046\u3053\u3068\u304c\u8003\u3048\u3089\u308c\u308b\u3002 In this way, this study randomly showed object samples within each sub-space, and used the sample purity in the sub-space to discover potential representation flaws hidden in a pre-trained CNN. \u3053\u306e\u3088\u3046\u306b\u3001\u3053\u306e\u7814\u7a76\u3067\u306f\u5404\u90e8\u5206\u7a7a\u9593\u5185\u306b\u5bfe\u8c61\u30b5\u30f3\u30d7\u30eb\u3092\u30e9\u30f3\u30c0\u30e0\u306b\u8868\u793a\u3057\u3001\u90e8\u5206\u7a7a\u9593\u5185\u306esample purity\u3092\u4f7f\u7528\u3057\u3066\u3001\u4e8b\u524d\u306b\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u3055\u308c\u305fCNN\u306b\u96a0\u308c\u305f\u8868\u73fe\u4e0a\u306e\u6b20\u9665\u3092\u767a\u898b\u3059\u308b\u3002 To distill representations of a teacher network to a student network for sentiment analysis, [Hu et al., 2016] has proposed using logic rules of natural languages (e.g. I-ORG cannot follow B-PER) to construct a distillation loss to supervise the knowledge distillation of neural networks, in order to obtain more meaningful network representations. \u611f\u60c5\u5206\u6790\u306e\u305f\u3081\u306b\u6559\u5e2b\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u8868\u73fe\u3092\u751f\u5f92\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306b\u5f15\u304d\u51fa\u3059\u305f\u3081\u306b\u3001[Hu et al\u3002\u30012016]\u306f\u81ea\u7136\u8a00\u8a9e\u306e\u8ad6\u7406\u898f\u5247\u3092\u4f7f\u7528\u3059\u308b\u3053\u3068\u3092\u63d0\u6848\u3057\u3066\u3044\u308b\uff08\u4f8b\u3048\u3070I-ORG\u306fB-PER\u306b\u5f93\u3046\u3053\u3068\u304c\u3067\u304d\u306a\u3044\uff09 \u3088\u308a\u610f\u5473\u306e\u3042\u308b\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u5f97\u308b\u305f\u3081\u306b\u3001\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u77e5\u8b58\u84b8\u7559 \u8868\u73fe\u3002 Finally, [Zhang et al., 2018b] has presented a method to discover potential, biased representations of a CNN. Fig. 1 shows biased representations of a CNN trained for the estimation of face attributes. When an attribute usually co-appears with specific visual features in training images, then the CNN may use such co-appearing features to represent the attribute. \u6700\u5f8c\u306b\u3001[Zhang et al 2018b]\u306f\u3001CNN\u306e\u6f5c\u5728\u7684\u3067\u504f\u898b\u306e\u3042\u308b\u8868\u73fe\u3092\u767a\u898b\u3059\u308b\u65b9\u6cd5\u3092\u63d0\u793a\u3057\u305f\u3002 \u56f31\u306f\u3001\u9854\u5c5e\u6027\u306e\u63a8\u5b9a\u306e\u305f\u3081\u306b\u8a13\u7df4\u3055\u308c\u305fCNN\u306e\u30d0\u30a4\u30a2\u30b9\u3055\u308c\u305f\u8868\u73fe\u3092\u793a\u3059\u3002 \u5c5e\u6027\u304c\u3001\u901a\u5e38\u3001\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u753b\u50cf\u5185\u306e\u7279\u5b9a\u306e\u8996\u899a\u7684\u7279\u5fb4\u3068\u540c\u6642\u306b\u73fe\u308c\u308b\u5834\u5408\u3001CNN\u306f\u3001\u305d\u306e\u3088\u3046\u306a\u5171\u6f14\u3059\u308b\u7279\u5fb4\u3092\u4f7f\u7528\u3057\u3066\u3001\u305d\u306e\u5c5e\u6027\u3092\u8868\u3059\u3053\u3068\u304c\u3067\u304d\u308b\u3002 Zhang Q, Cao R, Shi F, et al., 2018b. Interpreting CNN knowledge via an explanatory graph. Proc 32nd AAAI Conf on Artificial Intelligence, p.2124-2132. When the used co-appearing features are not semantically related to the target attribute, these features can be considered as biased representations. Given a pre-trained CNN (e.g. a CNN that was trained to estimate face attributes), [Zhang et al., 2018b] required people to annotate some ground-truth relationships between attributes, e.g. the lipstick attribute is positively related to the heavy-makeup attribute, and is not related to the black hair attribute. \u4f7f\u7528\u3055\u308c\u305f\u5171\u51fa\u73fe\u7279\u5fb4\u304c\u30bf\u30fc\u30b2\u30c3\u30c8\u5c5e\u6027\u306b\u610f\u5473\u7684\u306b\u95a2\u9023\u3057\u3066\u3044\u306a\u3044\u5834\u5408\u3001\u3053\u308c\u3089\u306e\u7279\u5fb4\u306f\u30d0\u30a4\u30a2\u30b9\u3055\u308c\u305f\u8868\u73fe\u3068\u898b\u306a\u3059\u3053\u3068\u304c\u3067\u304d\u308b\u3002 \u4e8b\u524d\u306b\u8a13\u7df4\u3055\u308c\u305fCNN\uff08\u4f8b\u3048\u3070\u3001\u9854\u5c5e\u6027\u3092\u63a8\u5b9a\u3059\u308b\u3088\u3046\u306b\u8a13\u7df4\u3055\u308c\u305fCNN\uff09\u304c\u4e0e\u3048\u3089\u308c\u305f\u5834\u5408\u3001[Zhang et al 2018b]\u306f\u3001\u4eba\u3005\u306b\u5c5e\u6027\u9593\u306e\u3044\u304f\u3064\u304b\u306e\u771f\u5b9f\u306e\u95a2\u4fc2\u3092\u6ce8\u91c8\u3059\u308b\u5fc5\u8981\u304c\u3042\u3063\u305f\u3002 \u53e3\u7d05\u306e\u5c5e\u6027\u306f\u91cd\u3044\u5316\u7ca7\u306e\u5c5e\u6027\u306b\u7a4d\u6975\u7684\u306b\u95a2\u9023\u3057\u3001\u9ed2\u9aea\u306e\u5c5e\u6027\u306b\u306f\u95a2\u4fc2\u3057\u307e\u305b\u3093\u3002 Then, the method mined inference patterns of each attribute output from conv-layers, and used inference patterns to compute actual attribute relationships encoded in the CNN. \u6b21\u306b\u3001conv-layers\u304b\u3089\u51fa\u529b\u3055\u308c\u305f\u5404\u5c5e\u6027\u306e\u63a8\u8ad6\u30d1\u30bf\u30fc\u30f3\u3092\u30de\u30a4\u30cb\u30f3\u30b0\u3057\u3001\u63a8\u8ad6\u30d1\u30bf\u30fc\u30f3\u3092\u4f7f\u7528\u3057\u3066CNN\u3067\u30a8\u30f3\u30b3\u30fc\u30c9\u3055\u308c\u305f\u5b9f\u969b\u306e\u5c5e\u6027\u95a2\u4fc2\u3092\u8a08\u7b97\u3057\u307e\u3059\u3002 Conflicts between the ground-truth and the mined attribute relationships indicated biased representations. \u5730\u4e0a\u771f\u7406\u3068\u63a1\u6398\u3055\u308c\u305f\u5c5e\u6027\u95a2\u4fc2\u3068\u306e\u9593\u306e\u885d\u7a81\u306f\u3001\u504f\u3063\u305f\u8868\u73fe\u3092\u793a\u3057\u305f\u3002 4 Disentangling CNN representations into explanatory graphs & decision trees \u5224\u65ad\u6839\u62e0\u3068\u306a\u308a\u3046\u308b\u8907\u6570\u306e\u30d1\u30bf\u30fc\u30f3\u306e\u5171\u8d77\u95a2\u4fc2\u3092\u660e\u3089\u304b\u306b\u3059\u308b\u624b\u6cd5 4.1 Disentangling CNN representations into explanatory graphs \u8b58\u5225\u7406\u7531\u3067\u306f\u306a\u304fCNN\u306e\u5185\u90e8\u8868\u73fe\u3092\u7406\u89e3\u3059\u308b\u65b9\u6cd5(\u3082\u3061\u308d\u3093\u3001\u9593\u63a5\u7684\u306b\u8b58\u5225\u7406\u7531\u306e\u7406\u89e3\u306b\u3082\u3064\u306a\u304c\u308a\u307e\u3059) Compared with the visualization and diagnosis of network representations in previous sections, disentangling CNN features into human-interpretable graphical representations (namely explanatory graphs) provides a more thorough explanation of network representations. \u524d\u7bc0\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u8868\u73fe\u306e\u8996\u899a\u5316\u3068\u8a3a\u65ad\u3068\u6bd4\u8f03\u3057\u3066\u3001CNN\u6a5f\u80fd\u3092\u4eba\u9593\u304c\u89e3\u91c8\u53ef\u80fd\u306a\u30b0\u30e9\u30d5\u30a3\u30ab\u30eb\u306a\u8868\u73fe\uff08\u3059\u306a\u308f\u3061\u3001\u8aac\u660e\u7684\u306a\u30b0\u30e9\u30d5\uff09\u306b\u89e3\u304d\u307b\u3050\u3059\u3053\u3068\u306b\u3088\u308a\u3001\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u8868\u73fe\u306e\u3088\u308a\u5b8c\u5168\u306a\u8aac\u660e\u304c\u63d0\u4f9b\u3055\u308c\u308b\u3002 [Zhang et al., 2018a; Zhang et al.,2016] have proposed disentangling features in conv-layers of a pre-trained CNN and have used a graphical model to represent the semantic hierarchy hidden inside a CNN. [Zhang\u3089\u30012018a; Zhang et al\u3002\u30012016]\u306f\u3001\u4e8b\u524d\u306b\u8a13\u7df4\u3055\u308c\u305fCNN\u306econv-layers\u306e\u660e\u5feb\u306a\u7279\u5fb4\u3092\u63d0\u6848\u3057\u3001\u30b0\u30e9\u30d5\u30a3\u30ab\u30eb\u30e2\u30c7\u30eb\u3092\u4f7f\u7528\u3057\u3066CNN\u5185\u306b\u96a0\u3055\u308c\u305f\u610f\u5473\u968e\u5c64\u3092\u8868\u73fe\u3057\u3066\u3044\u308b\u3002 As shown in Fig. 2, each filter in a high conv-layer of a CNN usually represents a mixture of patterns. For example, the filter may be activated by both the head and the tail parts of an object. Thus, to provide a global view of how visual knowledge is organized in a pre-trained CNN, studies of [Zhang et al., 2018a; Zhang et al., 2016] aim to answer the following three questions. \u56f32\u306b\u793a\u3059\u3088\u3046\u306b\u3001CNN\u306ehigh conv-layer\u306e\u5404\u30d5\u30a3\u30eb\u30bf\u306f\u3001\u901a\u5e38\u3001\u30d1\u30bf\u30fc\u30f3\u306e\u6df7\u5408\u3092\u8868\u3059\u3002 \u4f8b\u3048\u3070\u3001\u30d5\u30a3\u30eb\u30bf\u306f\u3001\u7269\u4f53\u306e\u982d\u90e8\u53ca\u3073\u5c3e\u90e8\u306e\u4e21\u65b9\u306b\u3088\u3063\u3066\u4f5c\u52d5\u3055\u305b\u3089\u308c\u308b\u3002 \u3057\u305f\u304c\u3063\u3066\u3001\u4e8b\u524d\u306b\u8a13\u7df4\u3055\u308c\u305fCNN\u3067\u3069\u306e\u3088\u3046\u306b\u8996\u899a\u77e5\u8b58\u304c\u7d44\u7e54\u5316\u3055\u308c\u3066\u3044\u308b\u304b\u3092\u5168\u4f53\u7684\u306b\u898b\u308b\u305f\u3081\u306b\u3001[Zhang et al\u3002\u30012018a; Zhang et al\u3002\u30012016]\u306f\u3001\u4ee5\u4e0b\u306e3\u3064\u306e\u8cea\u554f\u306b\u7b54\u3048\u308b\u3053\u3068\u3092\u76ee\u6307\u3057\u3066\u3044\u308b Zhang Q, Cao R, Wu YN, et al., 2016. Growing interpretable part graphs on convnets via multi-shot learning. Proc 30th AAAI Conf on Artificial Intelligence, p.2898-2906. Zhang Q, Wang W, Zhu SC, 2018a. Examining CNN representations with respect to dataset bias. Proc 32nd AAAI Conf on Artificial Intelligence, in press How many types of visual patterns are memorized by each convolutional filter of the CNN (here, a visual pat tern may describe a specific object part or a certain texture)? Which patterns are co-activated to describe an object part? What is the spatial relationship between two co-activated patterns? CNN\u306e\u5404\u7573\u307f\u8fbc\u307f\u30d5\u30a3\u30eb\u30bf\u306b\u3088\u3063\u3066\u4f55\u7a2e\u985e\u306e\u30d3\u30b8\u30e5\u30a2\u30eb\u30d1\u30bf\u30fc\u30f3\u304c\u8a18\u61b6\u3055\u308c\u3066\u3044\u308b\u304b\uff08\u3053\u3053\u3067\u3001\u30d3\u30b8\u30e5\u30a2\u30eb\u30d1\u30bf\u30fc\u30f3\u306f\u7279\u5b9a\u306e\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u90e8\u5206\u307e\u305f\u306f\u7279\u5b9a\u306e\u30c6\u30af\u30b9\u30c1\u30e3\u3092\u8a18\u8ff0\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u308b\uff09 \u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u90e8\u5206\u3092\u8a18\u8ff0\u3059\u308b\u305f\u3081\u306b\u3069\u306e\u30d1\u30bf\u30fc\u30f3\u304c\u5171\u306b\u8d77\u52d5\u3055\u308c\u3066\u3044\u307e\u3059\u304b\uff1f 2\u3064\u306e\u5171\u6d3b\u6027\u5316\u30d1\u30bf\u30fc\u30f3\u9593\u306e\u7a7a\u9593\u7684\u95a2\u4fc2\u306f\u4f55\u3067\u3059\u304b\uff1f As shown in Fig. 3, the explanatory graph explains the knowledge semantic hidden inside the CNN. The explanatory graph disentangles the mixture of part patterns in each filter\u2019s feature map of a conv-layer, and uses each graph node to represent a part. \u56f33\u306b\u793a\u3059\u3088\u3046\u306b\u3001\u8aac\u660e\u30b0\u30e9\u30d5\u306f\u3001CNN\u5185\u306b\u96a0\u3055\u308c\u305f\u77e5\u8b58\u610f\u5473\u3092\u8aac\u660e\u3057\u3066\u3044\u308b\u3002 \u8aac\u660e\u30b0\u30e9\u30d5\u306f\u3001\u5404\u30d5\u30a3\u30eb\u30bf\u306econv-layer\u306e\u30d5\u30a3\u30fc\u30c1\u30e3\u30de\u30c3\u30d7\u5185\u306e\u30d1\u30fc\u30c4\u30d1\u30bf\u30fc\u30f3\u306e\u6df7\u5408\u3092\u89e3\u304d\u3001\u5404\u30b0\u30e9\u30d5\u30ce\u30fc\u30c9\u3092\u4f7f\u7528\u3057\u3066\u30d1\u30fc\u30c4\u3092\u8868\u3057\u307e\u3059\u3002 \u2022 The explanatory graph has multiple layers. Each graph layer corresponds to a specific conv-layer of a CNN. \u2022 Each filter in a conv-layer may represent the appearance of different object parts. The algorithm automatically disentangles the mixture of part patterns encoded in a single filter, and uses a node in the explanatory graph to represent each part pattern. \u8aac\u660e\u30b0\u30e9\u30d5\u306b\u306f\u8907\u6570\u306e\u30ec\u30a4\u30e4\u30fc\u304c\u3042\u308a\u307e\u3059\u3002 \u5404\u30b0\u30e9\u30d5\u5c64\u306f\u3001CNN\u306e\u7279\u5b9a\u306e\u30b3\u30f3\u30d0\u30ec\u30a4\u5c64\u306b\u5bfe\u5fdc\u3059\u308b\u3002 conv-layer\u306e\u5404\u30d5\u30a3\u30eb\u30bf\u306f\u3001\u7570\u306a\u308b\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u90e8\u5206\u306e\u5916\u89b3\u3092\u8868\u3059\u3053\u3068\u304c\u3042\u308a\u307e\u3059\u3002 \u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306f\u3001\u5358\u4e00\u306e\u30d5\u30a3\u30eb\u30bf\u306b\u7b26\u53f7\u5316\u3055\u308c\u305f\u90e8\u5206\u30d1\u30bf\u30fc\u30f3\u306e\u6df7\u5408\u3092\u81ea\u52d5\u7684\u306b\u89e3\u304d\u3001\u8aac\u660e\u30b0\u30e9\u30d5\u306e\u30ce\u30fc\u30c9\u3092\u4f7f\u7528\u3057\u3066\u5404\u90e8\u5206\u30d1\u30bf\u30fc\u30f3\u3092\u8868\u3059\u3002 \u2022 Each node in the explanatory graph consistently represents the same object part through different images. We can use the node to localize the corresponding part on the input image. To some extent, the node is robust to shape deformation and pose variations. \u8aac\u660e\u30b0\u30e9\u30d5\u306e\u5404\u30ce\u30fc\u30c9\u306f\u3001\u4e00\u8cab\u3057\u3066\u3001\u7570\u306a\u308b\u753b\u50cf\u3092\u4ecb\u3057\u3066\u540c\u3058\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u90e8\u5206\u3092\u8868\u3059\u3002 \u3053\u306e\u30ce\u30fc\u30c9\u3092\u4f7f\u7528\u3057\u3066\u3001\u5165\u529b\u753b\u50cf\u4e0a\u306e\u5bfe\u5fdc\u3059\u308b\u90e8\u5206\u3092\u30ed\u30fc\u30ab\u30e9\u30a4\u30ba\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002 \u3042\u308b\u7a0b\u5ea6\u307e\u3067\u3001\u30ce\u30fc\u30c9\u306f\u3001\u5f62\u72b6\u5909\u5f62\u304a\u3088\u3073\u59ff\u52e2\u5909\u52d5\u306b\u5bfe\u3057\u3066\u30ed\u30d0\u30b9\u30c8\u3067\u3042\u308b\u3002 \u2022 Each edge encodes the co-activation relationship and the spatial relationship between two nodes in adjacent layers. \u5404\u30a8\u30c3\u30b8\u306f\u3001\u5171\u6d3b\u6027\u5316\u95a2\u4fc2\u304a\u3088\u3073\u96a3\u63a5\u3059\u308b\u5c64\u5185\u306e2\u3064\u306e\u30ce\u30fc\u30c9\u9593\u306e\u7a7a\u9593\u7684\u95a2\u4fc2\u3092\u7b26\u53f7\u5316\u3059\u308b\u3002 \u2022 We can regard an explanatory graph as a compression of feature maps of conv-layers. A CNN has multiple convlayers. \u6211\u3005\u306f\u3001\u8aac\u660e\u30b0\u30e9\u30d5\u3092conv-layers\u306e\u7279\u5fb4\u30de\u30c3\u30d7\u306e\u5727\u7e2e\u3068\u898b\u306a\u3059\u3053\u3068\u304c\u3067\u304d\u308b\u3002 CNN\u306b\u306f\u8907\u6570\u306econvlayers\u304c\u3042\u308b\u3002 Each conv-layer may have hundreds of filters, and each filter may produce a feature map with hundreds of neural units. We can use tens of thousands of nodes in the explanatory graph to represent information contained in all tens of millions of neural units in these feature maps, i.e. by which part patterns the feature maps are activated, and where the part patterns are localized in input images. \u5404conv-layer\u306f\u4f55\u767e\u3082\u306e\u30d5\u30a3\u30eb\u30bf\u3092\u6301\u3061\u3001\u5404\u30d5\u30a3\u30eb\u30bf\u306f\u4f55\u767e\u3082\u306e\u795e\u7d4c\u5358\u4f4d\u3092\u6301\u3064\u30d5\u30a3\u30fc\u30c1\u30e3\u30de\u30c3\u30d7\u3092\u751f\u6210\u3057\u307e\u3059\u3002 \u8aac\u660e\u30b0\u30e9\u30d5\u306e\u4f55\u4e07\u3082\u306e\u30ce\u30fc\u30c9\u3092\u4f7f\u7528\u3057\u3066\u3001\u3053\u308c\u3089\u306e\u7279\u5fb4\u30de\u30c3\u30d7\u5185\u306e\u3059\u3079\u3066\u306e\u6570\u5343\u4e07\u306e\u795e\u7d4c\u5358\u4f4d\u306b\u542b\u307e\u308c\u308b\u60c5\u5831\u3001\u3059\u306a\u308f\u3061\u3001\u7279\u5fb4\u30de\u30c3\u30d7\u304c\u3069\u306e\u90e8\u5206\u30d1\u30bf\u30fc\u30f3\u3092\u6d3b\u6027\u5316\u3057\u3001\u90e8\u5206\u30d1\u30bf\u30fc\u30f3\u304c\u5165\u529b\u753b\u50cf\u306b\u5c40\u5728\u3059\u308b\u304b \u3002 \u2022 Just like a dictionary, each input image can only trigger a small subset of part patterns (nodes) in the explanatory graph. \u8f9e\u66f8\u3068\u540c\u69d8\u306b\u3001\u5404\u5165\u529b\u753b\u50cf\u306f\u8aac\u660e\u30b0\u30e9\u30d5\u306e\u90e8\u5206\u30d1\u30bf\u30fc\u30f3\uff08\u30ce\u30fc\u30c9\uff09\u306e\u5c0f\u3055\u306a\u30b5\u30d6\u30bb\u30c3\u30c8\u3092\u30c8\u30ea\u30ac\u3059\u308b\u3060\u3051\u3067\u3059\u3002 Each node describes a common part pattern with high transferability, which is shared by hundreds or thousands of training images. \u5404\u30ce\u30fc\u30c9\u306f\u3001\u6570\u767e\u307e\u305f\u306f\u6570\u5343\u306e\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u753b\u50cf\u306b\u3088\u3063\u3066\u5171\u6709\u3055\u308c\u308b\u9ad8\u3044\u53ef\u642c\u6027\u3092\u5099\u3048\u305f\u5171\u901a\u90e8\u54c1\u30d1\u30bf\u30fc\u30f3\u3092\u8a18\u8ff0\u3057\u307e\u3059\u3002 Fig. 4 lists top-ranked image patches corresponding to different nodes in the explanatory graph. Fig. 5 visualizes the spatial distribution of object parts inferred by the top 50% nodes in the L-th layer of the explanatory graph with the highest inference scores. Fig. 6 shows object parts inferred by a single node. \u56f34\u306f\u3001\u8aac\u660e\u30b0\u30e9\u30d5\u4e2d\u306e\u7570\u306a\u308b\u30ce\u30fc\u30c9\u306b\u5bfe\u5fdc\u3059\u308b\u6700\u4e0a\u4f4d\u753b\u50cf\u30d1\u30c3\u30c1\u3092\u30ea\u30b9\u30c8\u30a2\u30c3\u30d7\u3057\u305f\u3082\u306e\u3067\u3042\u308b\u3002 \u56f35\u306f\u3001\u63a8\u8ad6\u30b9\u30b3\u30a2\u304c\u6700\u3082\u9ad8\u3044\u8aac\u660e\u30b0\u30e9\u30d5\u306eL\u756a\u76ee\u306e\u5c64\u306e\u4e0a\u4f4d50\uff05\u30ce\u30fc\u30c9\u306b\u3088\u3063\u3066\u63a8\u8ad6\u3055\u308c\u305f\u5bfe\u8c61\u90e8\u54c1\u306e\u7a7a\u9593\u5206\u5e03\u3092\u8996\u899a\u5316\u3057\u305f\u3082\u306e\u3067\u3042\u308b\u3002 \u56f36\u306f\u3001\u5358\u4e00\u306e\u30ce\u30fc\u30c9\u306b\u3088\u3063\u3066\u63a8\u8ad6\u3055\u308c\u308b\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u90e8\u5206\u3092\u793a\u3059\u3002 Application: multi-shot part localization There are many potential applications based on the explanatory graph. \u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\uff1a\u30de\u30eb\u30c1\u30b7\u30e7\u30c3\u30c8\u90e8\u5206\u306e\u30ed\u30fc\u30ab\u30ea\u30bc\u30fc\u30b7\u30e7\u30f3\u8aac\u660e\u30b0\u30e9\u30d5\u306b\u57fa\u3065\u3044\u3066\u591a\u304f\u306e\u6f5c\u5728\u7684\u306a\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u304c\u3042\u308a\u307e\u3059\u3002 For example, we can regard the explanatory graph as a visual dictionary of a category and transfer graph nodes to other applications, such as multi-shot part localization. \u4f8b\u3048\u3070\u3001\u8aac\u660e\u30b0\u30e9\u30d5\u3092\u30ab\u30c6\u30b4\u30ea\u306e\u8996\u899a\u7684\u8f9e\u66f8\u3068\u898b\u306a\u3057\u3066\u3001\u30b0\u30e9\u30d5\u30ce\u30fc\u30c9\u3092multi-shot part localization\u306a\u3069\u306e\u4ed6\u306e\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u306b\u8ee2\u9001\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002 Given very few bounding boxes of an object part, [Zhang et al., 2018a] has proposed retrieving hundreds of nodes that are related to the part annotations from the explanatory graph, and then use the retrieved nodes to localize object parts in previously unseen images. \u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u90e8\u5206\u306e\u5883\u754c\u30dc\u30c3\u30af\u30b9\u306e\u6570\u304c\u975e\u5e38\u306b\u5c11\u306a\u3044\u305f\u3081\u3001[Zhang et al\u3002\u30012018a]\u306f\u6570\u767e\u306e\u8aac\u660e\u30b0\u30e9\u30d5\u304b\u3089\u306e\u90e8\u54c1\u6ce8\u91c8\u306b\u95a2\u9023\u3057\u3066\u3044\u308b\u30ce\u30fc\u30c9\u3092\u691c\u7d22\u3059\u308b\u3053\u3068\u3092\u63d0\u6848\u3057\u3066\u3044\u308b \u6b21\u306b\u3001\u691c\u7d22\u3055\u308c\u305f\u30ce\u30fc\u30c9\u3092\u4f7f\u7528\u3057\u3066\u3001\u4ee5\u524d\u306b\u898b\u3048\u306a\u304b\u3063\u305f\u753b\u50cf\u5185\u306e\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u90e8\u54c1\u3092\u4f4d\u7f6e\u7279\u5b9a\u3059\u308b\u3002 Because each node in the explanatory graph encodes a part pattern shared by numerous training images, the retrieved nodes describe a general appearance of the target part without being over-fitted to the limited annotations of part bounding boxes. \u8aac\u660e\u30b0\u30e9\u30d5\u306e\u5404\u30ce\u30fc\u30c9\u306f\u3001\u591a\u6570\u306e\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u753b\u50cf\u306b\u3088\u3063\u3066\u5171\u6709\u3055\u308c\u308b\u30d1\u30fc\u30c4\u30d1\u30bf\u30fc\u30f3\u3092\u7b26\u53f7\u5316\u3059\u308b\u306e\u3067\u3001\u691c\u7d22\u3055\u308c\u305f\u30ce\u30fc\u30c9\u306f\u3001\u90e8\u54c1\u30d0\u30a6\u30f3\u30c7\u30a3\u30f3\u30b0\u30dc\u30c3\u30af\u30b9\u306e\u9650\u5b9a\u3055\u308c\u305f\u6ce8\u91c8\u306b\u3042\u307e\u308a\u9069\u5408\u3059\u308b\u3053\u3068\u306a\u304f\u3001\u30bf\u30fc\u30b2\u30c3\u30c8\u90e8\u54c1\u306e\u4e00\u822c\u7684\u306a\u5916\u89b3\u3092\u8a18\u8ff0\u3059\u308b\u3002 Given three annotations for each object part, the explanatory-graph-based method has exhibited superior performance of part localization and has decreased by about 1/3 localization errors w.r.t. the second-best baseline. \u5404\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u90e8\u5206\u306b\u3064\u3044\u30663\u3064\u306e\u6ce8\u91c8\u304c\u4e0e\u3048\u3089\u308c\u305f\u5834\u5408\u3001\u8aac\u660e\u30b0\u30e9\u30d5\u306b\u57fa\u3065\u304f\u65b9\u6cd5\u306f\u3001\u30d1\u30fc\u30c4\u30ed\u30fc\u30ab\u30ea\u30bc\u30fc\u30b7\u30e7\u30f3\u306e\u512a\u308c\u305f\u6027\u80fd\u3092\u793a\u3057\u3001\u7d041/3\u306e\u30ed\u30fc\u30ab\u30e9\u30a4\u30bc\u30fc\u30b7\u30e7\u30f3\u30a8\u30e9\u30fc\u3067\u6e1b\u5c11\u3057\u305f\u3002 2\u756a\u76ee\u306b\u826f\u3044\u30d9\u30fc\u30b9\u30e9\u30a4\u30f3\u3002 4.2 Disentangling CNN representations into decision trees \u7279\u6b8a\u306a\u69cb\u9020\u306e\u8b58\u5225\u5668\u3092\u4f7f\u7528\u3059\u308b\u65b9\u6cd5(\u3053\u306e\u8a18\u4e8b\u3067\u306e\u305d\u306e\u4ed6) [Zhang et al., 2018c] has further proposed a decision tree to encode decision modes in fully-connected layers. The decision tree is not designed for classification. Instead, the decision tree is used to quantitatively explain the logic for each CNN prediction. I.e. given an input image, we use the CNN to make a prediction. The decision tree tells people which filters in a conv-layer are used for the prediction and how much they contribute to the prediction. As shown in Fig. 7, the method mines potential decision modes memorized in fully-connected layers. The decision tree organizes these potential decision modes in a coarseto-fine manner. Furthermore, this study uses the method of [Zhang et al., 2017c] to disentangle representations of filters in the top conv-layers, i.e. making each filter represent a specific object part. In this way, people can use the decision tree to explain rationales for each CNN prediction at the semantic level, i.e. which object parts are used by the CNN to make the prediction. 5 Learning neural networks with interpretable/disentangled representations Almost all methods mentioned in previous sections focus on the understanding of a pre-trained network. In this section, we review studies of learning disentangled representations of neural networks, where representations in middle layers are no longer a black box but have clear semantic meanings. Compared to the understanding of pre-trained networks, learning networks with disentangled representations present more challenges. Up to now, only a few studies have been published in this direction. 5.1 Interpretable convolutional neural networks As shown in Fig. 8, [Zhang et al., 2017c] has developed a method to modify an ordinary CNN to obtain disentangled representations in high conv-layers by adding a loss to each filter in the conv-layers. The loss is used to regularize the feature map towards the representation of a specific object part. Note that people do not need to annotate any object parts or textures to supervise the learning of interpretable CNNs. Instead, the loss automatically assigns an object part to each filter during the end-to-end learning process. As shown in Fig. 9, this method designs some templates. Each template T\u00b5i is a matrix with the same size of feature map. T\u00b5i describes the ideal distribution of activations for the feature map when the target part mainly triggers the i-th unit in the feature map. Given the joint probability of fitting a feature map to a template, the loss of a filter is formulated as the mutual information between the feature map and the templates. This loss encourages a low entropy of inter-category activations. I.e. each filter in the conv-layer is assigned to a certain category. If the input image belongs to the target category, then the loss expects the filter\u2019s feature map to match a template well; otherwise, the filter needs to remain inactivated. In addition, the loss also encourages a low entropy of spatial distributions of neural activations. I.e. when the input image belongs the target category, the feature map is supposed to exclusively fit a single template. In other words, the filter needs to activate a single location on the feature map. This study assumes that if a filter repetitively activates various feature-map regions, then this filter is more likely to describe low-level textures (e.g. colors and edges), instead of high-level parts. For example, the left eye and the right eye may be represented by different filters, because contexts of the two eyes are symmetric, but not the same. Fig.10 shows feature maps produced by different filters of an interpretable CNN. Each filter consistently represents the same object part through various images. 5.2 Interpretable R-CNN [Wu et al., 2017] has proposed the learning of qualitatively interpretable models for object detection based on the RCNN. The objective is to unfold latent configurations of object parts automatically during the object-detection process. This method is learned without using any part annotations for supervision. [Wu et al., 2017] uses a topdown hierarchical and compositional grammar, namely an And-Or graph (AOG), to model latent configurations of object parts. This method uses an AOG-based parsing operator to substitute for the RoI-Pooling operator used in the RCNN. The AOG-based parsing harnesses explainable compositional structures of objects and maintains the discrimination power of a R-CNN. This idea is related to the disentanglement of the local, bottom-up, and top-down information components for prediction [Wu et al., 2007; Yang et al., 2009; Wu and Zhu, 2011]. During the detection process, a bounding box is interpreted as the best parse tree derived from the AOG on-the-fly. During the learning process, a folding-unfolding method is used to train the AOG and R-CNN in an end-to-end manner. Fig. 11 illustrates an example of object detection. The proposed method detects object bounding boxes. The method also determines the latent parse tree and part configurations of objects as the qualitatively extractive rationale in detection. 5.3 Capsule networks [Sabour et al., 2017] has designed novel neural units, namely capsules, in order to substitute for traditional neural units to construct a capsule network. Each capsule outputs an activity vector instead of a scalar. The length of the activity vector represents the activation strength of the capsule, and the orientation of the activity vector encodes instantiation parameters. Active capsules in the lower layer send messages to capsules in the adjacent higher layer. This method uses an iterative routing-by-agreement mechanism to assign higher weights with the low-layer capsules whose outputs better fit the instantiation parameters of the high-layer capsule. Experiments showed that when people trained capsule networks using the MNIST dataset [LeCun et al., 1998b], a capsule encoded a specific semantic concept. Different dimensions of the activity vector of a capsule controlled different features, including 1) scale and thickness, 2) localized part, 3) stroke thickness, 3) localized skew, and 4) width and translation. 5.4 Information maximizing generative adversarial nets The information maximizing generative adversarial net [Chen et al., 2016], namely InfoGAN, is an extension of the generative adversarial network. The InfoGAN maximizes the mutual information between certain dimensions of the latent representation and the image observation. The InfoGAN separates input variables of the generator into two types, i.e. the incompressible noise z and the latent code c. This study aims to learn the latent code c to encode certain semantic concepts in an unsupervised manner. The InfoGAN has been trained using the MNIST dataset [LeCun et al., 1998b], the CelebA dataset [Liu et al., 2015], the SVHN dataset [Netzer et al., 2011], the 3D face dataset [Paysan et al., 2009], and the 3D chair dataset [Aubry et al., 2014]. Experiments have shown that the latent code has successfully encoded the digit type, the rotation, and the width of digits in the MNIST dataset, the lighting condition and the plate context in the SVHN dataset, the azimuth, the existence of glasses, the hairstyle, and the emotion in the CelebA dataset, and the width and 3D rotation in the 3D face and chair datasets. 6 Evaluation metrics for network interpretability Evaluation metrics for model interpretability are crucial for the development of explainable models. \u30e2\u30c7\u30eb\u89e3\u91c8\u80fd\u529b\u306e\u8a55\u4fa1\u57fa\u6e96\u306f\u3001\u8aac\u660e\u53ef\u80fd\u306a\u30e2\u30c7\u30eb\u306e\u958b\u767a\u306b\u3068\u3063\u3066\u91cd\u8981\u3067\u3059\u3002 This is because unlike traditional well-defined visual applications (e.g. object detection and segmentation), network interpretability is more difficult to define and evaluate. The evaluation metric of network interpretability can help people define the concept of network interpretability and guide the development of learning interpretable network representations. \u3053\u308c\u306f\u3001\u5f93\u6765\u306e\u660e\u78ba\u306b\u5b9a\u7fa9\u3055\u308c\u305f\u30d3\u30b8\u30e5\u30a2\u30eb\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\uff08\u4f8b\u3048\u3070\u3001\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u691c\u51fa\u304a\u3088\u3073\u30bb\u30b0\u30e1\u30f3\u30c8\u5316\uff09\u3068\u306f\u7570\u306a\u308a\u3001\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u89e3\u91c8\u53ef\u80fd\u6027\u3092\u5b9a\u7fa9\u304a\u3088\u3073\u8a55\u4fa1\u3059\u308b\u3053\u3068\u304c\u3088\u308a\u56f0\u96e3\u3067\u3042\u308b\u305f\u3081\u3067\u3042\u308b\u3002 \u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u89e3\u91c8\u80fd\u529b\u306e\u8a55\u4fa1\u57fa\u6e96\u306f\u3001\u4eba\u3005\u304c\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u89e3\u91c8\u80fd\u529b\u306e\u6982\u5ff5\u3092\u5b9a\u7fa9\u3057\u3001\u89e3\u91c8\u53ef\u80fd\u306a\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u8868\u73fe\u3092\u5b66\u7fd2\u3059\u308b\u3053\u3068\u3092\u5c0e\u304f\u306e\u306b\u5f79\u7acb\u3064\u3002 Up to now, only very few studies have discussed the evaluation of network interpretability. Proposing a promising evaluation metric is still a big challenge to state-of-the-art algorithms. In this section, we simply introduce two latest evaluation metrics for the interpretability of CNN filters, i.e. the filter interpretability proposed by [Bau et al., 2017] and the location instability proposed by [Zhang et al., 2018a]. \u4eca\u307e\u3067\u306f\u3001\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u89e3\u91c8\u53ef\u80fd\u6027\u306e\u8a55\u4fa1\u306b\u3064\u3044\u3066\u8b70\u8ad6\u3055\u308c\u305f\u7814\u7a76\u306f\u3054\u304f\u308f\u305a\u304b\u3067\u3057\u305f\u3002 \u6709\u671b\u306a\u8a55\u4fa1\u57fa\u6e96\u3092\u63d0\u6848\u3059\u308b\u3053\u3068\u306f\u3001\u6700\u5148\u7aef\u306e\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306b\u3068\u3063\u3066\u4f9d\u7136\u3068\u3057\u3066\u5927\u304d\u306a\u8ab2\u984c\u3067\u3059\u3002 \u3053\u306e\u30bb\u30af\u30b7\u30e7\u30f3\u3067\u306f\u3001CNN\u30d5\u30a3\u30eb\u30bf\u306e\u89e3\u91c8\u53ef\u80fd\u6027\u306e\u305f\u3081\u306e2\u3064\u306e\u6700\u65b0\u8a55\u4fa1\u30e1\u30c8\u30ea\u30c3\u30af\u3001\u3059\u306a\u308f\u3061[Bau\u3089\u30012017]\u306b\u3088\u3063\u3066\u63d0\u6848\u3055\u308c\u305f\u30d5\u30a3\u30eb\u30bf\u306e\u89e3\u91c8\u53ef\u80fd\u6027\u3068[Zhang et al\u3002\u30012018a]\u306b\u3088\u3063\u3066\u63d0\u6848\u3055\u308c\u305f\u4f4d\u7f6e\u4e0d\u5b89\u5b9a\u6027\u3092\u7c21\u5358\u306b\u7d39\u4ecb\u3059\u308b\u3002 6.1 Filter interpretability [Bau et al., 2017] has defined six types of semantics for CNN filters, i.e. objects, parts, scenes, textures, materials, and colors. The evaluation of filter interpretability requires people to annotate these six types of semantics on testing images at the pixel level. [Bau\u3089\u30012017]\u306f\u3001CNN\u30d5\u30a3\u30eb\u30bf\u3001\u3059\u306a\u308f\u3061\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3001\u30d1\u30fc\u30c4\u3001\u30b7\u30fc\u30f3\u3001\u30c6\u30af\u30b9\u30c1\u30e3\u3001\u30de\u30c6\u30ea\u30a2\u30eb\u3001\u304a\u3088\u3073\u30ab\u30e9\u30fc\u306e6\u7a2e\u985e\u306e\u30bb\u30de\u30f3\u30c6\u30a3\u30af\u30b9\u3092\u5b9a\u7fa9\u3057\u3066\u3044\u307e\u3059\u3002 \u30d5\u30a3\u30eb\u30bf\u306e\u89e3\u91c8\u80fd\u529b\u306e\u8a55\u4fa1\u3067\u306f\u3001\u30d4\u30af\u30bb\u30eb\u30ec\u30d9\u30eb\u3067\u753b\u50cf\u3092\u30c6\u30b9\u30c8\u3059\u308b\u969b\u306b\u3001\u3053\u308c\u3089\u306e6\u7a2e\u985e\u306e\u30bb\u30de\u30f3\u30c6\u30a3\u30af\u30b9\u306b\u6ce8\u91c8\u3092\u4ed8\u3051\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002 The evaluation metric measures the fitness between the image-resolution receptive field of a filter\u2019s neural activations1 and the pixel-level semantic annotations on the image. \u8a55\u4fa1\u30e1\u30c8\u30ea\u30c3\u30af\u306f\u3001\u30d5\u30a3\u30eb\u30bf\u306e\u795e\u7d4c\u6d3b\u6027\u5316\u306e\u753b\u50cf\u89e3\u50cf\u5ea6\u53d7\u5bb9\u91ce\u3068\u3001\u753b\u50cf\u4e0a\u306e\u30d4\u30af\u30bb\u30eb\u30ec\u30d9\u30eb\u306e\u610f\u5473\u6ce8\u91c8\u3068\u306e\u9069\u5408\u5ea6\u3092\u6e2c\u5b9a\u3059\u308b\u3002 For example, if the receptive field of a filter\u2019s neural activations usually highly overlaps with ground-truth image regions of a specific semantic concept through different images, then we can consider that the filter represents this semantic concept. \u4f8b\u3048\u3070\u3001\u30d5\u30a3\u30eb\u30bf\u306e\u795e\u7d4c\u6d3b\u52d5\u306e\u53d7\u5bb9\u5834\u304c\u3001\u901a\u5e38\u3001\u7570\u306a\u308b\u753b\u50cf\u3092\u901a\u3057\u3066\u7279\u5b9a\u306e\u610f\u5473\u6982\u5ff5\u306eground-truth\u753b\u50cf\u9818\u57df\u3068\u9ad8\u5ea6\u306b\u91cd\u306a\u308b\u5834\u5408\u3001\u30d5\u30a3\u30eb\u30bf\u304c\u3053\u306e\u610f\u5473\u6982\u5ff5\u3092\u8868\u3059\u3068\u8003\u3048\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u3002 For each filter f, this method computes its feature maps X = {x = f(I)|I \u2208 I} on different testing images. Then, the distribution of activation scores in all positions of all feature maps is computed. [Bau et al., 2017] set an activation threshold Tf such that p(xij > Tf ) = 0.005, to select top activations from all spatial locations [i, j] of all feature maps x \u2208 X as valid map regions corresponding to f\u2019s semantics. Then, the method scales up low-resolution valid map regions to the image resolution, thereby obtaining the receptive field of valid activations on each image. \u5404\u30d5\u30a3\u30eb\u30bff\u306b\u3064\u3044\u3066\u3001\u3053\u306e\u65b9\u6cd5\u306f\u7570\u306a\u308b\u30c6\u30b9\u30c8\u753b\u50cf\u4e0a\u3067\u305d\u306e\u7279\u5fb4\u30de\u30c3\u30d7X = {x = f\uff08I\uff09| I\u2208I}\u3092\u8a08\u7b97\u3059\u308b\u3002 \u6b21\u306b\u3001\u3059\u3079\u3066\u306e\u7279\u5fb4\u30de\u30c3\u30d7\u306e\u3059\u3079\u3066\u306e\u4f4d\u7f6e\u306b\u304a\u3051\u308b\u6d3b\u6027\u5316\u30b9\u30b3\u30a2\u306e\u5206\u5e03\u304c\u8a08\u7b97\u3055\u308c\u308b\u3002 [Bau\u3089\u30012017]\u306f\u3001\u3059\u3079\u3066\u306e\u7279\u5fb4\u30de\u30c3\u30d7x\u2208X\u306e\u3059\u3079\u3066\u306e\u7a7a\u9593\u4f4d\u7f6e[i\u3001j]\u304b\u3089\u306e\u30c8\u30c3\u30d7\u30a2\u30af\u30c6\u30a3\u30d9\u30fc\u30b7\u30e7\u30f3\u3092\u3001\u5bfe\u5fdc\u3059\u308b\u6709\u52b9\u306a\u30de\u30c3\u30d7\u9818\u57df\u3068\u3057\u3066\u9078\u629e\u3059\u308b\u305f\u3081\u306b\u3001p\uff08xij> Tf\uff09= 0.005\u3068\u306a\u308b\u3088\u3046\u306b\u3001 f\u306e\u30bb\u30de\u30f3\u30c6\u30a3\u30af\u30b9\u3002 \u6b21\u306b\u3001\u3053\u306e\u65b9\u6cd5\u306f\u3001\u4f4e\u89e3\u50cf\u5ea6\u306e\u6709\u52b9\u306a\u30de\u30c3\u30d7\u9818\u57df\u3092\u753b\u50cf\u89e3\u50cf\u5ea6\u306b\u30b9\u30b1\u30fc\u30eb\u30a2\u30c3\u30d7\u3057\u3001\u305d\u308c\u306b\u3088\u308a\u5404\u753b\u50cf\u4e0a\u306e\u6709\u52b9\u306a\u30a2\u30af\u30c1\u30d9\u30fc\u30b7\u30e7\u30f3\u306e\u53d7\u5bb9\u91ce\u3092\u5f97\u308b\u3002 We use S I f to denote the receptive field of f\u2019s valid activations w.r.t. the image I. The compatibility between a filter f and a specific semantic concept is reported as an intersection-over-union score IoUI f,k = kS I f \u2229S I k k kSI f \u222aSI k k , where S I k denotes the ground-truth mask of the k-th semantic concept on the image I. Given an image I, filter f is associated with the k-th concept if IoUI f,k > 0.04. The probability of the k-th concept being associated with the filter f is given as Pf,k = meanI:with k-th concept1(IoUI f,k > 0.04). Thus, we can use Pf,k to evaluate the filter interpretability of f. 6.2 Location instability Another evaluation metric is location instability. This metric is proposed by [Zhang et al., 2018a] to evaluate the fitness between a CNN filter and the representation of an object part. \u5225\u306e\u8a55\u4fa1\u57fa\u6e96\u306f\u3001\u4f4d\u7f6e\u4e0d\u5b89\u5b9a\u6027\u3067\u3042\u308b\u3002 \u3053\u306e\u30e1\u30c8\u30ea\u30c3\u30af\u306f\u3001[Zhang et al\u3002\u30012018a]\u306b\u3088\u3063\u3066\u3001CNN\u30d5\u30a3\u30eb\u30bf\u3068\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u90e8\u5206\u306e\u8868\u73fe\u3068\u306e\u9593\u306e\u9069\u5408\u5ea6\u3092\u8a55\u4fa1\u3059\u308b\u305f\u3081\u306b\u63d0\u6848\u3055\u308c\u3066\u3044\u308b\u3002 Given an input image I, the CNN computes a feature map x \u2208 R N\u00d7N of filter f. We can regard the unit xi,j (1 \u2264 i, j \u2264 N) with the highest activation as the location inference of f, where N \u00d7 N is referred to as the size of the feature map. We use p\u02c6 to denote the image position that corresponds to the inferred feature map location (i, j), i.e. the center of the unit xi,j \u2019s receptive field when we backward propagated the receptive field to the image plane. \u5165\u529b\u753b\u50cfI\u304c\u4e0e\u3048\u3089\u308c\u308b\u3068\u3001CNN\u306f\u30d5\u30a3\u30eb\u30bff\u306e\u7279\u5fb4\u30de\u30c3\u30d7x\u2208RN\u00d7N\u3092\u8a08\u7b97\u3059\u308b\u3002 \u6700\u3082\u9ad8\u3044\u6d3b\u6027\u5316\u3092\u6709\u3059\u308b\u30e6\u30cb\u30c3\u30c8xi\u3001j\uff081\u2266i\u3001j\u2266N\uff09\u3092f\u306e\u4f4d\u7f6e\u63a8\u8ad6\u3068\u3057\u3066\u8003\u3048\u308b\u3053\u3068\u304c\u3067\u304d\u3001N\u00d7N\u306f\u7279\u5fb4\u30de\u30c3\u30d7\u306e\u30b5\u30a4\u30ba\u3068\u547c\u3070\u308c\u308b\u3002 \u63a8\u5b9a\u3055\u308c\u305f\u7279\u5fb4\u30de\u30c3\u30d7\u4f4d\u7f6e\uff08i\u3001j\uff09\u306b\u5bfe\u5fdc\u3059\u308b\u753b\u50cf\u4f4d\u7f6e\u3001\u3059\u306a\u308f\u3061\u3001\u53d7\u5bb9\u91ce\u3092\u753b\u50cf\u5e73\u9762\u306b\u9006\u65b9\u5411\u306b\u4f1d\u64ad\u3055\u305b\u305f\u3068\u304d\u306e\u5358\u4f4dx i\u3001j\u306e\u53d7\u5bb9\u91ce\u306e\u4e2d\u5fc3\u3092\u8868\u3059\u305f\u3081\u306bp\u3092\u4f7f\u7528\u3059\u308b\u3002 The evaluation assumes that if f consistently represented the same object part (the object part may not have an explicit name according to people\u2019s cognition) through different objects, then distances between the image position p\u02c6 and some object landmarks should not change much among different objects. \u3053\u306e\u8a55\u4fa1\u3067\u306f\u3001f\u304c\u7570\u306a\u308b\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3092\u901a\u3057\u3066\u540c\u3058\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u90e8\u5206\uff08\u4eba\u306e\u8a8d\u77e5\u306b\u5fdc\u3058\u3066\u660e\u793a\u7684\u306a\u540d\u524d\u3092\u6301\u305f\u306a\u3044\u3053\u3068\u304c\u3042\u308b\uff09\u3092\u4e00\u8cab\u3057\u3066\u8868\u73fe\u3059\u308b\u3068\u3001\u753b\u50cf\u4f4d\u7f6ep\u3068\u3044\u304f\u3064\u304b\u306e\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u30e9\u30f3\u30c9\u30de\u30fc\u30af\u9593\u306e\u8ddd\u96e2\u306f\u3001\u7570\u306a\u308b\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u9593\u3067\u3042\u307e\u308a\u5909\u5316\u3057\u3066\u306f\u306a\u3089\u306a\u3044\u3082\u306e\u3068\u4eee\u5b9a\u3059\u308b\u3002 For example, if filter f represents the shoulder, then the distance between the shoulder and the head should remain stable through different objects. \u4f8b\u3048\u3070\u3001\u30d5\u30a3\u30eb\u30bff\u304c\u80a9\u90e8\u3092\u8868\u3059\u5834\u5408\u3001\u80a9\u90e8\u3068\u982d\u90e8\u3068\u306e\u9593\u306e\u8ddd\u96e2\u306f\u3001\u7570\u306a\u308b\u7269\u4f53\u306b\u3088\u3063\u3066\u5b89\u5b9a\u3057\u305f\u307e\u307e\u3067\u3042\u308b\u3079\u304d\u3067\u3042\u308b\u3002 Therefore, people can compute the deviation of the distance between the inferred position p\u02c6 and a specific groundtruth landmark among different images. The average deviation w.r.t. various landmarks can be used to evaluate the location \u3057\u305f\u304c\u3063\u3066\u3001\u4eba\u3005\u306f\u3001\u63a8\u6e2c\u3055\u308c\u305f\u4f4d\u7f6ep\u3068\u7570\u306a\u308b\u753b\u50cf\u9593\u306e\u7279\u5b9a\u306e\u5730\u4e0a\u306e\u30e9\u30f3\u30c9\u30de\u30fc\u30af\u3068\u306e\u9593\u306e\u8ddd\u96e2\u306e\u504f\u5dee\u3092\u8a08\u7b97\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u3002 \u5e73\u5747\u504f\u5deew.r.t. \u3055\u307e\u3056\u307e\u306a\u30e9\u30f3\u30c9\u30de\u30fc\u30af\u3092\u4f7f\u7528\u3057\u3066\u5834\u6240\u3092\u8a55\u4fa1\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059 instability of f. As shown in Fig. 12, let dI (pk, p\u02c6) = \u221a kpk\u2212p\u02c6k w2+h2 denote the normalized distance between the inferred part and the k-th landmark pk on image I. \u221a w2 + h2 dep notes the diagonal length of the input image. Thus, Df,k = varI [dI (pk, p\u02c6)] is reported as the relative location deviation of filter f w.r.t. the k-th landmark, where varI [dI (pk, p\u02c6)] is referred to as the variation of the distance dI (pk, p\u02c6). Because each landmark cannot appear in all testing images, for each filter f, the metric only uses inference results with the topM highest activation scores on images containing the k-th landmark to compute Df,k. In this way, the average of relative location deviations of all the filters in a conv-layer w.r.t. all landmarks, i.e. meanfmeanK k=1Df,k, measures the location instability of a CNN, where K denotes the number of landmarks. \u5165\u529b\u753b\u50cf\u306e\u5bfe\u89d2\u9577\u3055\u3092\u8a18\u9332\u3059\u308b\u3002 \u3057\u305f\u304c\u3063\u3066\u3001Df\u3001k = varI [dI\uff08pk\u3001p\uff09]\u306f\u3001\u30d5\u30a3\u30eb\u30bff w.r.t\u306e\u76f8\u5bfe\u4f4d\u7f6e\u504f\u5dee\u3068\u3057\u3066\u5831\u544a\u3055\u308c\u308b\u3002 \u3053\u3053\u3067\u3001varI [dI\uff08pk\u3001p\uff09]\u306f\u3001\u8ddd\u96e2dI\uff08pk\u3001p\uff09\u306e\u5909\u5316\u3068\u547c\u3070\u308c\u308bk\u756a\u76ee\u306e\u30e9\u30f3\u30c9\u30de\u30fc\u30af\u3067\u3042\u308b\u3002 \u5404\u30e9\u30f3\u30c9\u30de\u30fc\u30af\u306f\u3059\u3079\u3066\u306e\u30c6\u30b9\u30c8\u753b\u50cf\u306b\u8868\u793a\u3055\u308c\u306a\u3044\u305f\u3081\u3001\u5404\u30d5\u30a3\u30eb\u30bff\u306b\u3064\u3044\u3066\u3001Df\u3001k\u3092\u8a08\u7b97\u3059\u308b\u305f\u3081\u306ek\u756a\u76ee\u306e\u30e9\u30f3\u30c9\u30de\u30fc\u30af\u3092\u542b\u3080\u753b\u50cf\u4e0a\u3067topM\u306e\u6700\u3082\u9ad8\u3044\u6d3b\u6027\u5316\u30b9\u30b3\u30a2\u3067\u63a8\u8ad6\u7d50\u679c\u306e\u307f\u304c\u4f7f\u7528\u3055\u308c\u307e\u3059\u3002 \u3053\u306e\u3088\u3046\u306b\u3057\u3066\u3001conv-layer w.r.t.\u306b\u304a\u3051\u308b\u3059\u3079\u3066\u306e\u30d5\u30a3\u30eb\u30bf\u306e\u76f8\u5bfe\u7684\u4f4d\u7f6e\u504f\u5dee\u306e\u5e73\u5747\u304c\u8a08\u7b97\u3055\u308c\u308b\u3002 \u3059\u3079\u3066\u306e\u30e9\u30f3\u30c9\u30de\u30fc\u30af\u3001\u3059\u306a\u308f\u3061meanfmeanK k = 1Df\u3001k\u306f\u3001CNN\u306e\u4f4d\u7f6e\u4e0d\u5b89\u5b9a\u6027\u3092\u6e2c\u5b9a\u3059\u308b\u3002\u3053\u3053\u3067\u3001K\u306f\u30e9\u30f3\u30c9\u30de\u30fc\u30af\u306e\u6570\u3092\u793a\u3059\u3002 7 Network interpretability for middle-to-end learning Based on studies discussed in Sections 4 and 5, people may either disentangle representations of a pre-trained CNN or learn a new network with interpretable, disentangled representations. Such interpretable/disentangled network representations can further enable middle-to-end model learning at the semantic level without strong supervision. We briefly review two typical studies [Zhang et al., 2017a; Zhang et al., 2017b] of middle-to-end learning as follows. 7.1 Active question-answering for learning And-Or graphs Based on the semantic And-Or representation proposed in [Zhang et al., 2016], [Zhang et al., 2017a] has developed a method to use active question-answering to semanticize neural patterns in conv-layers of a pre-trained CNN and build a model for hierarchical object understanding. As shown in Fig. 13, the CNN is pre-trained for object classification. The method aims to extract a four-layer interpretable And-Or graph (AOG) to explain the semantic hierarchy hidden in a CNN. The AOG encodes four-layer semantics, ranging across the semantic part (OR node), part templates (AND nodes), latent patterns (OR nodes), and neural units (terminal nodes) on feature maps. In the AOG, AND nodes represent compositional regions of a part, and OR nodes encode a list of alternative template/deformation candidates for a local part. The top part node (OR node) uses its children to represent some template candidates for the part. Each part template (AND node) in the second layer uses children latent patterns to represent its constituent regions. Each latent pattern in the third layer (OR node) naturally corresponds to a certain range of units within the feature map of a filter. The latent pattern selects a unit within this range to account for its geometric deformation. To learn an AOG, [Zhang et al., 2017a] allows the computer to actively identify and ask about objects, whose neural patterns cannot be explained by the current AOG. As shown in Fig. 14, in each step of the active question-answering, the current AOG is used to localize object parts among all the unannotated images. The method actively selects objects that cannot well fit the AOG, namely unexplained objects. The method predicts the potential gain of asking about each unexplained object, and thus determines the best sequence of questions (e.g. asking about template types and bounding boxes of unexplained object parts). In this way, the method uses the answers to either refine an existing part template or mine latent patterns for new object-part templates, to grow AOG branches. Fig. 15 compares the part-localization performance of different methods. The QA-based learning exhibits significantly higher efficiency than other baselines. The proposed method uses about 1/6\u20131/3 of the part annotations for training, but achieves similar or better part-localization per- formance than fast-RCNN methods. 7.2 Interactive manipulations of CNN patterns Let a CNN be pre-trained using annotations of object bounding boxes for object classification. [Zhang et al., 2017b] has explored an interactive method to diagnose knowledge representations of a CNN, in order to transfer CNN patterns to model object parts. Unlike traditional end-to-end learning of CNNs that requires numerous training samples, this method mines object part patterns from the CNN in the scenario of one/multi-shot learning. More specifically, the method uses part annotations on very few (e.g. three) object images for supervision. Given a bounding-box annotation of a part, the proposed method first uses [Zhang et al., 2016] to mine latent patterns, which are related to the annotated part, from conv-layers of the CNN. An AOG is used to organize all mined patterns as the representation of the target part. The method visualizes the mined latent patterns and asks people to remove latent patterns unrelated to the target part interactively. In this way, people can simply prune incorrect latent patterns from AOG branches to refine the AOG. Fig. 16 visualizes initially mined patterns and the remaining patterns after human interaction. With the guidance of human interactions, [Zhang et al., 2017b] has exhibited superior performance of part localization. 8 Prospective trends and conclusions In this paper, we have reviewed several research directions within the scope of network interpretability. Visualization of a neural unit\u2019s patterns was the starting point of understanding network representations in the early years. Then, people gradually developed methods to analyze feature spaces of neural networks and diagnose potential representation flaws hidden inside neural networks. At present, disentangling chaotic representations of conv-layers into graphical models and/or symbolic logic has become an emerging research direction to open the black-box of neural networks. The approach for transforming a pre-trained CNN into an explanatory graph has been proposed and has exhibited significant efficiency in knowledge transfer and weakly-supervised learning. End-to-end learning interpretable neural networks, whose intermediate layers encode comprehensible patterns, is also a prospective trend. Interpretable CNNs have been developed, where each filter in high conv-layers represents a specific object part. Furthermore, based on interpretable representations of CNN patterns, semantic-level middle-to-end learning has been proposed to speed up the learning process. Compared to traditional end-to-end learning, middle-to-end learning allows human interactions to guide the learning process and can be applied with very few annotations for supervision. In the future, we believe the middle-to-end learning will continuously be a fundamental research direction. In addition, based on the semantic hierarchy of an interpretable network, debugging CNN representations at the semantic level will create new visual applications. Acknowledgement This work is supported by ONR MURI project N00014-16-1- 2007 and DARPA XAI Award N66001-17-2-4029, and NSF IIS 1423305.","title":"Visual_Interpretability"},{"location":"DL/survey/Visual_Interpretability/#visual_interpretability","text":"","title":"Visual_Interpretability"},{"location":"DL/survey/Visual_Interpretability/#visualinterpretability-for-deep-learning-a-survey","text":"Quan-shi ZHANG, Song-chun ZHU","title":"VisualInterpretability for Deep Learning: a Survey"},{"location":"DL/survey/Visual_Interpretability/#abstract","text":"This paper reviews recent studies in understanding neural-network representations and learning neural networks with interpretable/disentangled middlelayer representations. Although deep neural networks have exhibited superior performance in various tasks, the interpretability is always the Achilles\u2019 heel of deep neural networks. \u672c\u7a3f\u3067\u306f\u3001\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u8868\u73fe\u3092\u7406\u89e3\u3059\u308b\u305f\u3081\u306e\u6700\u8fd1\u306e\u7814\u7a76\u3092\u89e3\u8aac\u3059\u308b\uff0e\u307e\u305f\uff0c\u89e3\u91c8\u53ef\u80fd\u306a\u7d61\u307e\u3063\u3066\u3044\u306a\u3044\u4e2d\u5c64\u8868\u73fe\u3092\u7528\u3044\u3066\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u5b66\u3076\uff0e \u30c7\u30a3\u30fc\u30d7\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306f\u69d8\u3005\u306a\u30bf\u30b9\u30af\u306b\u304a\u3044\u3066\u512a\u308c\u305f\u6027\u80fd\u3092\u767a\u63ee\u3057\u305f\u304c\u3001\u89e3\u91c8\u6027\u306f\u5e38\u306b\u30c7\u30a3\u30fc\u30d7\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u5f31\u70b9\u3067\u3042\u308b\u3002 At present, deep neural networks obtain high discrimination power at the cost of low interpretability of their black-box representations. \u73fe\u5728\u3001\u30c7\u30a3\u30fc\u30d7\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306f\u3001\u30d6\u30e9\u30c3\u30af\u30dc\u30c3\u30af\u30b9\u306a\u8868\u73fe\u304b\u3089\u304f\u308b\u89e3\u91c8\u6027\u306e\u4f4e\u3055\u3092\u72a0\u7272\u306b\u3057\u3066\u3001\u9ad8\u3044\u8b58\u5225\u529b\u3092\u5f97\u3066\u3044\u308b\u3002 We believe that high model interpretability may help people to break several bottlenecks of deep learning, e.g. learning from very few annotations, learning via human-computer communications at the semantic level, and semantically debugging network representations. \u79c1\u305f\u3061\u306f\u89e3\u91c8\u80fd\u529b\u306e\u9ad8\u3044\u30e2\u30c7\u30eb\u306f\u3001\u4eba\u3005\u304c\u6df1\u3044\u5b66\u7fd2\u306e\u3044\u304f\u3064\u304b\u306e\u30dc\u30c8\u30eb\u30cd\u30c3\u30af\u3092\u89e3\u6d88\u3059\u308b\u306e\u306b\u5f79\u7acb\u3064\u304b\u3082\u3057\u308c\u306a\u3044\u3068\u8003\u3048\u3066\u3044\u308b\u3002 \u975e\u5e38\u306b\u5c11\u6570\u306e\u6ce8\u91c8\u304b\u3089\u306e\u5b66\u7fd2\u3001\u610f\u5473\u30ec\u30d9\u30eb\u3067\u306e\u4eba\u9593\u3068\u6a5f\u68b0\u306e\u4f1a\u8a71\u304b\u3089\u306e\u5b66\u7fd2\u3001\u304a\u3088\u3073\u610f\u5473\u8ad6\u7684\u306a\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u8868\u73fe\u306e\u30c7\u30d0\u30c3\u30b0\u3002 We focus on convolutional neural networks (CNNs), and we revisit the visualization of CNN representations, methods of diagnosing representations of pre-trained CNNs, approaches for disentangling pre-trained CNN representations, learning of CNNs with disentangled representations, and middle-to-end learning based on model interpretability. Finally, we discuss prospective trends in explainable artificial intelligence. \u6211\u3005\u306f\u3001\u7573\u307f\u8fbc\u307f\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\uff08CNN\uff09\u306b\u7126\u70b9\u3092\u5f53\u3066\u3001CNN\u8868\u73fe\u306e\u8996\u899a\u5316\u3001\u4e8b\u524d\u306b\u8a13\u7df4\u3055\u308c\u305fCNN\u306e\u8868\u73fe\u3092\u8a3a\u65ad\u3059\u308b\u65b9\u6cd5\u3001\u4e8b\u524d\u306b\u8a13\u7df4\u3055\u308c\u305fCNN\u8868\u73fe\u306e\u89e3\u304d\u65b9\u3001\u5206\u89e3\u8868\u73fe\u3092\u6301\u3064CNN\u306e\u5b66\u7fd2\u3001 \u30e2\u30c7\u30eb\u306e\u89e3\u91c8\u80fd\u529b\u306b\u57fa\u3065\u3044\u3066\u5b66\u7fd2\u3092\u7d42\u4e86\u3059\u308b\u3002 \u6700\u5f8c\u306b\u3001\u8aac\u660e\u53ef\u80fd\u306a\u4eba\u5de5\u77e5\u80fd\u306e\u5c06\u6765\u306e\u52d5\u5411\u306b\u3064\u3044\u3066\u8b70\u8ad6\u3059\u308b\u3002","title":"Abstract"},{"location":"DL/survey/Visual_Interpretability/#introduction","text":"Convolutional neural networks (CNNs) [LeCun et al., 1998a; Krizhevsky et al., 2012; He et al., 2016; Huang et al., 2017] have achieved superior performance in many visual tasks, such as object classification and detection. However, the endto-end learning strategy makes CNN representations a black box. Except for the final network output, it is difficult for people to understand the logic of CNN predictions hidden inside the network. \u7573\u307f\u8fbc\u307f\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\uff08CNN\uff09[LeCun\u3089\u30011998a; Krizhevsky\u3089\u30012012; He\u3089\u30012016; Huang\u3089\u30012017]\u306f\u3001\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u306e\u5206\u985e\u3084\u691c\u51fa\u306a\u3069\u3001\u591a\u304f\u306e\u30d3\u30b8\u30e5\u30a2\u30eb\u30bf\u30b9\u30af\u3067\u512a\u308c\u305f\u30d1\u30d5\u30a9\u30fc\u30de\u30f3\u30b9\u3092\u9054\u6210\u3057\u3066\u3044\u307e\u3059\u3002 \u3057\u304b\u3057\u306a\u304c\u3089\uff0cend-to-end\u306e\u5b66\u7fd2\u6226\u7565\u306f\u3001CNN\u306e\u8868\u73fe\u3092\u30d6\u30e9\u30c3\u30af\u30dc\u30c3\u30af\u30b9\u5316\u3057\u307e\u3059\uff0e \u6700\u7d42\u7684\u306a\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u51fa\u529b\u3092\u9664\u3044\u3066\u3001\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u5185\u306b\u96a0\u308c\u3066\u3044\u308bCNN\u306e\u4e88\u6e2c\u306e\u30ed\u30b8\u30c3\u30af\u3092\u4eba\u3005\u304c\u7406\u89e3\u3059\u308b\u3053\u3068\u306f\u56f0\u96e3\u3067\u3059\u3002 LeCun Y, Bottou L, Bengio Y, et al., 1998a. Gradient-based learning applied to document recognition. Proc IEEE, 86(11):2278-2324. https://doi.org/10.1109/5.726791 Krizhevsky A, Sutskever I, Hinton GE, 2012. Imagenet classification with deep convolutional neural networks. NIPS, p.1097-1105. He K, Zhang X, Ren S, et al., 2016. Deep residual learning for image recognition. Proc IEEE Conf on Computer Vision and Pattern Recognition, p.770-778. Huang G, Liu Z, Weinberger KQ, et al., 2017. Densely connected convolutional networks. Proc IEEE Conf on Computer Vision and Pattern Recognition, p.4700-4708. In recent years, a growing number of researchers have realized that high model interpretability is of significant value in both theory and practice and have developed models with interpretable knowledge representations. \u8fd1\u5e74\u3001\u591a\u304f\u306e\u7814\u7a76\u8005\u304c\u9ad8\u3044\u30e2\u30c7\u30eb\u306e\u89e3\u91c8\u80fd\u529b\u306f\u7406\u8ad6\u3068\u5b9f\u8df5\u306e\u4e21\u9762\u3067\u91cd\u8981\u306a\u4fa1\u5024\u3092\u6301\u3064\u3053\u3068\u3092\u7406\u89e3\u3057\uff0c\u89e3\u91c8\u53ef\u80fd\u306a\u77e5\u8b58\u8868\u73fe\u3092\u6301\u3064\u30e2\u30c7\u30eb\u3092\u958b\u767a\u3057\u3066\u3044\u307e\u3059\uff0e In this paper, we conduct a survey of current studies in understanding neural-network representations and learning neural networks with interpretable/disentangled representations. We can roughly define the scope of the review into the following six research directions. \u672c\u7a3f\u3067\u306f\u3001\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u8868\u73fe\u3092\u7406\u89e3\u3059\u308b\u305f\u3081\u306e\u73fe\u72b6\u306e\u7814\u7a76\u3068\u3001\u89e3\u91c8\u53ef\u80fd/\u89e3\u304d\u307b\u3050\u3055\u308c\u305f\u8868\u73fe\u3092\u7528\u3044\u305f\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u5b66\u7fd2\u306b\u3064\u3044\u3066\u8abf\u67fb\u3059\u308b\u3002 \u79c1\u305f\u3061\u306f\u30ec\u30d3\u30e5\u30fc\u306e\u7bc4\u56f2\u3092\u5927\u307e\u304b\u306b\u4ee5\u4e0b\u306e6\u3064\u306e\u7814\u7a76\u65b9\u5411\u306b\u5b9a\u7fa9\u3057\u307e\u3057\u305f\u3002 Visualization of CNN representations in intermediate network layers. These methods mainly synthesize the image that maximizes the score of a given unit in a pretrained CNN or invert feature maps of a conv-layer back to the input image. Please see Section 2 for detailed discussion. \u4e2d\u9593\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u5c64\u306b\u304a\u3051\u308bCNN\u8868\u73fe\u306e\u8996\u899a\u5316 \u3053\u308c\u3089\u306e\u65b9\u6cd5\u306f\u3001\u4e3b\u306b\u3001\u4e88\u3081\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u3055\u308c\u305fCNN\u5185\u306e\u6240\u4e0e\u306e\u30e6\u30cb\u30c3\u30c8\u306e\u30b9\u30b3\u30a2\u3092\u6700\u5927\u5316\u3059\u308b\u753b\u50cf\u3092\u5408\u6210\u3059\u308b\u304b\u3001\u307e\u305f\u306fconv-layer\u306e\u7279\u5fb4\u30de\u30c3\u30d7\u3092\u5165\u529b\u753b\u50cf\u307e\u3067\u623b\u3057\u307e\u3059\u3002 \u8a73\u7d30\u306a\u8aac\u660e\u306f\u30bb\u30af\u30b7\u30e7\u30f32\u3092\u53c2\u7167\u3057\u3066\u304f\u3060\u3055\u3044\u3002 > 2. Diagnosis of CNN representations. Related studies may either diagnose a CNN\u2019s feature space for different object categories or discover potential representation flaws in conv-layers. Please see Section 3 for details. CNN\u8868\u73fe\u306e\u8a3a\u65ad\uff0e \u95a2\u9023\u3059\u308b\u7814\u7a76\u3067\u306f\u3001\u7570\u306a\u308b\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u30ab\u30c6\u30b4\u30ea\u306eCNN\u306e\u7279\u5fb4\u7a7a\u9593\u3092\u8a3a\u65ad\u3059\u308b\u304b\u3001conv-layers\u306e\u6f5c\u5728\u7684\u306a\u8868\u73fe\u6b20\u9665\u3092\u767a\u898b\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u3002 \u8a73\u7d30\u306f\u30bb\u30af\u30b7\u30e7\u30f33\u3092\u53c2\u7167\u3057\u3066\u304f\u3060\u3055\u3044\u3002 Disentanglement of \u201cthe mixture of patterns\u201d encoded in each filter of CNNs. These studies mainly disentangle complex representations in conv-layers and transform network representations into interpretable graphs. Please see Section 4 for details. CNN\u306e\u5404\u30d5\u30a3\u30eb\u30bf\u3067\u7b26\u53f7\u5316\u3055\u308c\u305f \"\u30d1\u30bf\u30fc\u30f3\u306e\u6df7\u5408\"\u3092\u7d10\u89e3\u304f. \u3053\u308c\u3089\u306e\u7814\u7a76\u306f\u4e3b\u306bconv-layer\u306e\u8907\u96d1\u306a\u8868\u73fe\u3092\u89e3\u304d\u3001\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u8868\u73fe\u3092\u89e3\u91c8\u53ef\u80fd\u306a\u30b0\u30e9\u30d5\u306b\u5909\u63db\u3059\u308b\u3002 \u8a73\u7d30\u306f\u30bb\u30af\u30b7\u30e7\u30f34\u3092\u53c2\u7167\u3057\u3066\u304f\u3060\u3055\u3044\u3002 Building explainable models. We discuss interpretable CNNs [Zhang et al., 2017c], capsule networks [Sabour et al., 2017], interpretable R-CNNs [Wu et al., 2017], and the InfoGAN [Chen et al., 2016] in Section 5. \u8aac\u660e\u53ef\u80fd\u306a\u30e2\u30c7\u30eb\u3092\u69cb\u7bc9\u3059\u308b\u3002 \u6211\u3005\u306f\u3001\u89e3\u91c8\u53ef\u80fd\u306aCNN [Zhang\u3089\u30012017c]\u3001\u30ab\u30d7\u30bb\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af[Sabour\u3089\u30012017]\u3001\u89e3\u91c8\u53ef\u80fd\u306aR-CNN [Wu\u3089\u30012017]\u3001\u304a\u3088\u3073InfoGAN [Chen et al\u3002\u30012016] Semantic-level middle-to-end learning via humancomputer interaction. A clear semantic disentanglement of CNN representations may further enable \u201cmiddle-toend\u201d learning of neural networks with weak supervision. Section 7 introduces methods to learn new models via human-computer interactions [Zhang et al., 2017b] and active question-answering with very limited human supervision [Zhang et al., 2017a]. \u4eba\u9593\u3068\u30b3\u30f3\u30d4\u30e5\u30fc\u30bf\u3068\u306e\u30a4\u30f3\u30bf\u30e9\u30af\u30b7\u30e7\u30f3\u306b\u3088\u308b\u610f\u5473\u30ec\u30d9\u30eb\u306e\u4e2d\u9593\u7684\u306a\u5b66\u7fd2\u3002 CNN\u8868\u73fe\u306e\u660e\u78ba\u306a\u610f\u5473\u89e3\u6790\u306f\u3001\u5f31\u3044\u76e3\u7763\u4e0b\u3067\u306e\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u300c\u4e2d\u9593\u304b\u3089\u5148\u3078\u306e\u300d\u5b66\u7fd2\u3092\u3055\u3089\u306b\u53ef\u80fd\u306b\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u3002 \u7b2c7\u7bc0\u3067\u306f\u3001\u4eba\u9593\u3068\u30b3\u30f3\u30d4\u30e5\u30fc\u30bf\u306e\u76f8\u4e92\u4f5c\u7528[Zhang et al\u3002\u30012017b]\u3092\u4ecb\u3057\u3066\u65b0\u3057\u3044\u30e2\u30c7\u30eb\u3092\u5b66\u7fd2\u3059\u308b\u65b9\u6cd5\u3068\u3001\u975e\u5e38\u306b\u9650\u5b9a\u3055\u308c\u305f\u4eba\u9593\u306e\u76e3\u7763[Zhang et al\u3002\u30012017a]\u3092\u7528\u3044\u305f\u80fd\u52d5\u7684\u8cea\u554f\u5fdc\u7b54\u65b9\u6cd5\u3092\u7d39\u4ecb\u3059\u308b\u3002 Among all the above, the visualization of CNN representations is the most direct way to explore network representations. The network visualization also provides a technical foundation for many approaches to diagnosing CNN representations. \u4e0a\u8a18\u306e\u3059\u3079\u3066\u306e\u4e2d\u3067\u3001CNN\u8868\u73fe\u306e\u8996\u899a\u5316\u306f\u3001\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u8868\u73fe\u3092\u63a2\u7d22\u3059\u308b\u6700\u3082\u76f4\u63a5\u7684\u306a\u65b9\u6cd5\u3067\u3059\u3002 \u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u8996\u899a\u5316\u306f\u307e\u305f\u3001CNN\u8868\u73fe\u3092\u8a3a\u65ad\u3059\u308b\u305f\u3081\u306e\u591a\u304f\u306e\u30a2\u30d7\u30ed\u30fc\u30c1\u306e\u305f\u3081\u306e\u6280\u8853\u7684\u57fa\u790e\u3092\u63d0\u4f9b\u3059\u308b\u3002 The disentanglement of feature representations of a pre-trained CNN and the learning of explainable network representations present bigger challenges to state-of-the-art algorithms. \u4e8b\u524d\u306b\u8a13\u7df4\u3055\u308c\u305fCNN\u306e\u7279\u5fb4\u8868\u73fe\u306e\u89e3\u6790\uff0c\u304a\u3088\u3073\u8aac\u660e\u53ef\u80fd\u306a\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u8868\u73fe\u306e\u5b66\u7fd2\u306f\u3001\u6700\u5148\u7aef\u306e\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306b\u5bfe\u3059\u308b\u3088\u308a\u5927\u304d\u306a\u8ab2\u984c\u3092\u63d0\u793a\u3059\u308b\u3002 Finally, explainable or disentangled network representations are also the starting point for weakly-supervised middle-to-end learning. Values of model interpretability: The clear semantics in high conv-layers can help people trust a network\u2019s prediction. \u6700\u5f8c\u306b\u3001\u8aac\u660e\u53ef\u80fd\u306a\u307e\u305f\u306f\u89e3\u6790\u3055\u308c\u305f\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u8868\u73fe\u306f\u3001\u5f31\u304f\u76e3\u7763\u3055\u308c\u308b\u4e2d\u9593\u304b\u3089\u7d42\u308f\u308a\u307e\u3067\u306e\u5b66\u7fd2\u306e\u51fa\u767a\u70b9\u3067\u3082\u3042\u308a\u307e\u3059\u3002 \u30e2\u30c7\u30eb\u306e\u89e3\u91c8\u80fd\u529b\u306e\u30ec\u30d9\u30eb\uff1ahigh conv-layers\u3067\u306e\u660e\u78ba\u306a\u610f\u5473\u306f\u3001\u4eba\u3005\u304c\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u4e88\u6e2c\u3092\u4fe1\u983c\u3059\u308b\u306e\u306b\u5f79\u7acb\u3061\u307e\u3059\u3002 As discussed in [Zhang et al., 2018b], considering dataset and representation bias, a high accuracy on testing images still cannot ensure that a CNN will encode correct representations. For example, a CNN may use an unreliable context\u2014eye features\u2014to identify the \u201clipstick\u201d attribute of a face image. [Zhang et al\u3002\u30012018b]\u3067\u8ad6\u3058\u3089\u308c\u3066\u3044\u308b\u3088\u3046\u306b\u3001\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3068\u8868\u73fe\u30d0\u30a4\u30a2\u30b9\u3092\u8003\u616e\u3059\u308b\u3068\u3001\u30c6\u30b9\u30c8\u753b\u50cf\u306e\u7cbe\u5ea6\u304c\u9ad8\u3044\u3053\u3068\u306f\u3001CNN\u304c\u6b63\u3057\u3044\u8868\u73fe\u3092\u7b26\u53f7\u5316\u3059\u308b\u3053\u3068\u3092\u4fdd\u8a3c\u3067\u304d\u307e\u305b\u3093\u3002 \u4f8b\u3048\u3070\u3001CNN\u306f\u3001\u9854\u753b\u50cf\u306e\u300c\u53e3\u7d05\u300d\u5c5e\u6027\u3092\u8b58\u5225\u3059\u308b\u305f\u3081\u306b\uff0c\u4fe1\u983c\u3067\u304d\u306a\u3044 context-eye features-to \u3092\u4f7f\u7528\u3059\u308b\u3060\u308d\u3046\uff0e Zhang Q, Cao R, Shi F, et al., 2018b. Interpreting CNN knowledge via an explanatory graph. Proc 32nd AAAI Conf on Arti\ufb01cial Intelligence, p.2124-2132. Therefore, people usually cannot fully trust a network unless a CNN can semantically or visually explain its logic, e.g. what patterns are used for prediction. \u3057\u305f\u304c\u3063\u3066\u3001CNN\u304c\u305d\u306e\u8ad6\u7406\u3092\u610f\u5473\u7684\u306b\u307e\u305f\u306f\u8996\u899a\u7684\u306b\u8aac\u660e\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u306a\u3044\u9650\u308a\u3001\u4eba\u3005\u306f\u901a\u5e38\u3001\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u5b8c\u5168\u306b\u4fe1\u983c\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u306a\u3044\u3002\u4f8b\u3048\u3070\u3069\u306e\u30d1\u30bf\u30fc\u30f3\u304c\u4e88\u6e2c\u306b\u4f7f\u7528\u3055\u308c\u308b\u304b\u3082\uff0e In addition, the middle-to-end learning or debugging of neural networks based on the explainable or disentangled network representations may also significantly reduce the requirement for human annotation. Furthermore, based on semantic representations of networks, it is possible to merge multiple CNNs into a universal network (i.e. a network encoding generic knowledge representations for different tasks) at the semantic level in the future. \u3055\u3089\u306b\u3001\u8aac\u660e\u53ef\u80fd\u306a\u307e\u305f\u306f\u7d10\u89e3\u304b\u308c\u305f\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u8868\u73fe\u306b\u57fa\u3065\u3044\u305f\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306emiddle-to-end\u306e\u5b66\u7fd2\u307e\u305f\u306f\u30c7\u30d0\u30c3\u30b0\u306f\u3001\u4eba\u9593\u306b\u3088\u308b\u30a2\u30ce\u30c6\u30fc\u30b7\u30e7\u30f3\u306e\u5fc5\u8981\u3092\u5927\u5e45\u306b\u524a\u6e1b\u3059\u308b\u53ef\u80fd\u6027\u304c\u3042\u308a\u307e\u3059\u3002 \u3055\u3089\u306b\u3001\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u610f\u5473\u8868\u73fe\u306b\u57fa\u3065\u3044\u3066\u3001\u5c06\u6765\u306e\u610f\u5473\u30ec\u30d9\u30eb\u3067\u8907\u6570\u306eCNN\u3092\u30e6\u30cb\u30d0\u30fc\u30b5\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\uff08\u3059\u306a\u308f\u3061\u3001\u7570\u306a\u308b\u30bf\u30b9\u30af\u306e\u305f\u3081\u306e\u6c4e\u7528\u77e5\u8b58\u8868\u73fe\u3092\u7b26\u53f7\u5316\u3059\u308b\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\uff09\u306b\u30de\u30fc\u30b8\u3059\u308b\u3053\u3068\u304c\u53ef\u80fd\u3067\u3042\u308b\u3002 In the following sections, we review the above research directions and discuss the potential future of technical developments. \u4ee5\u4e0b\u306e\u30bb\u30af\u30b7\u30e7\u30f3\u3067\u306f\u3001\u4e0a\u8a18\u306e\u7814\u7a76\u306e\u65b9\u5411\u6027\u3092\u898b\u76f4\u3057\u3001\u5c06\u6765\u306e\u6280\u8853\u958b\u767a\u306e\u5c06\u6765\u306b\u3064\u3044\u3066\u8b70\u8ad6\u3059\u308b\u3002","title":"Introduction"},{"location":"DL/survey/Visual_Interpretability/#2-visualization-of-cnn-representations","text":"Visualization of filters in a CNN is the most direct way of exploring visual patterns hidden inside a neural unit. Different types of visualization methods have been developed for network visualization. CNN\u306b\u304a\u3051\u308b\u30d5\u30a3\u30eb\u30bf\u306e\u8996\u899a\u5316\u306f\u3001\u795e\u7d4c\u30e6\u30cb\u30c3\u30c8\u5185\u306b\u96a0\u3055\u308c\u305f\u8996\u899a\u30d1\u30bf\u30fc\u30f3\u3092\u63a2\u7d22\u3059\u308b\u6700\u3082\u76f4\u63a5\u7684\u306a\u65b9\u6cd5\u3067\u3042\u308b\u3002 \u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u8996\u899a\u5316\u306e\u305f\u3081\u306e\u7570\u306a\u308b\u30bf\u30a4\u30d7\u306e\u8996\u899a\u5316\u65b9\u6cd5\u304c\u958b\u767a\u3055\u308c\u3066\u3044\u308b\u3002 First, gradient-based methods [Zeiler and Fergus, 2014; Mahendran and Vedaldi, 2015; Simonyan et al., 2013; Springenberg et al., 2015] are the mainstream of network visualization. These methods mainly compute gradients of the score of a given CNN unit w.r.t. the input image. They use the gradients to estimate the image appearance that maximizes the unit score. [Olah et al., 2017] has provided a toolbox of existing techniques to visualize patterns encoded in different conv-layers of a pre-trained CNN. \u307e\u305a\u3001gradient-based\u306e\u65b9\u6cd5[Zeiler and Fergus\u30012014; Mahendran and Vedaldi\u30012015; Simonyan et al\u3002\u30012013; Springenberg et al\u3002\u30012015]\u306f\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u53ef\u8996\u5316\u306e\u4e3b\u6d41\u3067\u3042\u308b\u3002 \u3053\u308c\u3089\u306e\u65b9\u6cd5\u306f\u3001\u4e3b\u306b\u3001\u4e0e\u3048\u3089\u308c\u305fCNN\u30e6\u30cb\u30c3\u30c8\u306e\u30b9\u30b3\u30a2\u306e\u52fe\u914d\u3092\u8a08\u7b97\u3059\u308b\u3002 \u5165\u529b\u753b\u50cf\u306b\u95a2\u3057\u3066\u3002 \u5f7c\u3089\u306f\u3001\u5358\u4f4d\u30b9\u30b3\u30a2\u3092\u6700\u5927\u306b\u3059\u308b\u753b\u50cf\u306e\u72b6\u614b\u3092\u63a8\u5b9a\u3059\u308b\u305f\u3081\u306b\u52fe\u914d\u3092\u4f7f\u7528\u3059\u308b\u3002 [Olah et al\u3002\u30012017]\u306f\u3001\u4e8b\u524d\u306b\u8a13\u7df4\u3055\u308c\u305fCNN\u306e\u3055\u307e\u3056\u307e\u306aconv-layers\u3067\u30b3\u30fc\u30c9\u5316\u3055\u308c\u305f\u30d1\u30bf\u30fc\u30f3\u3092\u8996\u899a\u5316\u3059\u308b\u305f\u3081\u306e\u65e2\u5b58\u306e\u30c6\u30af\u30cb\u30c3\u30af\u306e\u30c4\u30fc\u30eb\u30dc\u30c3\u30af\u30b9\u3092\u63d0\u4f9b\u3057\u3066\u3044\u307e\u3059\u3002 Zeiler MD, Fergus R, 2014. Visualizing and understanding convolutional networks. European Conf on Computer Vision, p.818-833. https://doi.org/10.1007/978-3-319-10590-1_53 Mahendran A, Vedaldi A, 2015. Understanding deep image representations by inverting them. Proc IEEE Conf on Computer Visionand Pattern Recognition, p.5188-5196. https://doi.org/10.1109/CVPR.2015.7299155 Simonyan K, Vedaldi A, Zisserman A, 2013. Deep inside convolutional networks: visualising image classi\ufb01cation models and saliency maps. http://arxiv.org/abs/1312.6034 Springenberg JT, Dosovitskiy A, Brox T, et al., 2015. Striving for simplicity: the all convolutional net. Inte Conf on Learning Representations, p.1-14. Olah C, Mordvintsev A, Schubert L, 2017. Feature visualization. Distill. https://doi.org/10.23915/distill.00007 Second, the up-convolutional net [Dosovitskiy and Brox, 2016] is another typical technique to visualize CNN representations. The up-convolutional net inverts CNN feature maps to images. We can regard up-convolutional nets as a tool that indirectly illustrates the image appearance corresponding to a feature map, although compared to gradient-based methods, up-convolutional nets cannot mathematically ensure that the visualization result exactly reflects actual representations in the CNN. \u6b21\u306b\u3001up-convolutional net[Dosovitskiy and Brox\u30012016]\u306f\u3001CNN\u8868\u73fe\u3092\u8996\u899a\u5316\u3059\u308b\u3082\u30461\u3064\u306e\u5178\u578b\u7684\u306a\u624b\u6cd5\u3067\u3059\u3002 up-convolutional net\u306f\u3001CNN\u306e\u7279\u5fb4\u30de\u30c3\u30d7\u3092\u753b\u50cf\u306b\u8ee2\u5316\u3057\u307e\u3059\u3002 up-convolutional nets\u306f\u52fe\u914d\u30d9\u30fc\u30b9\u306e\u65b9\u6cd5\u3068\u6bd4\u8f03\u3057\u305f\u3068\u304d\u3001feature map\u306b\u5bfe\u5fdc\u3059\u308b\u753b\u50cf\u306e\u72b6\u614b\u3092\u9593\u63a5\u7684\u306b\u793a\u3059\u305f\u3081\u306e\u30c4\u30fc\u30eb\u3068\u3057\u3066\u8003\u3048\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002 \u7573\u307f\u8fbc\u307f\u30cd\u30c3\u30c8\u306f\u3001\u8996\u899a\u5316\u7d50\u679c\u304cCNN\u306e\u5b9f\u969b\u306e\u8868\u73fe\u3092\u6b63\u78ba\u306b\u53cd\u6620\u3059\u308b\u3053\u3068\u3092\u6570\u5b66\u7684\u306b\u4fdd\u8a3c\u3067\u304d\u307e\u305b\u3093\u3002 Similarly, [Nguyen et al., 2017] has further introduced an additional prior, which controls the semantic meaning of the synthesized image, to the adversarial generative network. \u540c\u69d8\u306b\u3001[Nguyen et al\u3002\u30012017]\u306f\u3055\u3089\u306b\u3001\u5408\u6210\u753b\u50cf\u306e\u610f\u5473\u7684\u610f\u5473\u3092\u6575\u5bfe\u7684\u306a\u751f\u6210\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306b\u5236\u5fa1\u3059\u308b\u8ffd\u52a0\u306e\u5148\u884c\u6280\u8853\u3092\u5c0e\u5165\u3057\u305f\u3002 We can use CNN feature maps as the prior for visualization. In addition, [Zhou et al., 2015] has proposed a method to accurately compute the image-resolution receptive field of neural activations in a feature map. The actual receptive field of neural activation is smaller than the theoretical receptive field computed using the filter size. \u6211\u3005\u306f\u3001\u8996\u899a\u5316\u306e\u305f\u3081\u306b\u4e8b\u524d\u306bCNN\u7279\u5fb4\u30de\u30c3\u30d7\u3092\u4f7f\u7528\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u3002 \u3055\u3089\u306b\u3001[Zhou et al\u3002\u30012015]\u306f\u3001\u7279\u5fb4\u30de\u30c3\u30d7\u5185\u306e\u795e\u7d4c\u6d3b\u52d5\u306e\u753b\u50cf\u89e3\u50cf\u5ea6\u53d7\u5bb9\u91ce\u3092\u6b63\u78ba\u306b\u8a08\u7b97\u3059\u308b\u65b9\u6cd5\u3092\u63d0\u6848\u3057\u3066\u3044\u308b\u3002 \u795e\u7d4c\u6d3b\u6027\u5316\u306e\u5b9f\u969b\u306e\u53d7\u5bb9\u91ce\u306f\u3001\u30d5\u30a3\u30eb\u30bf\u30fc\u30b5\u30a4\u30ba\u3092\u7528\u3044\u3066\u8a08\u7b97\u3055\u308c\u305f\u7406\u8ad6\u53d7\u5bb9\u91ce\u3088\u308a\u3082\u5c0f\u3055\u3044\u3002 The accurate estimation of the receptive field helps people to understand the representation of a filter. \u53d7\u5bb9\u91ce\u306e\u6b63\u78ba\u306a\u63a8\u5b9a\u306f\u3001\u4eba\u3005\u304c\u30d5\u30a3\u30eb\u30bf\u306e\u8868\u73fe\u3092\u7406\u89e3\u3059\u308b\u306e\u306b\u5f79\u7acb\u3061\u307e\u3059\u3002","title":"2 Visualization of CNN representations"},{"location":"DL/survey/Visual_Interpretability/#3-diagnosis-of-cnn-representations","text":"Some methods go beyond the visualization of CNNs and diagnose CNN representations to obtain insight understanding of features encoded in a CNN. We roughly divide all relevant research into the following five directions. \u3044\u304f\u3064\u304b\u306e\u65b9\u6cd5\u306f\u3001CNN\u306e\u8996\u899a\u5316\u3092\u8d85\u3048\u3001CNN\u8868\u73fe\u3092\u8a3a\u65ad\u3057\u3066\u3001CNN\u3067\u30b3\u30fc\u30c9\u5316\u3055\u308c\u305f\u7279\u5fb4\u306e\u7406\u89e3\u3092\u5f97\u308b\u3002 \u95a2\u9023\u3059\u308b\u3059\u3079\u3066\u306e\u7814\u7a76\u3092\u4ee5\u4e0b\u306e5\u3064\u306e\u65b9\u5411\u306b\u5927\u307e\u304b\u306b\u5206\u3051\u307e\u3059\u3002 Studies in the first direction analyze CNN features from a global view. [Szegedy et al., 2014] has explored semantic meanings of each filter. [Yosinski et al., 2014] has analyzed the transferability of filter representations in intermediate conv-layers. [Lu, 2015; Aubry and Russell, 2015] have computed feature distributions of different categories/attributes in the feature space of a pre-trained CNN. \u7b2c1\u306e\u65b9\u5411\u306e\u7814\u7a76\u306f\u3001\u30b0\u30ed\u30fc\u30d0\u30eb\u306a\u8996\u70b9\u304b\u3089CNN\u306e\u7279\u5fb4\u3092\u5206\u6790\u3059\u308b\u3002 [Szegedy et al\u3002\u30012014]\u306f\u3001\u5404\u30d5\u30a3\u30eb\u30bf\u306e\u610f\u5473\u8ad6\u7684\u610f\u5473\u3092\u63a2\u6c42\u3057\u3066\u3044\u308b\u3002 [Yosinski et al\u3002\u30012014]\u306f\u3001\u4e2d\u9593\u30b3\u30f3\u30d0\u30fc\u30b8\u30e7\u30f3\u5c64\u306b\u304a\u3051\u308b\u30d5\u30a3\u30eb\u30bf\u8868\u73fe\u306e\u4f1d\u9054\u53ef\u80fd\u6027\u3092\u5206\u6790\u3057\u305f\u3002 [Lu\u30012015; Aubry and Russell\u30012015]\u306f\u3001\u4e8b\u524d\u306b\u8a13\u7df4\u3055\u308c\u305fCNN\u306e\u7279\u5fb4\u7a7a\u9593\u306b\u304a\u3051\u308b\u7570\u306a\u308b\u30ab\u30c6\u30b4\u30ea/\u5c5e\u6027\u306e\u7279\u5fb4\u5206\u5e03\u3092\u8a08\u7b97\u3057\u305f\u3002 The second research direction extracts image regions that directly contribute the network output for a label/attribute to explain CNN representations of the label/attribute. This is similar to the visualization of CNNs. Methods of [Fong and Vedaldi, 2017; Selvaraju et al., 2017] have been proposed to propagate gradients of feature maps w.r.t. the final loss back to the image plane to estimate the image regions. The LIME model proposed in [Ribeiro et al., 2016] extracts image regions that are highly sensitive to the network output. \u7b2c2\u306e\u7814\u7a76\u65b9\u5411\u306f\u3001\u30e9\u30d9\u30eb/\u5c5e\u6027\u306eCNN\u8868\u73fe\u3092\u8aac\u660e\u3059\u308b\u305f\u3081\u306b\u30e9\u30d9\u30eb/\u5c5e\u6027\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u51fa\u529b\u306b\u76f4\u63a5\u5bc4\u4e0e\u3059\u308b\u753b\u50cf\u9818\u57df\u3092\u62bd\u51fa\u3059\u308b\u3002 \u3053\u308c\u306f\u3001CNN\u306e\u8996\u899a\u5316\u306b\u4f3c\u3066\u3044\u307e\u3059\u3002 [Fong and Vedaldi\u30012017; Selvaraju\u3089\u30012017] \u7279\u5fb4\u30de\u30c3\u30d7\u306e\u52fe\u914d\u3092\u4f1d\u64ad\u3059\u308b\u3053\u3068\u304c\u63d0\u6848\u3055\u308c\u3066\u3044\u308b\u3002 \u753b\u50cf\u9818\u57df\u3092\u63a8\u5b9a\u3059\u308b\u305f\u3081\u306b\u6700\u7d42\u7684\u306a\u640d\u5931\u3092\u753b\u50cf\u5e73\u9762\u306b\u623b\u3059\u3002 [Ribeiro et al\u3002\u30012016]\u3067\u63d0\u6848\u3055\u308c\u305fLIME\u30e2\u30c7\u30eb\u306f\u3001\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u51fa\u529b\u306b\u5bfe\u3057\u3066\u975e\u5e38\u306b\u654f\u611f\u306a\u753b\u50cf\u9818\u57df\u3092\u62bd\u51fa\u3059\u308b\u3002 Studies of [Zintgraf et al., 2017; Kindermans et al., 2017; Kumar et al., 2017] have invented methods to visualize areas in the input image that contribute the most to the decision-making process of the CNN. [Wang et al., 2017; Goyal et al., 2016] have tried to interpret the logic for visual question-answering encoded in neural networks. [Zintgraf et al\u3002\u30012017; Kindermans\u3089\u30012017; Kumar et al\u3002\u30012017]\u306f\u3001CNN\u306e\u610f\u601d\u6c7a\u5b9a\u30d7\u30ed\u30bb\u30b9\u306b\u6700\u3082\u5bc4\u4e0e\u3059\u308b\u5165\u529b\u753b\u50cf\u5185\u306e\u9818\u57df\u3092\u8996\u899a\u5316\u3059\u308b\u65b9\u6cd5\u3092\u767a\u660e\u3057\u305f\u3002 [Wang\u3089\u30012017; Goyal et al\u3002\u30012016]\u306f\u3001\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306b\u7b26\u53f7\u5316\u3055\u308c\u305f\u8996\u899a\u7684\u8cea\u554f\u5fdc\u7b54\u306e\u8ad6\u7406\u3092\u89e3\u91c8\u3057\u3088\u3046\u3068\u8a66\u307f\u305f\u3002 These studies list important objects (or regions of interests) detected from the images and crucial words in questions as the explanation of output answers. \u3053\u308c\u3089\u306e\u7814\u7a76\u306f\u3001\u753b\u50cf\u304b\u3089\u691c\u51fa\u3055\u308c\u305f\u91cd\u8981\u306a\u5bfe\u8c61\uff08\u307e\u305f\u306f\u95a2\u5fc3\u9818\u57df\uff09\u3068\u3001\u51fa\u529b\u56de\u7b54\u306e\u8aac\u660e\u3068\u3057\u3066\u306e\u8cea\u554f\u306e\u91cd\u8981\u306a\u5358\u8a9e\u3092\u30ea\u30b9\u30c8\u3057\u307e\u3059\u3002 The estimation of vulnerable points in the feature space of a CNN is also a popular direction for diagnosing network representations. CNN\u306e\u7279\u5fb4\u7a7a\u9593\u306b\u304a\u3051\u308b\u8106\u5f31\u306a\u70b9\u306e\u63a8\u5b9a\u306f\u3001\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u8868\u73fe\u3092\u8a3a\u65ad\u3059\u308b\u305f\u3081\u306e\u4e00\u822c\u7684\u306a\u65b9\u5411\u3067\u3082\u3042\u308b\u3002 Approaches of [Su et al., 2017; Koh and Liang, 2017; Szegedy et al., 2014] have been developed to compute adversarial samples for a CNN. I.e. these studies aim to estimate the minimum noisy perturbation of the input image that can change the final prediction. [Su\u3089\u30012017; Koh and Liang\u30012017; Szegedy et al\u3002\u30012014] \u306f\u3001CNN\u306e\u6575\u5bfe\u7684\u30b5\u30f3\u30d7\u30eb\u3092\u8a08\u7b97\u3059\u308b\u305f\u3081\u306b\u958b\u767a\u3055\u308c\u307e\u3057\u305f\u3002 \u3059\u306a\u308f\u3061\u3002 \u3053\u308c\u3089\u306e\u7814\u7a76\u306f\u3001\u5165\u529b\u753b\u50cf\u4e2d\u306b\u3069\u306e\u3088\u3046\u306aminumum noisy perturbation\u3092\u4e0e\u3048\u308b\u3068\u6700\u7d42\u7684\u306a\u4e88\u6e2c\u3092\u5909\u3048\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u304b\u63a8\u5b9a\u3059\u308b\u3053\u3068\u3092\u76ee\u7684\u3068\u3057\u3066\u3044\u308b\u3002 In particular, influence functions proposed in [Koh and Liang, 2017] can be used to compute adversarial samples. The influence function can also provide plausible ways to create training samples to attack the learning of CNNs, fix the training set, and further debug representations of a CNN. \u7279\u306b\u3001[Koh and Liang\u30012017]\u3067\u63d0\u6848\u3055\u308c\u305f\u5f71\u97ff\u95a2\u6570\u3092\u4f7f\u7528\u3057\u3066\u3001\u6575\u5bfe\u7684\u30b5\u30f3\u30d7\u30eb\u3092\u8a08\u7b97\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u3002 \u5f71\u97ff\u95a2\u6570\u306f\u3001CNN\u306e\u5b66\u7fd2\u3092\u653b\u6483\u3057\u3001\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u30bb\u30c3\u30c8\u3092\u4fee\u6b63\u3057\u3001\u3055\u3089\u306bCNN\u306e\u8868\u73fe\u3092\u30c7\u30d0\u30c3\u30b0\u3059\u308b\u305f\u3081\u306e\u8a13\u7df4\u30b5\u30f3\u30d7\u30eb\u3092\u4f5c\u6210\u3059\u308b\u305f\u3081\u306e\u3082\u3063\u3068\u3082\u3089\u3057\u3044\u65b9\u6cd5\u3092\u63d0\u4f9b\u3059\u308b\u3053\u3068\u3082\u3067\u304d\u308b\u3002 The fourth research direction is to refine network representations based on the analysis of network feature spaces. \u7b2c4\u306e\u7814\u7a76\u65b9\u5411\u306f\u3001\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u7279\u5fb4\u7a7a\u9593\u306e\u5206\u6790\u306b\u57fa\u3065\u3044\u3066\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u8868\u73fe\u3092\u6539\u826f\u3059\u308b\u3053\u3068\u3067\u3042\u308b\u3002 Given a CNN pre-trained for object classification, [Lakkaraju et al., 2017] has proposed a method to discover knowledge blind spots (unknown patterns) of the CNN in a weakly-supervised manner. \u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u5206\u985e\u306e\u305f\u3081\u306b\u4e8b\u524d\u306b\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u3055\u308c\u305fCNN\u304c\u4e0e\u3048\u3089\u308c\u305f\u5834\u5408\u3001[Lakkaraju et al\u3002\u30012017]\u306fCNN\u306e\u77e5\u8b58\u306e\u76f2\u70b9\uff08\u672a\u77e5\u306e\u30d1\u30bf\u30fc\u30f3\uff09\u3092\u5f31\u304f\u76e3\u8996\u3057\u305f\u65b9\u6cd5\u3067\u767a\u898b\u3059\u308b\u65b9\u6cd5\u3092\u63d0\u6848\u3057\u3066\u3044\u308b\u3002 This method grouped all sample points in the entire feature space of a CNN into thousands of pseudo-categories. It assumed that a well learned CNN would use the sub-space of each pseudo-category to exclusively represent a subset of a specific object class. \u3053\u306e\u65b9\u6cd5\u306f\u3001CNN\u306e\u7279\u5fb4\u7a7a\u9593\u5168\u4f53\u306e\u3059\u3079\u3066\u306e\u30b5\u30f3\u30d7\u30eb\u70b9\u3092\u6570\u5343\u306e\u64ec\u4f3c\u30ab\u30c6\u30b4\u30ea\u306b\u5206\u985e\u3059\u308b\u3002 \u3088\u304f\u5b66\u7fd2\u3055\u308c\u305fCNN\u306f\u3001\u7279\u5b9a\u306e\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u30af\u30e9\u30b9\u306e\u30b5\u30d6\u30bb\u30c3\u30c8\u3092\u6392\u4ed6\u7684\u306b\u8868\u3059\u305f\u3081\u306b\u5404\u7591\u4f3c\u30ab\u30c6\u30b4\u30ea\u306e\u90e8\u5206\u7a7a\u9593\u3092\u4f7f\u7528\u3059\u308b\u3068\u3044\u3046\u3053\u3068\u304c\u8003\u3048\u3089\u308c\u308b\u3002 In this way, this study randomly showed object samples within each sub-space, and used the sample purity in the sub-space to discover potential representation flaws hidden in a pre-trained CNN. \u3053\u306e\u3088\u3046\u306b\u3001\u3053\u306e\u7814\u7a76\u3067\u306f\u5404\u90e8\u5206\u7a7a\u9593\u5185\u306b\u5bfe\u8c61\u30b5\u30f3\u30d7\u30eb\u3092\u30e9\u30f3\u30c0\u30e0\u306b\u8868\u793a\u3057\u3001\u90e8\u5206\u7a7a\u9593\u5185\u306esample purity\u3092\u4f7f\u7528\u3057\u3066\u3001\u4e8b\u524d\u306b\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u3055\u308c\u305fCNN\u306b\u96a0\u308c\u305f\u8868\u73fe\u4e0a\u306e\u6b20\u9665\u3092\u767a\u898b\u3059\u308b\u3002 To distill representations of a teacher network to a student network for sentiment analysis, [Hu et al., 2016] has proposed using logic rules of natural languages (e.g. I-ORG cannot follow B-PER) to construct a distillation loss to supervise the knowledge distillation of neural networks, in order to obtain more meaningful network representations. \u611f\u60c5\u5206\u6790\u306e\u305f\u3081\u306b\u6559\u5e2b\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u8868\u73fe\u3092\u751f\u5f92\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306b\u5f15\u304d\u51fa\u3059\u305f\u3081\u306b\u3001[Hu et al\u3002\u30012016]\u306f\u81ea\u7136\u8a00\u8a9e\u306e\u8ad6\u7406\u898f\u5247\u3092\u4f7f\u7528\u3059\u308b\u3053\u3068\u3092\u63d0\u6848\u3057\u3066\u3044\u308b\uff08\u4f8b\u3048\u3070I-ORG\u306fB-PER\u306b\u5f93\u3046\u3053\u3068\u304c\u3067\u304d\u306a\u3044\uff09 \u3088\u308a\u610f\u5473\u306e\u3042\u308b\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u5f97\u308b\u305f\u3081\u306b\u3001\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u77e5\u8b58\u84b8\u7559 \u8868\u73fe\u3002 Finally, [Zhang et al., 2018b] has presented a method to discover potential, biased representations of a CNN. Fig. 1 shows biased representations of a CNN trained for the estimation of face attributes. When an attribute usually co-appears with specific visual features in training images, then the CNN may use such co-appearing features to represent the attribute. \u6700\u5f8c\u306b\u3001[Zhang et al 2018b]\u306f\u3001CNN\u306e\u6f5c\u5728\u7684\u3067\u504f\u898b\u306e\u3042\u308b\u8868\u73fe\u3092\u767a\u898b\u3059\u308b\u65b9\u6cd5\u3092\u63d0\u793a\u3057\u305f\u3002 \u56f31\u306f\u3001\u9854\u5c5e\u6027\u306e\u63a8\u5b9a\u306e\u305f\u3081\u306b\u8a13\u7df4\u3055\u308c\u305fCNN\u306e\u30d0\u30a4\u30a2\u30b9\u3055\u308c\u305f\u8868\u73fe\u3092\u793a\u3059\u3002 \u5c5e\u6027\u304c\u3001\u901a\u5e38\u3001\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u753b\u50cf\u5185\u306e\u7279\u5b9a\u306e\u8996\u899a\u7684\u7279\u5fb4\u3068\u540c\u6642\u306b\u73fe\u308c\u308b\u5834\u5408\u3001CNN\u306f\u3001\u305d\u306e\u3088\u3046\u306a\u5171\u6f14\u3059\u308b\u7279\u5fb4\u3092\u4f7f\u7528\u3057\u3066\u3001\u305d\u306e\u5c5e\u6027\u3092\u8868\u3059\u3053\u3068\u304c\u3067\u304d\u308b\u3002 Zhang Q, Cao R, Shi F, et al., 2018b. Interpreting CNN knowledge via an explanatory graph. Proc 32nd AAAI Conf on Artificial Intelligence, p.2124-2132. When the used co-appearing features are not semantically related to the target attribute, these features can be considered as biased representations. Given a pre-trained CNN (e.g. a CNN that was trained to estimate face attributes), [Zhang et al., 2018b] required people to annotate some ground-truth relationships between attributes, e.g. the lipstick attribute is positively related to the heavy-makeup attribute, and is not related to the black hair attribute. \u4f7f\u7528\u3055\u308c\u305f\u5171\u51fa\u73fe\u7279\u5fb4\u304c\u30bf\u30fc\u30b2\u30c3\u30c8\u5c5e\u6027\u306b\u610f\u5473\u7684\u306b\u95a2\u9023\u3057\u3066\u3044\u306a\u3044\u5834\u5408\u3001\u3053\u308c\u3089\u306e\u7279\u5fb4\u306f\u30d0\u30a4\u30a2\u30b9\u3055\u308c\u305f\u8868\u73fe\u3068\u898b\u306a\u3059\u3053\u3068\u304c\u3067\u304d\u308b\u3002 \u4e8b\u524d\u306b\u8a13\u7df4\u3055\u308c\u305fCNN\uff08\u4f8b\u3048\u3070\u3001\u9854\u5c5e\u6027\u3092\u63a8\u5b9a\u3059\u308b\u3088\u3046\u306b\u8a13\u7df4\u3055\u308c\u305fCNN\uff09\u304c\u4e0e\u3048\u3089\u308c\u305f\u5834\u5408\u3001[Zhang et al 2018b]\u306f\u3001\u4eba\u3005\u306b\u5c5e\u6027\u9593\u306e\u3044\u304f\u3064\u304b\u306e\u771f\u5b9f\u306e\u95a2\u4fc2\u3092\u6ce8\u91c8\u3059\u308b\u5fc5\u8981\u304c\u3042\u3063\u305f\u3002 \u53e3\u7d05\u306e\u5c5e\u6027\u306f\u91cd\u3044\u5316\u7ca7\u306e\u5c5e\u6027\u306b\u7a4d\u6975\u7684\u306b\u95a2\u9023\u3057\u3001\u9ed2\u9aea\u306e\u5c5e\u6027\u306b\u306f\u95a2\u4fc2\u3057\u307e\u305b\u3093\u3002 Then, the method mined inference patterns of each attribute output from conv-layers, and used inference patterns to compute actual attribute relationships encoded in the CNN. \u6b21\u306b\u3001conv-layers\u304b\u3089\u51fa\u529b\u3055\u308c\u305f\u5404\u5c5e\u6027\u306e\u63a8\u8ad6\u30d1\u30bf\u30fc\u30f3\u3092\u30de\u30a4\u30cb\u30f3\u30b0\u3057\u3001\u63a8\u8ad6\u30d1\u30bf\u30fc\u30f3\u3092\u4f7f\u7528\u3057\u3066CNN\u3067\u30a8\u30f3\u30b3\u30fc\u30c9\u3055\u308c\u305f\u5b9f\u969b\u306e\u5c5e\u6027\u95a2\u4fc2\u3092\u8a08\u7b97\u3057\u307e\u3059\u3002 Conflicts between the ground-truth and the mined attribute relationships indicated biased representations. \u5730\u4e0a\u771f\u7406\u3068\u63a1\u6398\u3055\u308c\u305f\u5c5e\u6027\u95a2\u4fc2\u3068\u306e\u9593\u306e\u885d\u7a81\u306f\u3001\u504f\u3063\u305f\u8868\u73fe\u3092\u793a\u3057\u305f\u3002","title":"3 Diagnosis of CNN representations"},{"location":"DL/survey/Visual_Interpretability/#4-disentangling-cnn-representations-into-explanatory-graphs-decision-trees","text":"\u5224\u65ad\u6839\u62e0\u3068\u306a\u308a\u3046\u308b\u8907\u6570\u306e\u30d1\u30bf\u30fc\u30f3\u306e\u5171\u8d77\u95a2\u4fc2\u3092\u660e\u3089\u304b\u306b\u3059\u308b\u624b\u6cd5","title":"4 Disentangling CNN representations into explanatory graphs &amp; decision trees"},{"location":"DL/survey/Visual_Interpretability/#41-disentangling-cnn-representations-into-explanatory-graphs","text":"\u8b58\u5225\u7406\u7531\u3067\u306f\u306a\u304fCNN\u306e\u5185\u90e8\u8868\u73fe\u3092\u7406\u89e3\u3059\u308b\u65b9\u6cd5(\u3082\u3061\u308d\u3093\u3001\u9593\u63a5\u7684\u306b\u8b58\u5225\u7406\u7531\u306e\u7406\u89e3\u306b\u3082\u3064\u306a\u304c\u308a\u307e\u3059) Compared with the visualization and diagnosis of network representations in previous sections, disentangling CNN features into human-interpretable graphical representations (namely explanatory graphs) provides a more thorough explanation of network representations. \u524d\u7bc0\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u8868\u73fe\u306e\u8996\u899a\u5316\u3068\u8a3a\u65ad\u3068\u6bd4\u8f03\u3057\u3066\u3001CNN\u6a5f\u80fd\u3092\u4eba\u9593\u304c\u89e3\u91c8\u53ef\u80fd\u306a\u30b0\u30e9\u30d5\u30a3\u30ab\u30eb\u306a\u8868\u73fe\uff08\u3059\u306a\u308f\u3061\u3001\u8aac\u660e\u7684\u306a\u30b0\u30e9\u30d5\uff09\u306b\u89e3\u304d\u307b\u3050\u3059\u3053\u3068\u306b\u3088\u308a\u3001\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u8868\u73fe\u306e\u3088\u308a\u5b8c\u5168\u306a\u8aac\u660e\u304c\u63d0\u4f9b\u3055\u308c\u308b\u3002 [Zhang et al., 2018a; Zhang et al.,2016] have proposed disentangling features in conv-layers of a pre-trained CNN and have used a graphical model to represent the semantic hierarchy hidden inside a CNN. [Zhang\u3089\u30012018a; Zhang et al\u3002\u30012016]\u306f\u3001\u4e8b\u524d\u306b\u8a13\u7df4\u3055\u308c\u305fCNN\u306econv-layers\u306e\u660e\u5feb\u306a\u7279\u5fb4\u3092\u63d0\u6848\u3057\u3001\u30b0\u30e9\u30d5\u30a3\u30ab\u30eb\u30e2\u30c7\u30eb\u3092\u4f7f\u7528\u3057\u3066CNN\u5185\u306b\u96a0\u3055\u308c\u305f\u610f\u5473\u968e\u5c64\u3092\u8868\u73fe\u3057\u3066\u3044\u308b\u3002 As shown in Fig. 2, each filter in a high conv-layer of a CNN usually represents a mixture of patterns. For example, the filter may be activated by both the head and the tail parts of an object. Thus, to provide a global view of how visual knowledge is organized in a pre-trained CNN, studies of [Zhang et al., 2018a; Zhang et al., 2016] aim to answer the following three questions. \u56f32\u306b\u793a\u3059\u3088\u3046\u306b\u3001CNN\u306ehigh conv-layer\u306e\u5404\u30d5\u30a3\u30eb\u30bf\u306f\u3001\u901a\u5e38\u3001\u30d1\u30bf\u30fc\u30f3\u306e\u6df7\u5408\u3092\u8868\u3059\u3002 \u4f8b\u3048\u3070\u3001\u30d5\u30a3\u30eb\u30bf\u306f\u3001\u7269\u4f53\u306e\u982d\u90e8\u53ca\u3073\u5c3e\u90e8\u306e\u4e21\u65b9\u306b\u3088\u3063\u3066\u4f5c\u52d5\u3055\u305b\u3089\u308c\u308b\u3002 \u3057\u305f\u304c\u3063\u3066\u3001\u4e8b\u524d\u306b\u8a13\u7df4\u3055\u308c\u305fCNN\u3067\u3069\u306e\u3088\u3046\u306b\u8996\u899a\u77e5\u8b58\u304c\u7d44\u7e54\u5316\u3055\u308c\u3066\u3044\u308b\u304b\u3092\u5168\u4f53\u7684\u306b\u898b\u308b\u305f\u3081\u306b\u3001[Zhang et al\u3002\u30012018a; Zhang et al\u3002\u30012016]\u306f\u3001\u4ee5\u4e0b\u306e3\u3064\u306e\u8cea\u554f\u306b\u7b54\u3048\u308b\u3053\u3068\u3092\u76ee\u6307\u3057\u3066\u3044\u308b Zhang Q, Cao R, Wu YN, et al., 2016. Growing interpretable part graphs on convnets via multi-shot learning. Proc 30th AAAI Conf on Artificial Intelligence, p.2898-2906. Zhang Q, Wang W, Zhu SC, 2018a. Examining CNN representations with respect to dataset bias. Proc 32nd AAAI Conf on Artificial Intelligence, in press How many types of visual patterns are memorized by each convolutional filter of the CNN (here, a visual pat tern may describe a specific object part or a certain texture)? Which patterns are co-activated to describe an object part? What is the spatial relationship between two co-activated patterns? CNN\u306e\u5404\u7573\u307f\u8fbc\u307f\u30d5\u30a3\u30eb\u30bf\u306b\u3088\u3063\u3066\u4f55\u7a2e\u985e\u306e\u30d3\u30b8\u30e5\u30a2\u30eb\u30d1\u30bf\u30fc\u30f3\u304c\u8a18\u61b6\u3055\u308c\u3066\u3044\u308b\u304b\uff08\u3053\u3053\u3067\u3001\u30d3\u30b8\u30e5\u30a2\u30eb\u30d1\u30bf\u30fc\u30f3\u306f\u7279\u5b9a\u306e\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u90e8\u5206\u307e\u305f\u306f\u7279\u5b9a\u306e\u30c6\u30af\u30b9\u30c1\u30e3\u3092\u8a18\u8ff0\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u308b\uff09 \u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u90e8\u5206\u3092\u8a18\u8ff0\u3059\u308b\u305f\u3081\u306b\u3069\u306e\u30d1\u30bf\u30fc\u30f3\u304c\u5171\u306b\u8d77\u52d5\u3055\u308c\u3066\u3044\u307e\u3059\u304b\uff1f 2\u3064\u306e\u5171\u6d3b\u6027\u5316\u30d1\u30bf\u30fc\u30f3\u9593\u306e\u7a7a\u9593\u7684\u95a2\u4fc2\u306f\u4f55\u3067\u3059\u304b\uff1f As shown in Fig. 3, the explanatory graph explains the knowledge semantic hidden inside the CNN. The explanatory graph disentangles the mixture of part patterns in each filter\u2019s feature map of a conv-layer, and uses each graph node to represent a part. \u56f33\u306b\u793a\u3059\u3088\u3046\u306b\u3001\u8aac\u660e\u30b0\u30e9\u30d5\u306f\u3001CNN\u5185\u306b\u96a0\u3055\u308c\u305f\u77e5\u8b58\u610f\u5473\u3092\u8aac\u660e\u3057\u3066\u3044\u308b\u3002 \u8aac\u660e\u30b0\u30e9\u30d5\u306f\u3001\u5404\u30d5\u30a3\u30eb\u30bf\u306econv-layer\u306e\u30d5\u30a3\u30fc\u30c1\u30e3\u30de\u30c3\u30d7\u5185\u306e\u30d1\u30fc\u30c4\u30d1\u30bf\u30fc\u30f3\u306e\u6df7\u5408\u3092\u89e3\u304d\u3001\u5404\u30b0\u30e9\u30d5\u30ce\u30fc\u30c9\u3092\u4f7f\u7528\u3057\u3066\u30d1\u30fc\u30c4\u3092\u8868\u3057\u307e\u3059\u3002 \u2022 The explanatory graph has multiple layers. Each graph layer corresponds to a specific conv-layer of a CNN. \u2022 Each filter in a conv-layer may represent the appearance of different object parts. The algorithm automatically disentangles the mixture of part patterns encoded in a single filter, and uses a node in the explanatory graph to represent each part pattern. \u8aac\u660e\u30b0\u30e9\u30d5\u306b\u306f\u8907\u6570\u306e\u30ec\u30a4\u30e4\u30fc\u304c\u3042\u308a\u307e\u3059\u3002 \u5404\u30b0\u30e9\u30d5\u5c64\u306f\u3001CNN\u306e\u7279\u5b9a\u306e\u30b3\u30f3\u30d0\u30ec\u30a4\u5c64\u306b\u5bfe\u5fdc\u3059\u308b\u3002 conv-layer\u306e\u5404\u30d5\u30a3\u30eb\u30bf\u306f\u3001\u7570\u306a\u308b\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u90e8\u5206\u306e\u5916\u89b3\u3092\u8868\u3059\u3053\u3068\u304c\u3042\u308a\u307e\u3059\u3002 \u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306f\u3001\u5358\u4e00\u306e\u30d5\u30a3\u30eb\u30bf\u306b\u7b26\u53f7\u5316\u3055\u308c\u305f\u90e8\u5206\u30d1\u30bf\u30fc\u30f3\u306e\u6df7\u5408\u3092\u81ea\u52d5\u7684\u306b\u89e3\u304d\u3001\u8aac\u660e\u30b0\u30e9\u30d5\u306e\u30ce\u30fc\u30c9\u3092\u4f7f\u7528\u3057\u3066\u5404\u90e8\u5206\u30d1\u30bf\u30fc\u30f3\u3092\u8868\u3059\u3002 \u2022 Each node in the explanatory graph consistently represents the same object part through different images. We can use the node to localize the corresponding part on the input image. To some extent, the node is robust to shape deformation and pose variations. \u8aac\u660e\u30b0\u30e9\u30d5\u306e\u5404\u30ce\u30fc\u30c9\u306f\u3001\u4e00\u8cab\u3057\u3066\u3001\u7570\u306a\u308b\u753b\u50cf\u3092\u4ecb\u3057\u3066\u540c\u3058\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u90e8\u5206\u3092\u8868\u3059\u3002 \u3053\u306e\u30ce\u30fc\u30c9\u3092\u4f7f\u7528\u3057\u3066\u3001\u5165\u529b\u753b\u50cf\u4e0a\u306e\u5bfe\u5fdc\u3059\u308b\u90e8\u5206\u3092\u30ed\u30fc\u30ab\u30e9\u30a4\u30ba\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002 \u3042\u308b\u7a0b\u5ea6\u307e\u3067\u3001\u30ce\u30fc\u30c9\u306f\u3001\u5f62\u72b6\u5909\u5f62\u304a\u3088\u3073\u59ff\u52e2\u5909\u52d5\u306b\u5bfe\u3057\u3066\u30ed\u30d0\u30b9\u30c8\u3067\u3042\u308b\u3002 \u2022 Each edge encodes the co-activation relationship and the spatial relationship between two nodes in adjacent layers. \u5404\u30a8\u30c3\u30b8\u306f\u3001\u5171\u6d3b\u6027\u5316\u95a2\u4fc2\u304a\u3088\u3073\u96a3\u63a5\u3059\u308b\u5c64\u5185\u306e2\u3064\u306e\u30ce\u30fc\u30c9\u9593\u306e\u7a7a\u9593\u7684\u95a2\u4fc2\u3092\u7b26\u53f7\u5316\u3059\u308b\u3002 \u2022 We can regard an explanatory graph as a compression of feature maps of conv-layers. A CNN has multiple convlayers. \u6211\u3005\u306f\u3001\u8aac\u660e\u30b0\u30e9\u30d5\u3092conv-layers\u306e\u7279\u5fb4\u30de\u30c3\u30d7\u306e\u5727\u7e2e\u3068\u898b\u306a\u3059\u3053\u3068\u304c\u3067\u304d\u308b\u3002 CNN\u306b\u306f\u8907\u6570\u306econvlayers\u304c\u3042\u308b\u3002 Each conv-layer may have hundreds of filters, and each filter may produce a feature map with hundreds of neural units. We can use tens of thousands of nodes in the explanatory graph to represent information contained in all tens of millions of neural units in these feature maps, i.e. by which part patterns the feature maps are activated, and where the part patterns are localized in input images. \u5404conv-layer\u306f\u4f55\u767e\u3082\u306e\u30d5\u30a3\u30eb\u30bf\u3092\u6301\u3061\u3001\u5404\u30d5\u30a3\u30eb\u30bf\u306f\u4f55\u767e\u3082\u306e\u795e\u7d4c\u5358\u4f4d\u3092\u6301\u3064\u30d5\u30a3\u30fc\u30c1\u30e3\u30de\u30c3\u30d7\u3092\u751f\u6210\u3057\u307e\u3059\u3002 \u8aac\u660e\u30b0\u30e9\u30d5\u306e\u4f55\u4e07\u3082\u306e\u30ce\u30fc\u30c9\u3092\u4f7f\u7528\u3057\u3066\u3001\u3053\u308c\u3089\u306e\u7279\u5fb4\u30de\u30c3\u30d7\u5185\u306e\u3059\u3079\u3066\u306e\u6570\u5343\u4e07\u306e\u795e\u7d4c\u5358\u4f4d\u306b\u542b\u307e\u308c\u308b\u60c5\u5831\u3001\u3059\u306a\u308f\u3061\u3001\u7279\u5fb4\u30de\u30c3\u30d7\u304c\u3069\u306e\u90e8\u5206\u30d1\u30bf\u30fc\u30f3\u3092\u6d3b\u6027\u5316\u3057\u3001\u90e8\u5206\u30d1\u30bf\u30fc\u30f3\u304c\u5165\u529b\u753b\u50cf\u306b\u5c40\u5728\u3059\u308b\u304b \u3002 \u2022 Just like a dictionary, each input image can only trigger a small subset of part patterns (nodes) in the explanatory graph. \u8f9e\u66f8\u3068\u540c\u69d8\u306b\u3001\u5404\u5165\u529b\u753b\u50cf\u306f\u8aac\u660e\u30b0\u30e9\u30d5\u306e\u90e8\u5206\u30d1\u30bf\u30fc\u30f3\uff08\u30ce\u30fc\u30c9\uff09\u306e\u5c0f\u3055\u306a\u30b5\u30d6\u30bb\u30c3\u30c8\u3092\u30c8\u30ea\u30ac\u3059\u308b\u3060\u3051\u3067\u3059\u3002 Each node describes a common part pattern with high transferability, which is shared by hundreds or thousands of training images. \u5404\u30ce\u30fc\u30c9\u306f\u3001\u6570\u767e\u307e\u305f\u306f\u6570\u5343\u306e\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u753b\u50cf\u306b\u3088\u3063\u3066\u5171\u6709\u3055\u308c\u308b\u9ad8\u3044\u53ef\u642c\u6027\u3092\u5099\u3048\u305f\u5171\u901a\u90e8\u54c1\u30d1\u30bf\u30fc\u30f3\u3092\u8a18\u8ff0\u3057\u307e\u3059\u3002 Fig. 4 lists top-ranked image patches corresponding to different nodes in the explanatory graph. Fig. 5 visualizes the spatial distribution of object parts inferred by the top 50% nodes in the L-th layer of the explanatory graph with the highest inference scores. Fig. 6 shows object parts inferred by a single node. \u56f34\u306f\u3001\u8aac\u660e\u30b0\u30e9\u30d5\u4e2d\u306e\u7570\u306a\u308b\u30ce\u30fc\u30c9\u306b\u5bfe\u5fdc\u3059\u308b\u6700\u4e0a\u4f4d\u753b\u50cf\u30d1\u30c3\u30c1\u3092\u30ea\u30b9\u30c8\u30a2\u30c3\u30d7\u3057\u305f\u3082\u306e\u3067\u3042\u308b\u3002 \u56f35\u306f\u3001\u63a8\u8ad6\u30b9\u30b3\u30a2\u304c\u6700\u3082\u9ad8\u3044\u8aac\u660e\u30b0\u30e9\u30d5\u306eL\u756a\u76ee\u306e\u5c64\u306e\u4e0a\u4f4d50\uff05\u30ce\u30fc\u30c9\u306b\u3088\u3063\u3066\u63a8\u8ad6\u3055\u308c\u305f\u5bfe\u8c61\u90e8\u54c1\u306e\u7a7a\u9593\u5206\u5e03\u3092\u8996\u899a\u5316\u3057\u305f\u3082\u306e\u3067\u3042\u308b\u3002 \u56f36\u306f\u3001\u5358\u4e00\u306e\u30ce\u30fc\u30c9\u306b\u3088\u3063\u3066\u63a8\u8ad6\u3055\u308c\u308b\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u90e8\u5206\u3092\u793a\u3059\u3002 Application: multi-shot part localization There are many potential applications based on the explanatory graph. \u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\uff1a\u30de\u30eb\u30c1\u30b7\u30e7\u30c3\u30c8\u90e8\u5206\u306e\u30ed\u30fc\u30ab\u30ea\u30bc\u30fc\u30b7\u30e7\u30f3\u8aac\u660e\u30b0\u30e9\u30d5\u306b\u57fa\u3065\u3044\u3066\u591a\u304f\u306e\u6f5c\u5728\u7684\u306a\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u304c\u3042\u308a\u307e\u3059\u3002 For example, we can regard the explanatory graph as a visual dictionary of a category and transfer graph nodes to other applications, such as multi-shot part localization. \u4f8b\u3048\u3070\u3001\u8aac\u660e\u30b0\u30e9\u30d5\u3092\u30ab\u30c6\u30b4\u30ea\u306e\u8996\u899a\u7684\u8f9e\u66f8\u3068\u898b\u306a\u3057\u3066\u3001\u30b0\u30e9\u30d5\u30ce\u30fc\u30c9\u3092multi-shot part localization\u306a\u3069\u306e\u4ed6\u306e\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u306b\u8ee2\u9001\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002 Given very few bounding boxes of an object part, [Zhang et al., 2018a] has proposed retrieving hundreds of nodes that are related to the part annotations from the explanatory graph, and then use the retrieved nodes to localize object parts in previously unseen images. \u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u90e8\u5206\u306e\u5883\u754c\u30dc\u30c3\u30af\u30b9\u306e\u6570\u304c\u975e\u5e38\u306b\u5c11\u306a\u3044\u305f\u3081\u3001[Zhang et al\u3002\u30012018a]\u306f\u6570\u767e\u306e\u8aac\u660e\u30b0\u30e9\u30d5\u304b\u3089\u306e\u90e8\u54c1\u6ce8\u91c8\u306b\u95a2\u9023\u3057\u3066\u3044\u308b\u30ce\u30fc\u30c9\u3092\u691c\u7d22\u3059\u308b\u3053\u3068\u3092\u63d0\u6848\u3057\u3066\u3044\u308b \u6b21\u306b\u3001\u691c\u7d22\u3055\u308c\u305f\u30ce\u30fc\u30c9\u3092\u4f7f\u7528\u3057\u3066\u3001\u4ee5\u524d\u306b\u898b\u3048\u306a\u304b\u3063\u305f\u753b\u50cf\u5185\u306e\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u90e8\u54c1\u3092\u4f4d\u7f6e\u7279\u5b9a\u3059\u308b\u3002 Because each node in the explanatory graph encodes a part pattern shared by numerous training images, the retrieved nodes describe a general appearance of the target part without being over-fitted to the limited annotations of part bounding boxes. \u8aac\u660e\u30b0\u30e9\u30d5\u306e\u5404\u30ce\u30fc\u30c9\u306f\u3001\u591a\u6570\u306e\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u753b\u50cf\u306b\u3088\u3063\u3066\u5171\u6709\u3055\u308c\u308b\u30d1\u30fc\u30c4\u30d1\u30bf\u30fc\u30f3\u3092\u7b26\u53f7\u5316\u3059\u308b\u306e\u3067\u3001\u691c\u7d22\u3055\u308c\u305f\u30ce\u30fc\u30c9\u306f\u3001\u90e8\u54c1\u30d0\u30a6\u30f3\u30c7\u30a3\u30f3\u30b0\u30dc\u30c3\u30af\u30b9\u306e\u9650\u5b9a\u3055\u308c\u305f\u6ce8\u91c8\u306b\u3042\u307e\u308a\u9069\u5408\u3059\u308b\u3053\u3068\u306a\u304f\u3001\u30bf\u30fc\u30b2\u30c3\u30c8\u90e8\u54c1\u306e\u4e00\u822c\u7684\u306a\u5916\u89b3\u3092\u8a18\u8ff0\u3059\u308b\u3002 Given three annotations for each object part, the explanatory-graph-based method has exhibited superior performance of part localization and has decreased by about 1/3 localization errors w.r.t. the second-best baseline. \u5404\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u90e8\u5206\u306b\u3064\u3044\u30663\u3064\u306e\u6ce8\u91c8\u304c\u4e0e\u3048\u3089\u308c\u305f\u5834\u5408\u3001\u8aac\u660e\u30b0\u30e9\u30d5\u306b\u57fa\u3065\u304f\u65b9\u6cd5\u306f\u3001\u30d1\u30fc\u30c4\u30ed\u30fc\u30ab\u30ea\u30bc\u30fc\u30b7\u30e7\u30f3\u306e\u512a\u308c\u305f\u6027\u80fd\u3092\u793a\u3057\u3001\u7d041/3\u306e\u30ed\u30fc\u30ab\u30e9\u30a4\u30bc\u30fc\u30b7\u30e7\u30f3\u30a8\u30e9\u30fc\u3067\u6e1b\u5c11\u3057\u305f\u3002 2\u756a\u76ee\u306b\u826f\u3044\u30d9\u30fc\u30b9\u30e9\u30a4\u30f3\u3002","title":"4.1 Disentangling CNN representations into explanatory graphs"},{"location":"DL/survey/Visual_Interpretability/#42-disentangling-cnn-representations-into-decision-trees","text":"\u7279\u6b8a\u306a\u69cb\u9020\u306e\u8b58\u5225\u5668\u3092\u4f7f\u7528\u3059\u308b\u65b9\u6cd5(\u3053\u306e\u8a18\u4e8b\u3067\u306e\u305d\u306e\u4ed6) [Zhang et al., 2018c] has further proposed a decision tree to encode decision modes in fully-connected layers. The decision tree is not designed for classification. Instead, the decision tree is used to quantitatively explain the logic for each CNN prediction. I.e. given an input image, we use the CNN to make a prediction. The decision tree tells people which filters in a conv-layer are used for the prediction and how much they contribute to the prediction. As shown in Fig. 7, the method mines potential decision modes memorized in fully-connected layers. The decision tree organizes these potential decision modes in a coarseto-fine manner. Furthermore, this study uses the method of [Zhang et al., 2017c] to disentangle representations of filters in the top conv-layers, i.e. making each filter represent a specific object part. In this way, people can use the decision tree to explain rationales for each CNN prediction at the semantic level, i.e. which object parts are used by the CNN to make the prediction.","title":"4.2 Disentangling CNN representations into decision trees"},{"location":"DL/survey/Visual_Interpretability/#5-learning-neural-networks-with-interpretabledisentangled-representations","text":"Almost all methods mentioned in previous sections focus on the understanding of a pre-trained network. In this section, we review studies of learning disentangled representations of neural networks, where representations in middle layers are no longer a black box but have clear semantic meanings. Compared to the understanding of pre-trained networks, learning networks with disentangled representations present more challenges. Up to now, only a few studies have been published in this direction. 5.1 Interpretable convolutional neural networks As shown in Fig. 8, [Zhang et al., 2017c] has developed a method to modify an ordinary CNN to obtain disentangled representations in high conv-layers by adding a loss to each filter in the conv-layers. The loss is used to regularize the feature map towards the representation of a specific object part. Note that people do not need to annotate any object parts or textures to supervise the learning of interpretable CNNs. Instead, the loss automatically assigns an object part to each filter during the end-to-end learning process. As shown in Fig. 9, this method designs some templates. Each template T\u00b5i is a matrix with the same size of feature map. T\u00b5i describes the ideal distribution of activations for the feature map when the target part mainly triggers the i-th unit in the feature map. Given the joint probability of fitting a feature map to a template, the loss of a filter is formulated as the mutual information between the feature map and the templates. This loss encourages a low entropy of inter-category activations. I.e. each filter in the conv-layer is assigned to a certain category. If the input image belongs to the target category, then the loss expects the filter\u2019s feature map to match a template well; otherwise, the filter needs to remain inactivated. In addition, the loss also encourages a low entropy of spatial distributions of neural activations. I.e. when the input image belongs the target category, the feature map is supposed to exclusively fit a single template. In other words, the filter needs to activate a single location on the feature map. This study assumes that if a filter repetitively activates various feature-map regions, then this filter is more likely to describe low-level textures (e.g. colors and edges), instead of high-level parts. For example, the left eye and the right eye may be represented by different filters, because contexts of the two eyes are symmetric, but not the same. Fig.10 shows feature maps produced by different filters of an interpretable CNN. Each filter consistently represents the same object part through various images. 5.2 Interpretable R-CNN [Wu et al., 2017] has proposed the learning of qualitatively interpretable models for object detection based on the RCNN. The objective is to unfold latent configurations of object parts automatically during the object-detection process. This method is learned without using any part annotations for supervision. [Wu et al., 2017] uses a topdown hierarchical and compositional grammar, namely an And-Or graph (AOG), to model latent configurations of object parts. This method uses an AOG-based parsing operator to substitute for the RoI-Pooling operator used in the RCNN. The AOG-based parsing harnesses explainable compositional structures of objects and maintains the discrimination power of a R-CNN. This idea is related to the disentanglement of the local, bottom-up, and top-down information components for prediction [Wu et al., 2007; Yang et al., 2009; Wu and Zhu, 2011]. During the detection process, a bounding box is interpreted as the best parse tree derived from the AOG on-the-fly. During the learning process, a folding-unfolding method is used to train the AOG and R-CNN in an end-to-end manner. Fig. 11 illustrates an example of object detection. The proposed method detects object bounding boxes. The method also determines the latent parse tree and part configurations of objects as the qualitatively extractive rationale in detection. 5.3 Capsule networks [Sabour et al., 2017] has designed novel neural units, namely capsules, in order to substitute for traditional neural units to construct a capsule network. Each capsule outputs an activity vector instead of a scalar. The length of the activity vector represents the activation strength of the capsule, and the orientation of the activity vector encodes instantiation parameters. Active capsules in the lower layer send messages to capsules in the adjacent higher layer. This method uses an iterative routing-by-agreement mechanism to assign higher weights with the low-layer capsules whose outputs better fit the instantiation parameters of the high-layer capsule. Experiments showed that when people trained capsule networks using the MNIST dataset [LeCun et al., 1998b], a capsule encoded a specific semantic concept. Different dimensions of the activity vector of a capsule controlled different features, including 1) scale and thickness, 2) localized part, 3) stroke thickness, 3) localized skew, and 4) width and translation. 5.4 Information maximizing generative adversarial nets The information maximizing generative adversarial net [Chen et al., 2016], namely InfoGAN, is an extension of the generative adversarial network. The InfoGAN maximizes the mutual information between certain dimensions of the latent representation and the image observation. The InfoGAN separates input variables of the generator into two types, i.e. the incompressible noise z and the latent code c. This study aims to learn the latent code c to encode certain semantic concepts in an unsupervised manner. The InfoGAN has been trained using the MNIST dataset [LeCun et al., 1998b], the CelebA dataset [Liu et al., 2015], the SVHN dataset [Netzer et al., 2011], the 3D face dataset [Paysan et al., 2009], and the 3D chair dataset [Aubry et al., 2014]. Experiments have shown that the latent code has successfully encoded the digit type, the rotation, and the width of digits in the MNIST dataset, the lighting condition and the plate context in the SVHN dataset, the azimuth, the existence of glasses, the hairstyle, and the emotion in the CelebA dataset, and the width and 3D rotation in the 3D face and chair datasets.","title":"5 Learning neural networks with interpretable/disentangled representations"},{"location":"DL/survey/Visual_Interpretability/#6-evaluation-metrics-for-network-interpretability","text":"Evaluation metrics for model interpretability are crucial for the development of explainable models. \u30e2\u30c7\u30eb\u89e3\u91c8\u80fd\u529b\u306e\u8a55\u4fa1\u57fa\u6e96\u306f\u3001\u8aac\u660e\u53ef\u80fd\u306a\u30e2\u30c7\u30eb\u306e\u958b\u767a\u306b\u3068\u3063\u3066\u91cd\u8981\u3067\u3059\u3002 This is because unlike traditional well-defined visual applications (e.g. object detection and segmentation), network interpretability is more difficult to define and evaluate. The evaluation metric of network interpretability can help people define the concept of network interpretability and guide the development of learning interpretable network representations. \u3053\u308c\u306f\u3001\u5f93\u6765\u306e\u660e\u78ba\u306b\u5b9a\u7fa9\u3055\u308c\u305f\u30d3\u30b8\u30e5\u30a2\u30eb\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\uff08\u4f8b\u3048\u3070\u3001\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u691c\u51fa\u304a\u3088\u3073\u30bb\u30b0\u30e1\u30f3\u30c8\u5316\uff09\u3068\u306f\u7570\u306a\u308a\u3001\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u89e3\u91c8\u53ef\u80fd\u6027\u3092\u5b9a\u7fa9\u304a\u3088\u3073\u8a55\u4fa1\u3059\u308b\u3053\u3068\u304c\u3088\u308a\u56f0\u96e3\u3067\u3042\u308b\u305f\u3081\u3067\u3042\u308b\u3002 \u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u89e3\u91c8\u80fd\u529b\u306e\u8a55\u4fa1\u57fa\u6e96\u306f\u3001\u4eba\u3005\u304c\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u89e3\u91c8\u80fd\u529b\u306e\u6982\u5ff5\u3092\u5b9a\u7fa9\u3057\u3001\u89e3\u91c8\u53ef\u80fd\u306a\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u8868\u73fe\u3092\u5b66\u7fd2\u3059\u308b\u3053\u3068\u3092\u5c0e\u304f\u306e\u306b\u5f79\u7acb\u3064\u3002 Up to now, only very few studies have discussed the evaluation of network interpretability. Proposing a promising evaluation metric is still a big challenge to state-of-the-art algorithms. In this section, we simply introduce two latest evaluation metrics for the interpretability of CNN filters, i.e. the filter interpretability proposed by [Bau et al., 2017] and the location instability proposed by [Zhang et al., 2018a]. \u4eca\u307e\u3067\u306f\u3001\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u89e3\u91c8\u53ef\u80fd\u6027\u306e\u8a55\u4fa1\u306b\u3064\u3044\u3066\u8b70\u8ad6\u3055\u308c\u305f\u7814\u7a76\u306f\u3054\u304f\u308f\u305a\u304b\u3067\u3057\u305f\u3002 \u6709\u671b\u306a\u8a55\u4fa1\u57fa\u6e96\u3092\u63d0\u6848\u3059\u308b\u3053\u3068\u306f\u3001\u6700\u5148\u7aef\u306e\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306b\u3068\u3063\u3066\u4f9d\u7136\u3068\u3057\u3066\u5927\u304d\u306a\u8ab2\u984c\u3067\u3059\u3002 \u3053\u306e\u30bb\u30af\u30b7\u30e7\u30f3\u3067\u306f\u3001CNN\u30d5\u30a3\u30eb\u30bf\u306e\u89e3\u91c8\u53ef\u80fd\u6027\u306e\u305f\u3081\u306e2\u3064\u306e\u6700\u65b0\u8a55\u4fa1\u30e1\u30c8\u30ea\u30c3\u30af\u3001\u3059\u306a\u308f\u3061[Bau\u3089\u30012017]\u306b\u3088\u3063\u3066\u63d0\u6848\u3055\u308c\u305f\u30d5\u30a3\u30eb\u30bf\u306e\u89e3\u91c8\u53ef\u80fd\u6027\u3068[Zhang et al\u3002\u30012018a]\u306b\u3088\u3063\u3066\u63d0\u6848\u3055\u308c\u305f\u4f4d\u7f6e\u4e0d\u5b89\u5b9a\u6027\u3092\u7c21\u5358\u306b\u7d39\u4ecb\u3059\u308b\u3002","title":"6 Evaluation metrics for network interpretability"},{"location":"DL/survey/Visual_Interpretability/#61-filter-interpretability","text":"[Bau et al., 2017] has defined six types of semantics for CNN filters, i.e. objects, parts, scenes, textures, materials, and colors. The evaluation of filter interpretability requires people to annotate these six types of semantics on testing images at the pixel level. [Bau\u3089\u30012017]\u306f\u3001CNN\u30d5\u30a3\u30eb\u30bf\u3001\u3059\u306a\u308f\u3061\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3001\u30d1\u30fc\u30c4\u3001\u30b7\u30fc\u30f3\u3001\u30c6\u30af\u30b9\u30c1\u30e3\u3001\u30de\u30c6\u30ea\u30a2\u30eb\u3001\u304a\u3088\u3073\u30ab\u30e9\u30fc\u306e6\u7a2e\u985e\u306e\u30bb\u30de\u30f3\u30c6\u30a3\u30af\u30b9\u3092\u5b9a\u7fa9\u3057\u3066\u3044\u307e\u3059\u3002 \u30d5\u30a3\u30eb\u30bf\u306e\u89e3\u91c8\u80fd\u529b\u306e\u8a55\u4fa1\u3067\u306f\u3001\u30d4\u30af\u30bb\u30eb\u30ec\u30d9\u30eb\u3067\u753b\u50cf\u3092\u30c6\u30b9\u30c8\u3059\u308b\u969b\u306b\u3001\u3053\u308c\u3089\u306e6\u7a2e\u985e\u306e\u30bb\u30de\u30f3\u30c6\u30a3\u30af\u30b9\u306b\u6ce8\u91c8\u3092\u4ed8\u3051\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002 The evaluation metric measures the fitness between the image-resolution receptive field of a filter\u2019s neural activations1 and the pixel-level semantic annotations on the image. \u8a55\u4fa1\u30e1\u30c8\u30ea\u30c3\u30af\u306f\u3001\u30d5\u30a3\u30eb\u30bf\u306e\u795e\u7d4c\u6d3b\u6027\u5316\u306e\u753b\u50cf\u89e3\u50cf\u5ea6\u53d7\u5bb9\u91ce\u3068\u3001\u753b\u50cf\u4e0a\u306e\u30d4\u30af\u30bb\u30eb\u30ec\u30d9\u30eb\u306e\u610f\u5473\u6ce8\u91c8\u3068\u306e\u9069\u5408\u5ea6\u3092\u6e2c\u5b9a\u3059\u308b\u3002 For example, if the receptive field of a filter\u2019s neural activations usually highly overlaps with ground-truth image regions of a specific semantic concept through different images, then we can consider that the filter represents this semantic concept. \u4f8b\u3048\u3070\u3001\u30d5\u30a3\u30eb\u30bf\u306e\u795e\u7d4c\u6d3b\u52d5\u306e\u53d7\u5bb9\u5834\u304c\u3001\u901a\u5e38\u3001\u7570\u306a\u308b\u753b\u50cf\u3092\u901a\u3057\u3066\u7279\u5b9a\u306e\u610f\u5473\u6982\u5ff5\u306eground-truth\u753b\u50cf\u9818\u57df\u3068\u9ad8\u5ea6\u306b\u91cd\u306a\u308b\u5834\u5408\u3001\u30d5\u30a3\u30eb\u30bf\u304c\u3053\u306e\u610f\u5473\u6982\u5ff5\u3092\u8868\u3059\u3068\u8003\u3048\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u3002 For each filter f, this method computes its feature maps X = {x = f(I)|I \u2208 I} on different testing images. Then, the distribution of activation scores in all positions of all feature maps is computed. [Bau et al., 2017] set an activation threshold Tf such that p(xij > Tf ) = 0.005, to select top activations from all spatial locations [i, j] of all feature maps x \u2208 X as valid map regions corresponding to f\u2019s semantics. Then, the method scales up low-resolution valid map regions to the image resolution, thereby obtaining the receptive field of valid activations on each image. \u5404\u30d5\u30a3\u30eb\u30bff\u306b\u3064\u3044\u3066\u3001\u3053\u306e\u65b9\u6cd5\u306f\u7570\u306a\u308b\u30c6\u30b9\u30c8\u753b\u50cf\u4e0a\u3067\u305d\u306e\u7279\u5fb4\u30de\u30c3\u30d7X = {x = f\uff08I\uff09| I\u2208I}\u3092\u8a08\u7b97\u3059\u308b\u3002 \u6b21\u306b\u3001\u3059\u3079\u3066\u306e\u7279\u5fb4\u30de\u30c3\u30d7\u306e\u3059\u3079\u3066\u306e\u4f4d\u7f6e\u306b\u304a\u3051\u308b\u6d3b\u6027\u5316\u30b9\u30b3\u30a2\u306e\u5206\u5e03\u304c\u8a08\u7b97\u3055\u308c\u308b\u3002 [Bau\u3089\u30012017]\u306f\u3001\u3059\u3079\u3066\u306e\u7279\u5fb4\u30de\u30c3\u30d7x\u2208X\u306e\u3059\u3079\u3066\u306e\u7a7a\u9593\u4f4d\u7f6e[i\u3001j]\u304b\u3089\u306e\u30c8\u30c3\u30d7\u30a2\u30af\u30c6\u30a3\u30d9\u30fc\u30b7\u30e7\u30f3\u3092\u3001\u5bfe\u5fdc\u3059\u308b\u6709\u52b9\u306a\u30de\u30c3\u30d7\u9818\u57df\u3068\u3057\u3066\u9078\u629e\u3059\u308b\u305f\u3081\u306b\u3001p\uff08xij> Tf\uff09= 0.005\u3068\u306a\u308b\u3088\u3046\u306b\u3001 f\u306e\u30bb\u30de\u30f3\u30c6\u30a3\u30af\u30b9\u3002 \u6b21\u306b\u3001\u3053\u306e\u65b9\u6cd5\u306f\u3001\u4f4e\u89e3\u50cf\u5ea6\u306e\u6709\u52b9\u306a\u30de\u30c3\u30d7\u9818\u57df\u3092\u753b\u50cf\u89e3\u50cf\u5ea6\u306b\u30b9\u30b1\u30fc\u30eb\u30a2\u30c3\u30d7\u3057\u3001\u305d\u308c\u306b\u3088\u308a\u5404\u753b\u50cf\u4e0a\u306e\u6709\u52b9\u306a\u30a2\u30af\u30c1\u30d9\u30fc\u30b7\u30e7\u30f3\u306e\u53d7\u5bb9\u91ce\u3092\u5f97\u308b\u3002 We use S I f to denote the receptive field of f\u2019s valid activations w.r.t. the image I. The compatibility between a filter f and a specific semantic concept is reported as an intersection-over-union score IoUI f,k = kS I f \u2229S I k k kSI f \u222aSI k k , where S I k denotes the ground-truth mask of the k-th semantic concept on the image I. Given an image I, filter f is associated with the k-th concept if IoUI f,k > 0.04. The probability of the k-th concept being associated with the filter f is given as Pf,k = meanI:with k-th concept1(IoUI f,k > 0.04). Thus, we can use Pf,k to evaluate the filter interpretability of f.","title":"6.1 Filter interpretability"},{"location":"DL/survey/Visual_Interpretability/#62-location-instability","text":"Another evaluation metric is location instability. This metric is proposed by [Zhang et al., 2018a] to evaluate the fitness between a CNN filter and the representation of an object part. \u5225\u306e\u8a55\u4fa1\u57fa\u6e96\u306f\u3001\u4f4d\u7f6e\u4e0d\u5b89\u5b9a\u6027\u3067\u3042\u308b\u3002 \u3053\u306e\u30e1\u30c8\u30ea\u30c3\u30af\u306f\u3001[Zhang et al\u3002\u30012018a]\u306b\u3088\u3063\u3066\u3001CNN\u30d5\u30a3\u30eb\u30bf\u3068\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u90e8\u5206\u306e\u8868\u73fe\u3068\u306e\u9593\u306e\u9069\u5408\u5ea6\u3092\u8a55\u4fa1\u3059\u308b\u305f\u3081\u306b\u63d0\u6848\u3055\u308c\u3066\u3044\u308b\u3002 Given an input image I, the CNN computes a feature map x \u2208 R N\u00d7N of filter f. We can regard the unit xi,j (1 \u2264 i, j \u2264 N) with the highest activation as the location inference of f, where N \u00d7 N is referred to as the size of the feature map. We use p\u02c6 to denote the image position that corresponds to the inferred feature map location (i, j), i.e. the center of the unit xi,j \u2019s receptive field when we backward propagated the receptive field to the image plane. \u5165\u529b\u753b\u50cfI\u304c\u4e0e\u3048\u3089\u308c\u308b\u3068\u3001CNN\u306f\u30d5\u30a3\u30eb\u30bff\u306e\u7279\u5fb4\u30de\u30c3\u30d7x\u2208RN\u00d7N\u3092\u8a08\u7b97\u3059\u308b\u3002 \u6700\u3082\u9ad8\u3044\u6d3b\u6027\u5316\u3092\u6709\u3059\u308b\u30e6\u30cb\u30c3\u30c8xi\u3001j\uff081\u2266i\u3001j\u2266N\uff09\u3092f\u306e\u4f4d\u7f6e\u63a8\u8ad6\u3068\u3057\u3066\u8003\u3048\u308b\u3053\u3068\u304c\u3067\u304d\u3001N\u00d7N\u306f\u7279\u5fb4\u30de\u30c3\u30d7\u306e\u30b5\u30a4\u30ba\u3068\u547c\u3070\u308c\u308b\u3002 \u63a8\u5b9a\u3055\u308c\u305f\u7279\u5fb4\u30de\u30c3\u30d7\u4f4d\u7f6e\uff08i\u3001j\uff09\u306b\u5bfe\u5fdc\u3059\u308b\u753b\u50cf\u4f4d\u7f6e\u3001\u3059\u306a\u308f\u3061\u3001\u53d7\u5bb9\u91ce\u3092\u753b\u50cf\u5e73\u9762\u306b\u9006\u65b9\u5411\u306b\u4f1d\u64ad\u3055\u305b\u305f\u3068\u304d\u306e\u5358\u4f4dx i\u3001j\u306e\u53d7\u5bb9\u91ce\u306e\u4e2d\u5fc3\u3092\u8868\u3059\u305f\u3081\u306bp\u3092\u4f7f\u7528\u3059\u308b\u3002 The evaluation assumes that if f consistently represented the same object part (the object part may not have an explicit name according to people\u2019s cognition) through different objects, then distances between the image position p\u02c6 and some object landmarks should not change much among different objects. \u3053\u306e\u8a55\u4fa1\u3067\u306f\u3001f\u304c\u7570\u306a\u308b\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3092\u901a\u3057\u3066\u540c\u3058\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u90e8\u5206\uff08\u4eba\u306e\u8a8d\u77e5\u306b\u5fdc\u3058\u3066\u660e\u793a\u7684\u306a\u540d\u524d\u3092\u6301\u305f\u306a\u3044\u3053\u3068\u304c\u3042\u308b\uff09\u3092\u4e00\u8cab\u3057\u3066\u8868\u73fe\u3059\u308b\u3068\u3001\u753b\u50cf\u4f4d\u7f6ep\u3068\u3044\u304f\u3064\u304b\u306e\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u30e9\u30f3\u30c9\u30de\u30fc\u30af\u9593\u306e\u8ddd\u96e2\u306f\u3001\u7570\u306a\u308b\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u9593\u3067\u3042\u307e\u308a\u5909\u5316\u3057\u3066\u306f\u306a\u3089\u306a\u3044\u3082\u306e\u3068\u4eee\u5b9a\u3059\u308b\u3002 For example, if filter f represents the shoulder, then the distance between the shoulder and the head should remain stable through different objects. \u4f8b\u3048\u3070\u3001\u30d5\u30a3\u30eb\u30bff\u304c\u80a9\u90e8\u3092\u8868\u3059\u5834\u5408\u3001\u80a9\u90e8\u3068\u982d\u90e8\u3068\u306e\u9593\u306e\u8ddd\u96e2\u306f\u3001\u7570\u306a\u308b\u7269\u4f53\u306b\u3088\u3063\u3066\u5b89\u5b9a\u3057\u305f\u307e\u307e\u3067\u3042\u308b\u3079\u304d\u3067\u3042\u308b\u3002 Therefore, people can compute the deviation of the distance between the inferred position p\u02c6 and a specific groundtruth landmark among different images. The average deviation w.r.t. various landmarks can be used to evaluate the location \u3057\u305f\u304c\u3063\u3066\u3001\u4eba\u3005\u306f\u3001\u63a8\u6e2c\u3055\u308c\u305f\u4f4d\u7f6ep\u3068\u7570\u306a\u308b\u753b\u50cf\u9593\u306e\u7279\u5b9a\u306e\u5730\u4e0a\u306e\u30e9\u30f3\u30c9\u30de\u30fc\u30af\u3068\u306e\u9593\u306e\u8ddd\u96e2\u306e\u504f\u5dee\u3092\u8a08\u7b97\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u3002 \u5e73\u5747\u504f\u5deew.r.t. \u3055\u307e\u3056\u307e\u306a\u30e9\u30f3\u30c9\u30de\u30fc\u30af\u3092\u4f7f\u7528\u3057\u3066\u5834\u6240\u3092\u8a55\u4fa1\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059 instability of f. As shown in Fig. 12, let dI (pk, p\u02c6) = \u221a kpk\u2212p\u02c6k w2+h2 denote the normalized distance between the inferred part and the k-th landmark pk on image I. \u221a w2 + h2 dep notes the diagonal length of the input image. Thus, Df,k = varI [dI (pk, p\u02c6)] is reported as the relative location deviation of filter f w.r.t. the k-th landmark, where varI [dI (pk, p\u02c6)] is referred to as the variation of the distance dI (pk, p\u02c6). Because each landmark cannot appear in all testing images, for each filter f, the metric only uses inference results with the topM highest activation scores on images containing the k-th landmark to compute Df,k. In this way, the average of relative location deviations of all the filters in a conv-layer w.r.t. all landmarks, i.e. meanfmeanK k=1Df,k, measures the location instability of a CNN, where K denotes the number of landmarks. \u5165\u529b\u753b\u50cf\u306e\u5bfe\u89d2\u9577\u3055\u3092\u8a18\u9332\u3059\u308b\u3002 \u3057\u305f\u304c\u3063\u3066\u3001Df\u3001k = varI [dI\uff08pk\u3001p\uff09]\u306f\u3001\u30d5\u30a3\u30eb\u30bff w.r.t\u306e\u76f8\u5bfe\u4f4d\u7f6e\u504f\u5dee\u3068\u3057\u3066\u5831\u544a\u3055\u308c\u308b\u3002 \u3053\u3053\u3067\u3001varI [dI\uff08pk\u3001p\uff09]\u306f\u3001\u8ddd\u96e2dI\uff08pk\u3001p\uff09\u306e\u5909\u5316\u3068\u547c\u3070\u308c\u308bk\u756a\u76ee\u306e\u30e9\u30f3\u30c9\u30de\u30fc\u30af\u3067\u3042\u308b\u3002 \u5404\u30e9\u30f3\u30c9\u30de\u30fc\u30af\u306f\u3059\u3079\u3066\u306e\u30c6\u30b9\u30c8\u753b\u50cf\u306b\u8868\u793a\u3055\u308c\u306a\u3044\u305f\u3081\u3001\u5404\u30d5\u30a3\u30eb\u30bff\u306b\u3064\u3044\u3066\u3001Df\u3001k\u3092\u8a08\u7b97\u3059\u308b\u305f\u3081\u306ek\u756a\u76ee\u306e\u30e9\u30f3\u30c9\u30de\u30fc\u30af\u3092\u542b\u3080\u753b\u50cf\u4e0a\u3067topM\u306e\u6700\u3082\u9ad8\u3044\u6d3b\u6027\u5316\u30b9\u30b3\u30a2\u3067\u63a8\u8ad6\u7d50\u679c\u306e\u307f\u304c\u4f7f\u7528\u3055\u308c\u307e\u3059\u3002 \u3053\u306e\u3088\u3046\u306b\u3057\u3066\u3001conv-layer w.r.t.\u306b\u304a\u3051\u308b\u3059\u3079\u3066\u306e\u30d5\u30a3\u30eb\u30bf\u306e\u76f8\u5bfe\u7684\u4f4d\u7f6e\u504f\u5dee\u306e\u5e73\u5747\u304c\u8a08\u7b97\u3055\u308c\u308b\u3002 \u3059\u3079\u3066\u306e\u30e9\u30f3\u30c9\u30de\u30fc\u30af\u3001\u3059\u306a\u308f\u3061meanfmeanK k = 1Df\u3001k\u306f\u3001CNN\u306e\u4f4d\u7f6e\u4e0d\u5b89\u5b9a\u6027\u3092\u6e2c\u5b9a\u3059\u308b\u3002\u3053\u3053\u3067\u3001K\u306f\u30e9\u30f3\u30c9\u30de\u30fc\u30af\u306e\u6570\u3092\u793a\u3059\u3002","title":"6.2 Location instability"},{"location":"DL/survey/Visual_Interpretability/#7-network-interpretability-for-middle-to-end","text":"learning Based on studies discussed in Sections 4 and 5, people may either disentangle representations of a pre-trained CNN or learn a new network with interpretable, disentangled representations. Such interpretable/disentangled network representations can further enable middle-to-end model learning at the semantic level without strong supervision. We briefly review two typical studies [Zhang et al., 2017a; Zhang et al., 2017b] of middle-to-end learning as follows. 7.1 Active question-answering for learning And-Or graphs Based on the semantic And-Or representation proposed in [Zhang et al., 2016], [Zhang et al., 2017a] has developed a method to use active question-answering to semanticize neural patterns in conv-layers of a pre-trained CNN and build a model for hierarchical object understanding. As shown in Fig. 13, the CNN is pre-trained for object classification. The method aims to extract a four-layer interpretable And-Or graph (AOG) to explain the semantic hierarchy hidden in a CNN. The AOG encodes four-layer semantics, ranging across the semantic part (OR node), part templates (AND nodes), latent patterns (OR nodes), and neural units (terminal nodes) on feature maps. In the AOG, AND nodes represent compositional regions of a part, and OR nodes encode a list of alternative template/deformation candidates for a local part. The top part node (OR node) uses its children to represent some template candidates for the part. Each part template (AND node) in the second layer uses children latent patterns to represent its constituent regions. Each latent pattern in the third layer (OR node) naturally corresponds to a certain range of units within the feature map of a filter. The latent pattern selects a unit within this range to account for its geometric deformation. To learn an AOG, [Zhang et al., 2017a] allows the computer to actively identify and ask about objects, whose neural patterns cannot be explained by the current AOG. As shown in Fig. 14, in each step of the active question-answering, the current AOG is used to localize object parts among all the unannotated images. The method actively selects objects that cannot well fit the AOG, namely unexplained objects. The method predicts the potential gain of asking about each unexplained object, and thus determines the best sequence of questions (e.g. asking about template types and bounding boxes of unexplained object parts). In this way, the method uses the answers to either refine an existing part template or mine latent patterns for new object-part templates, to grow AOG branches. Fig. 15 compares the part-localization performance of different methods. The QA-based learning exhibits significantly higher efficiency than other baselines. The proposed method uses about 1/6\u20131/3 of the part annotations for training, but achieves similar or better part-localization per- formance than fast-RCNN methods. 7.2 Interactive manipulations of CNN patterns Let a CNN be pre-trained using annotations of object bounding boxes for object classification. [Zhang et al., 2017b] has explored an interactive method to diagnose knowledge representations of a CNN, in order to transfer CNN patterns to model object parts. Unlike traditional end-to-end learning of CNNs that requires numerous training samples, this method mines object part patterns from the CNN in the scenario of one/multi-shot learning. More specifically, the method uses part annotations on very few (e.g. three) object images for supervision. Given a bounding-box annotation of a part, the proposed method first uses [Zhang et al., 2016] to mine latent patterns, which are related to the annotated part, from conv-layers of the CNN. An AOG is used to organize all mined patterns as the representation of the target part. The method visualizes the mined latent patterns and asks people to remove latent patterns unrelated to the target part interactively. In this way, people can simply prune incorrect latent patterns from AOG branches to refine the AOG. Fig. 16 visualizes initially mined patterns and the remaining patterns after human interaction. With the guidance of human interactions, [Zhang et al., 2017b] has exhibited superior performance of part localization. 8 Prospective trends and conclusions In this paper, we have reviewed several research directions within the scope of network interpretability. Visualization of a neural unit\u2019s patterns was the starting point of understanding network representations in the early years. Then, people gradually developed methods to analyze feature spaces of neural networks and diagnose potential representation flaws hidden inside neural networks. At present, disentangling chaotic representations of conv-layers into graphical models and/or symbolic logic has become an emerging research direction to open the black-box of neural networks. The approach for transforming a pre-trained CNN into an explanatory graph has been proposed and has exhibited significant efficiency in knowledge transfer and weakly-supervised learning. End-to-end learning interpretable neural networks, whose intermediate layers encode comprehensible patterns, is also a prospective trend. Interpretable CNNs have been developed, where each filter in high conv-layers represents a specific object part. Furthermore, based on interpretable representations of CNN patterns, semantic-level middle-to-end learning has been proposed to speed up the learning process. Compared to traditional end-to-end learning, middle-to-end learning allows human interactions to guide the learning process and can be applied with very few annotations for supervision. In the future, we believe the middle-to-end learning will continuously be a fundamental research direction. In addition, based on the semantic hierarchy of an interpretable network, debugging CNN representations at the semantic level will create new visual applications. Acknowledgement This work is supported by ONR MURI project N00014-16-1- 2007 and DARPA XAI Award N66001-17-2-4029, and NSF IIS 1423305.","title":"7 Network interpretability for middle-to-end"},{"location":"diary/8\u6708/","text":"8\u6708 8\u67081\u65e5(\u6728) \u653e\u7f6e\u3057\u3066\u3044\u305f\u30b5\u30a4\u30c8\u3092\u518d\u5229\u7528 \u3053\u306e\u30b5\u30a4\u30c8\u3092\u30dd\u30fc\u30c8\u30d5\u30a9\u30ea\u30aa\u306b\u4f7f\u3046\u3053\u3068\u3092\u6c7a\u3081\u308b","title":"8\u6708"},{"location":"diary/8\u6708/#8","text":"8\u67081\u65e5(\u6728) \u653e\u7f6e\u3057\u3066\u3044\u305f\u30b5\u30a4\u30c8\u3092\u518d\u5229\u7528 \u3053\u306e\u30b5\u30a4\u30c8\u3092\u30dd\u30fc\u30c8\u30d5\u30a9\u30ea\u30aa\u306b\u4f7f\u3046\u3053\u3068\u3092\u6c7a\u3081\u308b","title":"8\u6708"},{"location":"lecture/robo/what is intristic motivation/","text":"What is intrinsic motivation? A typology of computational approaches Introduction There exists a wide diversity of motivation systems in living organisms, and humans in particular. \u751f\u7269\u3084\u7279\u306b\u30d2\u30c8\u306b\u306f\u591a\u69d8\u306a\u30e2\u30c1\u30d9\u30fc\u30b7\u30e7\u30f3\u30b7\u30b9\u30c6\u30e0\u304c\u5b58\u5728\u3059\u308b\u3002 For example, there are systems that push the organism to maintain certain levels of chemical energy, involving the ingestion of food, or systems that push the organism to maintain its temperature or its physical integrity in a zone of viability. \u4f8b\u3048\u3070\u3001\u98df\u7269\u306e\u6442\u53d6\u3092\u542b\u3080\u4e00\u5b9a\u30ec\u30d9\u30eb\u306e\u5316\u5b66\u30a8\u30cd\u30eb\u30ae\u30fc\u3092\u7dad\u6301\u3059\u308b\u3088\u3046\u306b\u751f\u7269\u3092\u62bc\u3057\u51fa\u3059\u30b7\u30b9\u30c6\u30e0\u3001\u307e\u305f\u306f\u751f\u7269\u304c\u751f\u5b58\u30be\u30fc\u30f3\u5185\u3067\u6e29\u5ea6\u307e\u305f\u306f\u305d\u306e\u7269\u7406\u7684\u5b8c\u5168\u6027\u3092\u7dad\u6301\u3059\u308b\u3088\u3046\u306b\u30d7\u30c3\u30b7\u30e5\u3059\u308b\u30b7\u30b9\u30c6\u30e0\u304c\u3042\u308b\u3002 Inspired by these kinds of motivation and their understanding by (neuro-) ethologists, roboticists have built machines endowed with similar systems with the aim of providing them with autonomy and properties of life-like intelligence (Arkin, 2005 ). \u30ed\u30dc\u30c3\u30c8\u5de5\u5b66\u8005\u306f\u3001\u3053\u308c\u3089\u306e\u7a2e\u985e\u306e\u30e2\u30c1\u30d9\u30fc\u30b7\u30e7\u30f3\u3068\uff08\u795e\u7d4c\uff09\u751f\u7269\u5b66\u8005\u306b\u3088\u308b\u7406\u89e3\u306b\u3088\u3063\u3066\u30a4\u30f3\u30b9\u30d4\u30ec\u30fc\u30b7\u30e7\u30f3\u3092\u5f97\u3066\u3001\u540c\u69d8\u306e\u30b7\u30b9\u30c6\u30e0\u3092\u4e0e\u3048\u3089\u308c\u305f\u6a5f\u68b0\u3092\u69cb\u7bc9\u3057\u3001\u81ea\u5f8b\u6027\u3068\u751f\u547d\u306e\u3088\u3046\u306a\u77e5\u6027\u306e\u7279\u6027\u3092\u63d0\u4f9b\u3059\u308b\u3053\u3068\u3092\u76ee\u6307\u3057\u3066\u3044\u308b\uff08Arkin\u30012005\uff09\u3002 For example sowbug-inspired robots (Endo and Arkin, 2001 ), praying mantis robots (Arkin et al., 1998 ) dog-like robots (Fujita et al., 2001 ) have been constructed. \u4f8b\u3048\u3070\u3001\u30bd\u30a6\u30d0\u30b0\b(\u4e38\u866b)\u306b\u89e6\u767a\u3055\u308c\u305f\u30ed\u30dc\u30c3\u30c8\uff08Endo and Arkin\u30012001\uff09\u3001\u30ab\u30de\u30ad\u30ea\u306e\u30ed\u30dc\u30c3\u30c8\uff0c\u72ac\u306e\u3088\u3046\u306a\u30ed\u30dc\u30c3\u30c8\uff08Arkin et al\u3002\u30011998\uff09\uff082001\u5e74\uff09\u304c\u4f5c\u3089\u308c\u3066\u304d\u305f\u3002 Some animals, and this is most prominent in humans, also have more general motivations that push them to explore, manipulate or probe their environment, fostering curiosity and engagement in playful and new activities. This kind of motivation, which is called intrinsic motivation by psychologists (Ryan and Deci, 2000 ), is paramount for sensorimotor and cognitive development throughout lifespan. \u4eba\u9593\u306e\u4e2d\u3067\u6700\u3082\u9855\u8457\u306a\u52d5\u7269\u3082\u3042\u308c\u3070\u3001\u5f7c\u3089\u306e\u74b0\u5883\u3092\u63a2\u7d22\u3057\u3001\u64cd\u4f5c\u3057\u305f\u308a\u63a2\u691c\u3057\u305f\u308a\u3001\u904a\u3073\u5fc3\u306e\u3042\u308b\u65b0\u3057\u3044\u6d3b\u52d5\u306e\u597d\u5947\u5fc3\u3084\u95a2\u4e0e\u3092\u4fc3\u3059\u3088\u3046\u306a\u4e00\u822c\u7684\u306a\u52d5\u6a5f\u3082\u3042\u308a\u307e\u3059\u3002 \u5fc3\u7406\u5b66\u8005\uff08Ryan and Deci\u30012000\uff09\u306b\u3088\u308b\u5185\u767a\u7684\u52d5\u6a5f\u3065\u3051\u3068\u547c\u3070\u308c\u308b\u3053\u306e\u7a2e\u306e\u30e2\u30c1\u30d9\u30fc\u30b7\u30e7\u30f3\u306f\u3001\u5bff\u547d\u3092\u901a\u3058\u3066\u611f\u899a\u904b\u52d5\u304a\u3088\u3073\u8a8d\u77e5\u767a\u9054\u306b\u6700\u3082\u91cd\u8981\u3067\u3042\u308b\u3002 There is a vast literature in psychology that explains why it is essential for cognitive growth and organization, and investigates the actual potential cognitive processes underlying intrinsic motivation (Berlyne, 1960 ; Csikszentmihalyi, 1991 ; Deci and Ryan, 1985 ; Ryan and Deci, 2000 ; White, 1959 ). This has gathered the interest of a growing number of researchers in developmental robotics in the recent years, and several computational models have been developed (see Barto et al., 2004 ; Oudeyer et al., 2007 for reviews). \u306a\u305c\u3001\u305d\u308c\u304c\u8a8d\u77e5\u306e\u6210\u9577\u3068\u7d44\u7e54\u306b\u3068\u3063\u3066\u4e0d\u53ef\u6b20\u306a\u306e\u304b\u3092\u8aac\u660e\u3057\u3001\u5185\u5728\u7684\u306a\u52d5\u6a5f\u3065\u3051\u306e\u6839\u5e95\u306b\u3042\u308b\u5b9f\u969b\u306e\u6f5c\u5728\u7684\u306a\u8a8d\u77e5\u904e\u7a0b\u3092\u8abf\u3079\u308b\u5fc3\u7406\u5b66\u306e\u5e83\u7bc4\u306a\u6587\u732e\u304c\u3042\u308b\uff08Berlyne\u30011960; Csikszentmihalyi\u30011991; Deci and Ryan\u30011985; Ryan and Deci\u3001 White\u30011959\uff09\u3002 \u3053\u308c\u306f\u8fd1\u5e74\u3001\u958b\u767a\u30ed\u30dc\u30c3\u30c8\u306e\u7814\u7a76\u8005\u6570\u304c\u5897\u52a0\u3057\u3066\u3044\u308b\u3053\u3068\u306e\u95a2\u5fc3\u3092\u96c6\u3081\u3066\u304a\u308a\u3001\u3044\u304f\u3064\u304b\u306e\u8a08\u7b97\u30e2\u30c7\u30eb\u304c\u958b\u767a\u3055\u308c\u3066\u3044\u308b\uff08Barto et al\u3002\u30012004; Oudeyer et al\u3002\u30012007\uff09\u3002 However, the very concept of intrinsic motivation has never really been consistently and critically discussed from a computational point of view. It has been used intuitively by many authors without asking for what it really means. Thus, the first objective and contribution of this paper is to present an overview of this concept in psychology followed by a critical reinterpretation in computational terms. \u3057\u304b\u3057\u3001\u672c\u8cea\u7684\u306a\u52d5\u6a5f\u3065\u3051\u3068\u3044\u3046\u6982\u5ff5\u306f\u3001\u8a08\u7b97\u4e0a\u3001\u4e00\u8cab\u3057\u3066\u6279\u5224\u7684\u306b\u8b70\u8ad6\u3055\u308c\u305f\u3053\u3068\u306f\u3042\u308a\u307e\u305b\u3093\u3002 \u305d\u308c\u306f\u3001\u5b9f\u969b\u306b\u4f55\u3092\u610f\u5473\u3059\u308b\u306e\u304b\u3092\u554f\u308f\u305a\u3001\u591a\u304f\u306e\u8457\u8005\u306b\u3088\u3063\u3066\u76f4\u611f\u7684\u306b\u4f7f\u7528\u3055\u308c\u3066\u304d\u307e\u3057\u305f\u3002 \u3057\u305f\u304c\u3063\u3066\u3001\u3053\u306e\u8ad6\u6587\u306e\u7b2c\u4e00\u306e\u76ee\u7684\u3068\u8ca2\u732e\u306f\u3001\u5fc3\u7406\u5b66\u306b\u304a\u3051\u308b\u3053\u306e\u6982\u5ff5\u306e\u6982\u8981\u3092\u63d0\u793a\u3057\u3001\u6b21\u306b\u8a08\u7b97\u4e0a\u306e\u91cd\u8981\u306a\u518d\u89e3\u91c8\u3092\u63d0\u793a\u3059\u308b\u3053\u3068\u3067\u3042\u308b\u3002 We show that the definitions provided in psychology are actually unsatisfying. As a consequence, we will set the ground for a systematic operational study of intrinsic motivation by presenting a typology of possible computational approaches, and discuss whether it is possible or useful to give a single general computational definition of intrinsic motivation. \u79c1\u305f\u3061\u306f\u3001\u5fc3\u7406\u5b66\u3067\u63d0\u4f9b\u3055\u308c\u308b\u5b9a\u7fa9\u304c\u5b9f\u969b\u306b\u306f\u6e80\u8db3\u3067\u304d\u306a\u3044\u3053\u3068\u3092\u793a\u3057\u3066\u3044\u307e\u3059\u3002 \u7d50\u679c\u3068\u3057\u3066\u3001\u53ef\u80fd\u306a\u8a08\u7b97\u624b\u6cd5\u306e\u985e\u578b\u3092\u63d0\u793a\u3057\u3001\u5185\u5728\u7684\u306a\u52d5\u6a5f\u3065\u3051\u306e\u5358\u4e00\u306e\u4e00\u822c\u7684\u306a\u8a08\u7b97\u4e0a\u306e\u5b9a\u7fa9\u3092\u4e0e\u3048\u308b\u3053\u3068\u304c\u53ef\u80fd\u3067\u3042\u308b\u304b\u6709\u7528\u3067\u3042\u308b\u304b\u3092\u8b70\u8ad6\u3059\u308b\u3053\u3068\u306b\u3088\u3063\u3066\u3001\u5185\u5728\u7684\u306a\u52d5\u6a5f\u3065\u3051\u306e\u4f53\u7cfb\u7684\u306a\u904b\u7528\u7814\u7a76\u306e\u6839\u62e0\u3092\u5b9a\u3081\u308b\u3002 The typology that we will present is partly based on existing computational models, but also presents new ways of conceptualizing intrinsic motivation. We will try to focus on how these models relate to each other and propose a classification into broad but distinct categories. \u6211\u3005\u304c\u63d0\u793a\u3059\u308b\u985e\u578b\u5b66\u306f\u3001\u65e2\u5b58\u306e\u8a08\u7b97\u30e2\u30c7\u30eb\u306b\u90e8\u5206\u7684\u306b\u57fa\u3065\u3044\u3066\u3044\u308b\u304c\u3001\u5185\u5728\u7684\u306a\u52d5\u6a5f\u3065\u3051\u3092\u6982\u5ff5\u5316\u3059\u308b\u65b0\u3057\u3044\u65b9\u6cd5\u3092\u63d0\u793a\u3059\u308b\u3002 \u3053\u308c\u3089\u306e\u30e2\u30c7\u30eb\u304c\u4e92\u3044\u306b\u3069\u306e\u3088\u3046\u306b\u95a2\u4fc2\u3057\u3066\u3044\u308b\u304b\u306b\u7126\u70b9\u3092\u5f53\u3066\u3066\u3001\u5e83\u7bc4\u3067\u306f\u3063\u304d\u308a\u3057\u305f\u30ab\u30c6\u30b4\u30ea\u30fc\u306b\u5206\u985e\u3059\u308b\u3053\u3068\u3092\u63d0\u6848\u3057\u307e\u3059\u3002 (\u4e00\u4eba\u76ee\u306f\u3053\u3053\u307e\u3067) Intrinsic Motivation from the Psychologist\u2019s Point of View \u5fc3\u7406\u5b66\u8005\u306e\u8996\u70b9\u304b\u3089\u306e\u672c\u8cea\u7684\u306a\u52d5\u6a5f\u3065\u3051 Intrinsic Motivation and Instrumentalization \u672c\u8cea\u7684\u306a\u52d5\u6a5f\u4ed8\u3051\u3068\u5668\u68b0\u5316(\u5f79\u306b\u7acb\u305f\u305b\u308b) According to Ryan and Deci (2000) (pp. 56), Intrinsic motivation is defined as the doing of an activity for its inherent satisfaction rather than for some separable consequence. When intrinsically motivated, a person is moved to act for the fun or challenge entailed rather than because of external products, pressures, or rewards. Ryan and Deci\uff082000\uff09\uff08pp\u300256\uff09\u306b\u3088\u308b\u3068\u3001 \u5185\u5728\u7684\u306a\u52d5\u6a5f\u3065\u3051\u3068\u306f\u3001\u5206\u96e2\u53ef\u80fd\u306a\u7d50\u679c\u304c\u751f\u3058\u308b\u306e\u3067\u306f\u306a\u304f\u3001\u5185\u5728\u3059\u308b\u6e80\u8db3\u306e\u305f\u3081\u306e\u6d3b\u52d5\u3092\u884c\u3046\u3053\u3068\u3067\u3059\u3002 \u672c\u8cea\u7684\u306b\u52d5\u6a5f\u4ed8\u3051\u3055\u308c\u308b\u3068\u3001\u5916\u90e8\u306e\u88fd\u54c1\u3001\u5727\u529b\u3001\u307e\u305f\u306f\u5831\u916c\u306e\u305f\u3081\u306b\u3067\u306f\u306a\u304f\u3001\u4eba\u304c\u697d\u3057\u304f\u3084\u308a\u305f\u3044\u884c\u52d5\u3092\u3059\u308b\u3088\u3046\u52d5\u304b\u3055\u308c\u307e\u3059\u3002 Intrinsic motivation is clearly visible in young infants, that consistently try to grasp, throw, bite, squash or shout at new objects they encounter. Even if less important as they grow, human adults are still often intrinsically motivated while they play crosswords, make paintings, do gardening or just read novels or watch movies. Yet, to get a clearer picture of intrinsic motivation, one needs to understand that it has been defined by contrast to extrinsic motivation: \u5185\u56e0\u6027\u306e\u52d5\u6a5f\u3065\u3051\u306f\u3001\u5e7c\u5150\u306b\u306f\u3063\u304d\u308a\u3068\u76ee\u306b\u898b\u3048\u307e\u3059\u3002\u5e7c\u5150\u306f\u3001\u906d\u9047\u3059\u308b\u65b0\u3057\u3044\u7269\u4f53\u3092\u5e38\u306b\u628a\u63e1\u3057\u3001\u6295\u3052\u305f\u308a\u3001\u565b\u3093\u3060\u308a\u3057\u305f\u308a\u3001\u53eb\u3076\u3088\u3046\u306b\u3057\u307e\u3059\u3002 \u5f7c\u3089\u304c\u6210\u9577\u3059\u308b\u306b\u3064\u308c\u3066\u305d\u308c\u307b\u3069\u91cd\u8981\u3067\u306f\u306a\u3044\u3068\u3057\u3066\u3082\u3001\u4eba\u9593\u306e\u5927\u4eba\u306f\u3001\u30af\u30ed\u30b9\u30ef\u30fc\u30c9\u3092\u6f14\u3058\u305f\u308a\u3001\u7d75\u3092\u63cf\u3044\u305f\u308a\u3001\u30ac\u30fc\u30c7\u30cb\u30f3\u30b0\u3092\u3057\u305f\u308a\u3001\u5c0f\u8aac\u3092\u8aad\u3093\u3060\u308a\u3001\u6620\u753b\u3092\u898b\u305f\u308a\u3057\u3066\u3044\u308b\u9593\u3001\u4f9d\u7136\u3068\u3057\u3066\u672c\u8cea\u7684\u306b\u52d5\u6a5f\u4ed8\u3051\u3089\u308c\u3066\u3044\u307e\u3059\u3002 \u3057\u304b\u3057\u3001\u5185\u5728\u7684\u306a\u30e2\u30c1\u30d9\u30fc\u30b7\u30e7\u30f3\u306e\u660e\u78ba\u306a\u30a4\u30e1\u30fc\u30b8\u3092\u5f97\u308b\u305f\u3081\u306b\u306f\u3001\u305d\u308c\u304c\u5916\u7684\u30e2\u30c1\u30d9\u30fc\u30b7\u30e7\u30f3\u3068\u306f\u5bfe\u7167\u7684\u306b\u5b9a\u7fa9\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u7406\u89e3\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002 Extrinsic motivation is a construct that pertains whenever an activity is done in order to attain some separable outcome. Extrinsic motivation thus contrasts with intrinsic motivation, which refers to doing an activity simply for the enjoyment of the activity itself, rather than its instrumental value. (Ryan and Deci, 2000 ) \u5185\u56e0\u6027\u306e\u52d5\u6a5f\u3065\u3051\u306f\u3001\u5206\u96e2\u53ef\u80fd\u306a\u7d50\u679c\u3092\u9054\u6210\u3059\u308b\u305f\u3081\u306b\u6d3b\u52d5\u304c\u884c\u308f\u308c\u308b\u305f\u3073\u306b\u95a2\u4fc2\u3059\u308b\u69cb\u6210\u7269\u3067\u3042\u308b\u3002 \u3057\u305f\u304c\u3063\u3066\u5185\u5728\u7684\u306a\u52d5\u6a5f\u3065\u3051\u306f\u5185\u5728\u7684\u306a\u52d5\u6a5f\u3065\u3051\u3068\u5bfe\u7167\u7684\u3067\u3042\u308a\u3001\u305d\u308c\u306f\u5358\u306b\bintrinsic motivation\u3088\u308a\u3082\u3080\u3057\u308d\u6d3b\u52d5\u305d\u306e\u3082\u306e\u3092\u697d\u3057\u3080\u305f\u3081\u306e\u6d3b\u52d5\u3092\u5358\u306b\u884c\u3046\u3053\u3068\u3092\u6307\u3059\u3002 \uff08Ryan and Deci\u30012000\uff09 We see that a central feature that differentiates intrinsic and extrinsic motivation is instrumentalization. We also see that the concepts of intrinsic and extrinsic motivations form a different distinction than the one between internal and external motivations. In the computational literature, \u201cintrinsic\u201d is sometimes used as a synonym to \u201cinternal\u201d, and \u201cextrinsic\u201d as a synonym to \u201cexternal\u201d. Yet, it is in fact a confusion. Indeed, there are extrinsic motivations that can be internal and vice versa. In fact, there are different kinds of instrumentalizations that can be classified as more or less self-determined (Ryan and Deci, 2000 ). Let us give examples to be more clear. \u5185\u5728\u6027\u3068\u5916\u56e0\u6027\u306e\u52d5\u6a5f\u3065\u3051\u3092\u533a\u5225\u3059\u308b\u4e2d\u5fc3\u7684\u306a\u7279\u5fb4\u306f\u6a5f\u5668\u5316\u3067\u3042\u308b\u3053\u3068\u304c\u308f\u304b\u308a\u307e\u3059\u3002 \u307e\u305f\u3001\u5185\u5728\u7684\u304a\u3088\u3073\u5916\u7684\u306a\u52d5\u6a5f\u306e\u6982\u5ff5\u306f\u3001\u5185\u7684\u304a\u3088\u3073\u5916\u7684\u306a\u52d5\u6a5f\u3068\u306f\u7570\u306a\u308b\u533a\u5225\u3092\u5f62\u6210\u3059\u308b\u3053\u3068\u304c\u308f\u304b\u308a\u307e\u3059\u3002 \u6587\u732e\u3067\u306f\u3001\u300c\u5185\u5728\u300d\u306f\u6642\u306b\u306f\u300c\u5185\u90e8\u300d\u306e\u540c\u7fa9\u8a9e\u3068\u3057\u3066\u4f7f\u7528\u3055\u308c\u3001\u300c\u5916\u7684\u300d\u306f\u300c\u5916\u90e8\u300d\u306e\u540c\u7fa9\u8a9e\u3068\u3057\u3066\u4f7f\u7528\u3055\u308c\u307e\u3059\u3002 \u3057\u304b\u3057\u3001\u305d\u308c\u306f\u5b9f\u969b\u306b\u306f\u6df7\u4e71\u3092\u62db\u304d\u307e\u3059\u3002 \u78ba\u304b\u306b\u3001\u5185\u90e8\u7684\u306a\u3053\u3068\u304c\u3067\u304d\u308b\u5916\u7684\u306a\u52d5\u6a5f\u304c\u3042\u308a\u3001\u305d\u306e\u9006\u3082\u3042\u308a\u307e\u3059\u3002 \u5b9f\u969b\u306b\u306f\u3001\u591a\u304b\u308c\u5c11\u306a\u304b\u308c\u81ea\u5df1\u6c7a\u5b9a\u3055\u308c\u305f\u3082\u306e\u3068\u3057\u3066\u5206\u985e\u3055\u308c\u5f97\u308b\u69d8\u3005\u306a\u6a5f\u5668\u5316\u304c\u3042\u308b\uff08Ryan and Deci\u30012000\uff09\u3002 \u3082\u3063\u3068\u660e\u78ba\u306b\u3059\u308b\u305f\u3081\u306b\u4f8b\u3092\u6319\u3052\u307e\u3057\u3087\u3046\u3002 For example, a child that does thoroughly his homework might be motivated by avoiding the sanctions of his parents if he would not do it. The cause for action is here clearly external, and the homework is not done for its own sake but for the separate outcome of not getting sanctions. Here the child is extrinsically and externally motivated. \u4f8b\u3048\u3070\u3001\u5f7c\u306e\u5bbf\u984c\u3092\u5fb9\u5e95\u7684\u306b\u884c\u3046\u5b50\u4f9b\u306f\u3001\u5f7c\u304c\u305d\u308c\u3092\u3057\u306a\u3044\u306a\u3089\u3001\u4e21\u89aa\u306e\u5236\u88c1\u3092\u907f\u3051\u308b\u3053\u3068\u306b\u3088\u3063\u3066\u52d5\u6a5f\u3065\u3051\u308b\u304b\u3082\u3057\u308c\u306a\u3044\u3002 \u884c\u52d5\u306e\u539f\u56e0\u306f\u3053\u3053\u3067\u306f\u660e\u3089\u304b\u306b\u5916\u7684\u3067\u3042\u308a\u3001\u5bbf\u984c\u306f\u5b50\u3069\u3082\u81ea\u8eab\u306e\u305f\u3081\u3067\u306f\u306a\u304f\u3001\u5236\u88c1\u3092\u53d7\u3051\u306a\u3044\u5225\u306e\u7d50\u679c\u306e\u305f\u3081\u306b\u884c\u308f\u308c\u308b\u3002 \u3053\u308c\u304c\u5b50\u4f9b\u306f\u5916\u90e8\u7684\u304b\u3064\u5916\u90e8\u7684\u306b\u52d5\u6a5f\u4ed8\u3051\u3089\u308c\u3066\u3044\u308b\u3002 On the other hand, it is possible that a child could do thoroughly his homework because he is persuaded that it will help him get the job he dreams of, later when he will be an adult. In this case, the cause for action is internally generated, and the homework is again not achieved for its own sake but because the child thinks it will lead to the separate outcome of getting a good job. \u4e00\u65b9\u3001\u5b50\u4f9b\u304c\u5927\u4eba\u306b\u306a\u308b\u3068\u304d\u306b\u3001\u5f7c\u304c\u5922\u898b\u308b\u4ed5\u4e8b\u3092\u5f97\u308b\u306e\u306b\u5f79\u7acb\u3064\u3068\u8aac\u5f97\u3055\u308c\u3066\u3044\u308b\u306e\u3067\u3001\u5b50\u4f9b\u304c\u5fb9\u5e95\u7684\u306b\u5bbf\u984c\u3092\u3059\u308b\u3053\u3068\u306f\u53ef\u80fd\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u3002 \u3053\u306e\u5834\u5408\u3001\u884c\u52d5\u306e\u539f\u56e0\u306f\u5185\u90e8\u7684\u306b\u751f\u6210\u3055\u308c\u3001\u5bbf\u984c\u306f\u81ea\u5206\u306e\u305f\u3081\u306b\u9054\u6210\u3055\u308c\u308b\u306e\u3067\u306f\u306a\u304f\u3001\u5b50\u3069\u3082\u304c\u826f\u3044\u4ed5\u4e8b\u3092\u5f97\u308b\u3068\u3044\u3046\u5225\u306e\u7d50\u679c\u306b\u3064\u306a\u304c\u308b\u3068\u8003\u3048\u3066\u3044\u308b\u304b\u3089\u3067\u3059\u3002 Finally, it is also possible that a child does thoroughly its homework for the fun of it, and because he experiences pleasure in the discovery of new knowledge or considers for example its math problem just as fun as playing a video game. In this case, its behavior is intrinsically (and internally) motivated. \u6700\u5f8c\u306b\u3001\u5b50\u4f9b\u304c\u305d\u308c\u3092\u697d\u3057\u3080\u305f\u3081\u306e\u5bbf\u984c\u3092\u5fb9\u5e95\u7684\u306b\u884c\u3046\u3053\u3068\u3084\u3001\u65b0\u3057\u3044\u77e5\u8b58\u306e\u767a\u898b\u306b\u559c\u3073\u3092\u611f\u3058\u305f\u308a\u3001\u30d3\u30c7\u30aa\u30b2\u30fc\u30e0\u3092\u30d7\u30ec\u30a4\u3059\u308b\u306e\u3068\u540c\u3058\u304f\u3089\u3044\u697d\u3057\u3044\u3068\u3044\u3046\u6570\u5b66\u7684\u554f\u984c\u3092\u8003\u616e\u3057\u305f\u308a\u3059\u308b\u3053\u3068\u3082\u3042\u308a\u307e\u3059\u3002 \u3053\u306e\u5834\u5408\u3001\u305d\u306e\u632f\u821e\u3044\u306f\u672c\u8cea\u7684\u306b\uff08\u305d\u3057\u3066\u5185\u90e8\u7684\u306b\uff09\u5185\u7684\u52d5\u6a5f\u3068\u8003\u3048\u3089\u308c\u307e\u3059\u3002 These different kinds of motivations can also sometimes be superposed or interleaved in the same global activity. For example, it is quite possible that a child doing his homework is partly extrinsically motivated by getting a high grade at the exam and partly intrinsically motivated by learning new interesting things. Also, for example, imagine a child that is intrinsically motivated by playing tennis but has to ride its bicycle to get to the tennis court (and does not like particularly riding bicycles). In this case, the riding of the bicycle is an internal and extrinsically motivated behavior that spins out of the intrinsically motivated behavior of playing tennis. \u3053\u308c\u3089\u306e\u7570\u306a\u308b\u7a2e\u985e\u306e\u52d5\u6a5f\u306f\u3001\u6642\u306b\u306f\u540c\u3058\u30b0\u30ed\u30fc\u30d0\u30eb\u306a\u6d3b\u52d5\u306b\u91cd\u306d\u5408\u308f\u3055\u308c\u305f\u308a\u3001\u30a4\u30f3\u30bf\u30fc\u30ea\u30fc\u30d6\u3055\u308c\u305f\u308a\u3059\u308b\u3053\u3068\u3082\u3042\u308a\u307e\u3059\u3002 \u4f8b\u3048\u3070\u3001\u5b50\u4f9b\u304c\u5bbf\u984c\u3092\u3057\u3066\u3044\u308b\u3053\u3068\u306f\u3001\u90e8\u5206\u7684\u306b\u306f\u3001\u8a66\u9a13\u3067\u7279\u5178\u3092\u5f97\u3066\u3001\u4e00\u90e8\u306f\u672c\u8cea\u7684\u306b\u65b0\u3057\u3044\u8208\u5473\u6df1\u3044\u3053\u3068\u3092\u5b66\u3076\u3053\u3068\u306b\u3088\u3063\u3066\u52d5\u6a5f\u3065\u3051\u3089\u308c\u3066\u3044\u308b\u53ef\u80fd\u6027\u304c\u3042\u308a\u307e\u3059\u3002 \u307e\u305f\u3001\u4f8b\u3048\u3070\u3001\u30c6\u30cb\u30b9\u3092\u3057\u3066\u672c\u8cea\u7684\u306b\u52d5\u6a5f\u4ed8\u3051\u3089\u308c\u3066\u3044\u308b\u304c\u3001\u81ea\u8ee2\u8eca\u306b\u4e57\u3063\u3066\u30c6\u30cb\u30b9\u30b3\u30fc\u30c8\u306b\u7740\u304f\uff08\u7279\u306b\u81ea\u8ee2\u8eca\u304c\u597d\u304d\u3067\u306f\u306a\u3044\uff09\u5b50\u4f9b\u3092\u60f3\u50cf\u3057\u3066\u307f\u3066\u304f\u3060\u3055\u3044\u3002 \u3053\u306e\u5834\u5408\u3001\u81ea\u8ee2\u8eca\u306e\u4e57\u8eca\u306f\u3001\u672c\u8cea\u7684\u306b\u52d5\u6a5f\u4ed8\u3051\u3055\u308c\u305f\u30c6\u30cb\u30b9\u306e\u632f\u308b\u821e\u3044\u304b\u3089\u98db\u3073\u51fa\u3059\u3001\u5185\u7684\u304b\u3064\u5185\u5728\u7684\u306b\u52d5\u6a5f\u4ed8\u3051\u3055\u308c\u305f\u884c\u52d5\u3067\u3042\u308b\u3002 question: \b\u5185\u7684\u3068\u5916\u7684\u306a\u52d5\u6a5f\u3065\u3051\u306f\u3069\u3046\u306a\u308b\uff1f What Makes an Activity Intrinsically Motivating? Given this broad distinction between intrinsic and extrinsic motivation, psychologists have tried to build theories about which features of activities make them intrinsically motivating for some people (and not all) at some times (the same activity might be intrinsically motivating for a person at a given time, but no more later on). They have studied how these motivations could be functionally implemented in an organism, humans in particular, and several theoretical directions have been presented. \u5fc3\u7406\u5b66\u8005\u306f\u5185\u5728\u6027\u3068\u5916\u56e0\u6027\u306e\u30e2\u30c1\u30d9\u30fc\u30b7\u30e7\u30f3\u306e\u9593\u306b\u3053\u306e\u5e83\u3044\u5dee\u7570\u304c\u3042\u308b\u3053\u3068\u3092\u8003\u616e\u3057\u3066\u3001\u3042\u308b\u7a2e\u306e\u4eba\u3005\uff08\u305d\u3057\u3066\u3059\u3079\u3066\u3067\u306f\u306a\u3044\uff09\u306b\u3068\u3063\u3066\u672c\u8cea\u7684\u306a\u52d5\u6a5f\u3068\u306a\u308b\u7406\u8ad6\u3092\u69cb\u7bc9\u3057\u3088\u3046\u3068\u8a66\u307f\u3066\u304d\u305f\uff08\u540c\u3058\u30a2\u30af\u30c6\u30a3\u30d3\u30c6\u30a3\u306f\u3001 but no more later on\uff09\u3002 \u5f7c\u3089\u306f\u3001\u3053\u308c\u3089\u306e\u52d5\u6a5f\u304c\u751f\u7269\u3001\u7279\u306b\u30d2\u30c8\u306b\u304a\u3044\u3066\u6a5f\u80fd\u7684\u306b\u3069\u306e\u3088\u3046\u306b\u5b9f\u65bd\u3055\u308c\u5f97\u308b\u304b\u3092\u7814\u7a76\u3057\u3001\u3044\u304f\u3064\u304b\u306e\u7406\u8ad6\u7684\u6307\u91dd\u304c\u63d0\u793a\u3055\u308c\u3066\u3044\u308b\u3002 Drives to manipulate, drives to explore. In the 1950s, psychologists started by trying to give an account of intrinsic motivation and exploratory activities on the basis of the theory of drives (Hull, 1943 ), which are specific tissue deficits like hunger or pain that the organisms try to reduce. For example, (Montgomery, 1954 ) proposed a drive for exploration and (Harlow, 1950 ) a drive to manipulate. This drive naming approach had many short-comings which were criticized in detail by White (1959) : drive: \u52d5\u6a5f\u3065\u3051\uff1f \u52d5\u6a5f\u3065\u3051\u306b\u3088\u3063\u3066\u64cd\u4f5c\u3057\u3001\u63a2\u7d22\u306e\u305f\u3081\u306b\u52d5\u6a5f\u3065\u3051\u3057\u307e\u3059\u3002 1950\u5e74\u4ee3\u3001\u5fc3\u7406\u5b66\u8005\u306f\u3001\u98e2\u3048\u3084\u75db\u307f\u306e\u3088\u3046\u306a\u7279\u5b9a\u306e\u7d44\u7e54\u6b20\u640d\u3067\u3042\u308a\u3001\u751f\u7269\u304c\u6e1b\u5c11\u3055\u305b\u3088\u3046\u3068\u3057\u3066\u3044\u308b\u3001\u52d5\u6a5f\u3065\u3051\u7406\u8ad6\uff08Hull\u30011943\uff09\u306b\u57fa\u3065\u3044\u3066\u3001\u5185\u5728\u7684\u306a\u52d5\u6a5f\u4ed8\u3051\u3068\u63a2\u7d22\u7684\u6d3b\u52d5\u306e\u8aac\u660e\u3092\u8a66\u307f\u308b\u3053\u3068\u304b\u3089\u59cb\u3081\u305f\u3002 \u4f8b\u3048\u3070\u3001\uff08Montgomery\u30011954\uff09\u306f\u3001\u63a2\u7d22\u306e\u305f\u3081\u306e\u52d5\u6a5f\u3065\u3051\u3092\u63d0\u6848\u3057\uff08Harlow\u30011950\uff09\u3001\u64cd\u4f5c\u3059\u308b\u305f\u3081\u306e\u52d5\u6a5f\u3065\u3051\u3092\u63d0\u6848\u3057\u305f\u3002 \u3053\u306e\u52d5\u6a5f\u3065\u3051\u3068\u547d\u540d\u3055\u308c\u305f\u30a2\u30d7\u30ed\u30fc\u30c1\u306b\u306f\u3001white\uff081959\uff09\u306b\u3088\u3063\u3066\u8a73\u7d30\u306b\u6279\u5224\u3055\u308c\u305f\u591a\u304f\u306e\u77ed\u6240\u304c\u3042\u308a\u307e\u3057\u305f\u3002 intrinsically motivated exploratory activities have a fundamentally different dynamics. Indeed, they are not homeostatic: the general tendency to explore is not a consummatory response to a stressful perturbation of the organism\u2019s body. \u672c\u8cea\u7684\u306b\u52d5\u6a5f\u4ed8\u3051\u3055\u308c\u305f\u63a2\u7d22\u7684\u6d3b\u52d5\u306f\u3001\u6839\u672c\u7684\u306b\u7570\u306a\u308b\u52d5\u529b\u5b66\u3092\u6709\u3059\u308b\u3002 \u78ba\u304b\u306b\u3001\u5f7c\u3089\u306f\u6052\u5e38\u6027\u3067\u306f\u3042\u308a\u307e\u305b\u3093\u3002\u4e00\u822c\u7684\u306a\u63a2\u7d22\u306e\u50be\u5411\u306f\u3001\u751f\u7269\u4f53\u306e\u30b9\u30c8\u30ec\u30b9\u306b\u6e80\u3061\u305f\u6442\u52d5\u306b\u5bfe\u3059\u308b\u5b8c\u7d50\u7684\u306a\u53cd\u5fdc\u3067\u306f\u3042\u308a\u307e\u305b\u3093\u3002 Reduction of cognitive dissonance. Some researchers then proposed another conceptualization. Festinger\u2019s theory of cognitive dissonance (Festinger, 1957 ) asserted that organisms are motivated to reduce dissonance, which is the incompatibility between internal cognitive structures and the situations currently perceived. dissonance: \u4e0d\u4e00\u81f4\uff0c\u4e0d\u548c \u8a8d\u77e5\u4e0d\u8abf\u548c\u306e\u8efd\u6e1b\u3002 \u3044\u304f\u3064\u304b\u306e\u7814\u7a76\u8005\u306f\u5225\u306e\u6982\u5ff5\u5316\u3092\u63d0\u6848\u3057\u305f\u3002 Festinger\u306e\u8a8d\u77e5\u7684\u4e0d\u8abf\u548c\u306e\u7406\u8ad6\uff08Festinger\u30011957\uff09\u306f\u3001\u751f\u7269\u304c\u5185\u7684\u8a8d\u77e5\u69cb\u9020\u3068\u73fe\u5728\u8a8d\u8b58\u3055\u308c\u3066\u3044\u308b\u72b6\u6cc1\u3068\u306e\u9593\u306e\u4e0d\u9069\u5408\u3067\u3042\u308b\u4e0d\u5354\u548c\u3092\u6e1b\u3089\u3059\u3088\u3046\u52d5\u6a5f\u4ed8\u3051\u3089\u308c\u3066\u3044\u308b\u3068\u4e3b\u5f35\u3057\u305f\u3002 Fifteen years later a related view was articulated by Kagan stating that a primary motivation for humans is the reduction of uncertainty in the sense of the \u201cincompatibility between (two or more) cognitive structures, between cognitive structure and experience, or between structures and behavior\u201d (Kagan, 1972 ). 15\u5e74\u5f8c\u3001\u4eba\u9593\u306e\u4e3b\u306a\u52d5\u6a5f\u306f\u3001\u300c\uff082\u3064\u4ee5\u4e0a\u306e\uff09\u8a8d\u77e5\u69cb\u9020\u306e\u9593\u3001\u8a8d\u77e5\u69cb\u9020\u3068\u7d4c\u9a13\u306e\u9593\u3001\u307e\u305f\u306f\u69cb\u9020\u3068\u884c\u52d5\u3068\u306e\u9593\u306e\u4e0d\u9069\u5408\u6027\u300d\u306e\u610f\u5473\u306b\u304a\u3051\u308b\u4e0d\u78ba\u5b9f\u6027\u306e\u6e1b\u5c11\u3067\u3042\u308b\u3068\u3044\u3046Kagan\u306e\u95a2\u9023\u3059\u308b\u8ad6\u6587\u304c\u3042\u308b\uff08Kagan\u30011972\uff09\u3002 However, these theories were criticized on the basis that much human behavior is also intended to increase uncertainty, and not only to reduce it. Human seem to look for some forms of optimality between completely uncertain and completely certain situations. \u3057\u304b\u3057\u3001\u3053\u308c\u3089\u306e\u7406\u8ad6\u306f\u3001\u591a\u304f\u306e\u4eba\u9593\u306e\u884c\u52d5\u304c\u4e0d\u78ba\u5b9f\u6027\u3092\u9ad8\u3081\u3088\u3046\u3068\u3057\u3066\u3044\u308b\u3060\u3051\u3067\u306a\u304f\u3001\u305d\u308c\u3092\u6e1b\u3089\u3059\u3053\u3068\u3092\u610f\u56f3\u3057\u3066\u3044\u308b\u3068\u3044\u3046\u6279\u5224\u3092\u53d7\u3051\u3066\u3044\u308b\u3002 \u4eba\u9593\u306f\u3001\u5b8c\u5168\u306b\u4e0d\u78ba\u5b9f\u306a\u72b6\u6cc1\u3068\u5b8c\u5168\u306b\u4e00\u5b9a\u306e\u72b6\u6cc1\u3068\u306e\u9593\u3067\u3001\u3042\u308b\u7a2e\u306e\u5f62\u614b\u306e\u6700\u9069\u6027\u3092\u63a2\u3059\u3088\u3046\u306b\u898b\u3048\u308b\u3002 Optimal incongruity. In 1965, Hunt developed the idea that children and adult look for optimal incongruity (Hunt, 1965 ). He regarded children as information-processing systems and stated that interesting stimuli were those where there was a discrepancy between the perceived and standard levels of the stimuli. \u6700\u9069\u306a\u9055\u548c\u611f\u3002(\u5168\u7136\u9055\u3046\u3063\u3066\u306e\u3082\u5acc\u3060\u3057\uff0c\u5408\u3044\u307e\u304f\u3063\u3066\u308b\u306e\u3082\u5acc\u3060\u3068\u3044\u3046\u6700\u9069\u306a\u3068\u3053\u308d\u304c\u3042\u308b\u611f\u3058) 1965\u5e74\u3001\u30cf\u30f3\u30c8\u306f\u5b50\u4f9b\u3068\u5927\u4eba\u304c\u6700\u9069\u306a\u9055\u548c\u611f\u3092\u899a\u3048\u308b\u3068\u3044\u3046\u8003\u3048\u3092\u767a\u9054\u3055\u305b\u305f\uff08Hunt\u30011965\uff09\u3002 \u5f7c\u306f\u5b50\u4f9b\u3092\u60c5\u5831\u51e6\u7406\u30b7\u30b9\u30c6\u30e0\u3068\u307f\u306a\u3057\u3001\u8208\u5473\u6df1\u3044\u523a\u6fc0\u306f\u3001\u523a\u6fc0\u306e\u77e5\u899a\u30ec\u30d9\u30eb\u3068\u6a19\u6e96\u30ec\u30d9\u30eb\u3068\u306e\u9593\u306b\u76f8\u9055\u304c\u3042\u3063\u305f\u523a\u6fc0\u3067\u3042\u308b\u3068\u8ff0\u3079\u305f\u3002 For, Dember and Earl, the incongruity or discrepancy in intrinsically-motivated behaviors was between a person\u2019s expectations and the properties of the stimulus (Dember and Earl, 1957 ). Dember\u3068Earl\u306b\u3068\u3063\u3066\u3001\u5185\u7684\u52d5\u6a5f\u4ed8\u3051\u3055\u308c\u305f\u884c\u52d5\u306e\u4e0d\u4e00\u81f4\u307e\u305f\u306f\u4e0d\u4e00\u81f4\u306f\u3001\u4eba\u306e\u671f\u5f85\u3068\u523a\u6fc0\u306e\u7279\u6027\u3068\u306e\u9593\u306b\u3042\u3063\u305f\uff08Dember and Earl\u30011957\uff09\u3002 Berlyne developed similar notions as he observed that the most rewarding situations were those with an intermediate level of novelty, between already familiar and completely new situations (Berlyne, 1960 ). Berlyne\u306f\u3001\u540c\u69d8\u306e\u6982\u5ff5\u3092\u767a\u5c55\u3055\u305b\uff0c\u6700\u3082\u5831\u916c\u306e\u9ad8\u3044\u72b6\u6cc1\u306f\u3001\u3059\u3067\u306b\u3088\u304f\u77e5\u3089\u308c\u3066\u3044\u308b\u72b6\u6cc1\u3068\u5b8c\u5168\u306b\u65b0\u3057\u3044\u72b6\u6cc1\u306e\u9593\u3067\u3001\u4e2d\u9593\u30ec\u30d9\u30eb\u306e\u65b0\u898f\u6027\u3092\u6301\u3064\u3082\u306e\u3067\u3042\u308b\u3068\u6c17\u3065\u3044\u305f\uff08Berlyne\u30011960\uff09\u3002 Motivation for effectance, personal causation, competence and self-determination. Eventually, a last group of researchers preferred the concept of challenge to the notion of optimal incongruity. These researchers stated that what was driving human behavior was a motivation for effectance (White, 1959 ), personal causation (De Charms, 1968 ), competence and self-determination (Deci and Ryan, 1985 ). \u5b9f\u52b9\u6027\u3001\u500b\u4eba\u7684\u56e0\u679c\u95a2\u4fc2\u3001\u80fd\u529b\u3068\u81ea\u5df1\u6c7a\u5b9a\u306e\u52d5\u6a5f\u4ed8\u3051\u3002 \u7d50\u5c40\u3001\u6700\u5f8c\u306e\u30b0\u30eb\u30fc\u30d7\u306e\u7814\u7a76\u8005\u306f\u3001\u6700\u9069\u306a\u9055\u548c\u611f\u3068\u3044\u3046\u6982\u5ff5\u3088\u308a\u3082\uff0c\u30c1\u30e3\u30ec\u30f3\u30b8\u306e\u6982\u5ff5\u3092\u512a\u5148\u3055\u305b\u307e\u3057\u305f\u3002 \u3053\u308c\u3089\u306e\u7814\u7a76\u8005\u306f\u3001\u4eba\u9593\u306e\u884c\u52d5\u3092\u52d5\u304b\u3059\u3082\u306e\u306f\u3001\u52b9\u679c\u306e\u30e2\u30c1\u30d9\u30fc\u30b7\u30e7\u30f3\uff08White\u30011959\uff09\u3001\u500b\u4eba\u7684\u56e0\u679c\u95a2\u4fc2\uff08De Charms\u30011968\uff09\u3001\u80fd\u529b\u3068\u81ea\u5df1\u6c7a\u5b9a\uff08Deci and Ryan\u30011985\uff09\u3067\u3042\u308b\u3068\u8ff0\u3079\u305f\u3002 Basically, these approaches argue that what motivates people is the degree of control they can have on other people, external objects and themselves, or in other words, the amount of effective interaction. In an analogous manner, the concept of optimal challenge has been put forward, such as for example in the theory of \u201cFlow\u201d (Csikszentmihalyi, 1991 ). \u57fa\u672c\u7684\u306b\u3001\u3053\u308c\u3089\u306e\u30a2\u30d7\u30ed\u30fc\u30c1\u306f\u3001\u4eba\u3005\u3092\u52d5\u6a5f\u3065\u3051\u308b\u3082\u306e\u306f\u3001\u4ed6\u4eba\u3001\u5916\u90e8\u306e\u7269\u4f53\u304a\u3088\u3073\u81ea\u5206\u81ea\u8eab\u306b\u4e0e\u3048\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u5236\u5fa1\u306e\u7a0b\u5ea6\u3001\u3059\u306a\u308f\u3061\u6709\u52b9\u306a\u76f8\u4e92\u4f5c\u7528\u306e\u91cf\u3067\u3042\u308b\u3068\u4e3b\u5f35\u3057\u3066\u3044\u308b\u3002 \u540c\u69d8\u306e\u65b9\u6cd5\u3067\u3001\u4f8b\u3048\u3070\u300c\u30d5\u30ed\u30fc\u300d\u7406\u8ad6\uff08Csikszentmihalyi\u30011991\uff09\u306e\u3088\u3046\u306a\u6700\u9069\u306a\u6311\u6226\u306e\u6982\u5ff5\u304c\u63d0\u5531\u3055\u308c\u3066\u3044\u308b\u3002 Motivation in Computational Systems: Extrinsic vs. Intrinsic and External vs. Internal After having made a broad review of intrinsic motivation in psychology, we will here start to take a computational viewpoint. To begin with, we will describe how motivations in general are conceived and used in computer and robotic architectures. We will then present a set of important distinctive dimensions, among which the intrinsic-extrinsic distinction, that are useful to organize the space of possible motivation systems. \u5fc3\u7406\u5b66\u306b\u304a\u3051\u308b\u672c\u8cea\u7684\u306a\u52d5\u6a5f\u3065\u3051\u3092\u5e45\u5e83\u304f\u898b\u76f4\u3057\u305f\u5f8c\u3001\u6211\u3005\u306f\u3053\u3053\u3067\u8a08\u7b97\u4e0a\u306e\u8996\u70b9\u3092\u53d6\u308a\u59cb\u3081\u308b\u3002 \u306f\u3058\u3081\u306b\u3001\u79c1\u305f\u3061\u306f\u3001\u4e00\u822c\u7684\u306a\u52d5\u6a5f\u304c\u30b3\u30f3\u30d4\u30e5\u30fc\u30bf\u30fc\u3084\u30ed\u30dc\u30c3\u30c8\u306e\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u30fc\u3067\u3069\u306e\u3088\u3046\u306b\u69cb\u60f3\u3055\u308c\u4f7f\u7528\u3055\u308c\u3066\u3044\u308b\u304b\u3092\u8aac\u660e\u3057\u307e\u3059\u3002 \u53ef\u80fd\u6027\u306e\u3042\u308b\u30e2\u30c1\u30d9\u30fc\u30b7\u30e7\u30f3\u30b7\u30b9\u30c6\u30e0\u306e\u7a7a\u9593\u3092\u6574\u7406\u3059\u308b\u306e\u306b\u6709\u7528\u306a\u5185\u5728\u7684 - \u5185\u5728\u7684\u306a\u533a\u5225\u306e\u4e2d\u3067\u91cd\u8981\u306a\u7279\u5fb4\u7684\u306a\u6b21\u5143\u306e\u30bb\u30c3\u30c8\u3092\u63d0\u793a\u3059\u308b\u3002 Motivational variables and drives. While motivation is sometimes implemented in an implicit manner in simple robot architectures, such as phototaxis in Braitenberg vehicles (Braitenberg, 1984 ), it is now rather common to implement it directly and explicitly in the form of a module that tracks the value of a number of internal \u201cmotivational\u201d variables and sends signals to the rest of the architecture (Arkin, 2005 ; Breazeal, 2002 ; Huang and Weng, 2004 ; Konidaris and Barto, 2006 ). \u610f\u6b32\u7684\u306a\u5909\u6570\u3068\u30c9\u30e9\u30a4\u30d6\u3002 \u52d5\u6a5f\u3065\u3051\u306f\u3001Braitenberg\u8eca\u4e21\u306e\u5149\u8d70\u6027\u306e\u3088\u3046\u306a\u7c21\u5358\u306a\u30ed\u30dc\u30c3\u30c8\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u3067\u306f\u6697\u9ed9\u7684\u306b\u5b9f\u88c5\u3055\u308c\u308b\u3053\u3068\u304c\u3042\u308b\u304c\uff08Braitenberg\u30011984\uff09\u3001\u6570\u5024\u306e\u5024\u3092\u8ffd\u8de1\u3059\u308b\u30e2\u30b8\u30e5\u30fc\u30eb\u306e\u5f62\u3067\u76f4\u63a5\u7684\u304b\u3064\u660e\u793a\u7684\u306b\u5b9f\u88c5\u3059\u308b\u3053\u3068\u306f\u304b\u306a\u308a\u4e00\u822c\u7684\u3067\u3042\u308b \uff08Arkin\u30012005; Breazeal\u30012002; Huang and Weng\u30012004; Konidaris and Barto\u30012006\uff09\u3002 For example, one often encounters an energy level variable, associated with a zone of comfort (i.e., a range of values), and when this variable gets out of this zone, the system sends signals to the rest of the architecture, and to the action selection module in particular, so that the robot finds a charging station as soon as possible. This homeostatic system can also be implemented as a Hullian drive (Hull, 1943 ; Konidaris and Barto, 2006 ), energy level being a variable ranging from 0 (totally unsatisfied) to 1 (satiated), and constantly sending its value to the action selection system in order to maintain it as close to 1 as possible. \u305f\u3068\u3048\u3070\u3001\u5feb\u9069\u5ea6\u306e\u30be\u30fc\u30f3\uff08\u5024\u306e\u7bc4\u56f2\uff09\u306b\u95a2\u9023\u4ed8\u3051\u3089\u308c\u305f\u30a8\u30cd\u30eb\u30ae\u30fc\u30ec\u30d9\u30eb\u306e\u5909\u6570\u306b\u983b\u7e41\u306b\u906d\u9047\u3057\u3001\u3053\u306e\u5909\u6570\u304c\u3053\u306e\u30be\u30fc\u30f3\u304b\u3089\u629c\u3051\u51fa\u3059\u3068\u3001\u30b7\u30b9\u30c6\u30e0\u306f\u6b8b\u308a\u306e\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u306b\u4fe1\u53f7\u3092\u9001\u4fe1\u3057\u3001 \u7279\u306b\u30ed\u30dc\u30c3\u30c8\u304c\u5145\u96fb\u30b9\u30c6\u30fc\u30b7\u30e7\u30f3\u3092\u3067\u304d\u308b\u3060\u3051\u65e9\u304f\u898b\u3064\u3051\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u3002 \u3053\u306e\u30db\u30e1\u30aa\u30b9\u30bf\u30b7\u30b9\u30fb\u30b7\u30b9\u30c6\u30e0\u306f\u30010\uff08\u5b8c\u5168\u306b\u6e80\u305f\u3055\u308c\u3066\u3044\u306a\u3044\uff09\u304b\u30891\uff08\u98fd\u548c\u72b6\u614b\uff09\u306e\u7bc4\u56f2\u306e\u5909\u6570\u3067\u3042\u308a\u3001\u5e38\u306b\u305d\u306e\u5024\u3092\u30a2\u30af\u30b7\u30e7\u30f3\u9078\u629e\u306b\u9001\u308b\u3001Hullian\u30c9\u30e9\u30a4\u30d6\uff08Hull\u30011943; Konidaris and Barto\u30012006\uff09 \u30b7\u30b9\u30c6\u30e0\u3092\u53ef\u80fd\u306a\u9650\u308a1\u306b\u8fd1\u3065\u3051\u308b\u3088\u3046\u306b\u3057\u3066\u304f\u3060\u3055\u3044\u3002 Computational Reinforcement Learning and rewards. It is often the case in robotic systems that the action strategy that allows to keep motivational variables as satiated as possible is neither fixed nor initially hand-coded, but rather should be learnt. The standard framework in which this happens is \u201ccomputational reinforcement learning (CRL)\u201d (Sutton and Barto, 1998 ). This framework has introduced many algorithms targeted at finding strategies to maximize \u201crewards\u201d, which is the pivotal concept of CRL. Very importantly, the meaning of the term \u201creward\u201d is used in a specific technical manner in CRL and is different from the meaning of the term \u201creward\u201d in psychology, and in particular in the theory of operant conditioning (Skinner, 1953 ). Nevertheless, these two meanings overlap and this has produced a number of confusions in the literature. In CRL, a \u201creward\u201d is technically only a numerical quantity that is measured continuously and used to drive the action selection mechanism so that the cumulated value of this quantity in the future is maximized. CRL theory is completely agnostic about what/how/where this value is generated. Coming back to robots implementing ethologically inspired motivation system, this value could be for example the value of the robot\u2019s internal level of energy. But, and this is how CRL is often used in the computational literature, this value could also be set directly by a human engineer or by an external program built by a human engineer. For example, a number of experiments in which engineers try to build robots that can walk forward have used CRL algorithms with a reward being a value coming from an external system (e.g., camera on the ceiling) observing how fast (or not) the robot moves (the value being the speed). It is in these experiments that the term \u201creward\u201d overlaps with the term \u201creward\u201d used in the operant conditioning literature, and where it denotes the getting of an external object/event/property such as money, food or a high grade at school. But one has to keep in mind that in a robot using CRL, a reward can be completely internally defined and be analogous to the very release of a neurotransmitter. Rewards as a common currency for multiple motivations. One of the nice features of the reward concept in CRL is that, being a numerical quantity, it can act as a \u201ccommon currency\u201d among several coexisting motivations in a single architecture (McFarland and Bosser, 1994 ). Indeed, in a typical organism, natural or artificial, different and possibly conflicting motives can try to push actions in certain directions: for example, one may have a drive for energy level maintenance co-existing with a drive for physical integrity maintenance, a drive for sleeping, and a drive pushing towards the search for social partners. In order to arbitrate between the possibly conflicting actions entailed by all these motivations, one uses the possibility to numerically compare the expected rewards associated with each of them. Moreover, one often sees architectures in which a (possibly adaptive) numerical weight is associated to each of these rewards (Konidaris and Barto, 2006 ). Internal vs. external motivations. Given this architectural framework for implementing motivations in a robot, one can investigate a first kind of distinction between internal and external motivations. This difference relates to autonomy and lies in the functional location of the mechanism that computes/generates the reward. If the reward, i.e., the numerical quantity that the system has to maximize, comes from the outside of the autonomous system, then it is called external. This is the above mentioned example of the walking robot driven by a reward coming from a human or a system with a camera mounted on the ceiling. If the reward is computed and generated internally by the autonomous system, then it is called internal. This is the above mentioned example of the reward associated to the satiation of an energy maintenance drive. This difference is summarized on Figure 1 . Yet, this difference can be sometimes subtle in the case of robots. Computers allow us to do manipulations that are impossible with humans. For example, an engineer could very well build an autonomous machine that is capable of monitoring by itself whether it is walking forward or not and at what speed, and could incorporate in the robot\u2019s internal architecture a motivation to go forward as fast as possible. In practice, this will produce more or less the same behavior that with the walking detection system mounted on the ceiling, but technically we have here an internal reward (which is nevertheless extrinsic as we will see). Of course, this kind of manipulation is not possible with humans, and it is much more difficult to find this kind of \u201climit\u201d example in humans.","title":"What is intrinsic motivation? A typology of computational approaches"},{"location":"lecture/robo/what is intristic motivation/#what-is-intrinsic-motivation-a-typology-of-computational-approaches","text":"","title":"What is intrinsic motivation? A typology of computational approaches"},{"location":"lecture/robo/what is intristic motivation/#introduction","text":"There exists a wide diversity of motivation systems in living organisms, and humans in particular. \u751f\u7269\u3084\u7279\u306b\u30d2\u30c8\u306b\u306f\u591a\u69d8\u306a\u30e2\u30c1\u30d9\u30fc\u30b7\u30e7\u30f3\u30b7\u30b9\u30c6\u30e0\u304c\u5b58\u5728\u3059\u308b\u3002 For example, there are systems that push the organism to maintain certain levels of chemical energy, involving the ingestion of food, or systems that push the organism to maintain its temperature or its physical integrity in a zone of viability. \u4f8b\u3048\u3070\u3001\u98df\u7269\u306e\u6442\u53d6\u3092\u542b\u3080\u4e00\u5b9a\u30ec\u30d9\u30eb\u306e\u5316\u5b66\u30a8\u30cd\u30eb\u30ae\u30fc\u3092\u7dad\u6301\u3059\u308b\u3088\u3046\u306b\u751f\u7269\u3092\u62bc\u3057\u51fa\u3059\u30b7\u30b9\u30c6\u30e0\u3001\u307e\u305f\u306f\u751f\u7269\u304c\u751f\u5b58\u30be\u30fc\u30f3\u5185\u3067\u6e29\u5ea6\u307e\u305f\u306f\u305d\u306e\u7269\u7406\u7684\u5b8c\u5168\u6027\u3092\u7dad\u6301\u3059\u308b\u3088\u3046\u306b\u30d7\u30c3\u30b7\u30e5\u3059\u308b\u30b7\u30b9\u30c6\u30e0\u304c\u3042\u308b\u3002 Inspired by these kinds of motivation and their understanding by (neuro-) ethologists, roboticists have built machines endowed with similar systems with the aim of providing them with autonomy and properties of life-like intelligence (Arkin, 2005 ). \u30ed\u30dc\u30c3\u30c8\u5de5\u5b66\u8005\u306f\u3001\u3053\u308c\u3089\u306e\u7a2e\u985e\u306e\u30e2\u30c1\u30d9\u30fc\u30b7\u30e7\u30f3\u3068\uff08\u795e\u7d4c\uff09\u751f\u7269\u5b66\u8005\u306b\u3088\u308b\u7406\u89e3\u306b\u3088\u3063\u3066\u30a4\u30f3\u30b9\u30d4\u30ec\u30fc\u30b7\u30e7\u30f3\u3092\u5f97\u3066\u3001\u540c\u69d8\u306e\u30b7\u30b9\u30c6\u30e0\u3092\u4e0e\u3048\u3089\u308c\u305f\u6a5f\u68b0\u3092\u69cb\u7bc9\u3057\u3001\u81ea\u5f8b\u6027\u3068\u751f\u547d\u306e\u3088\u3046\u306a\u77e5\u6027\u306e\u7279\u6027\u3092\u63d0\u4f9b\u3059\u308b\u3053\u3068\u3092\u76ee\u6307\u3057\u3066\u3044\u308b\uff08Arkin\u30012005\uff09\u3002 For example sowbug-inspired robots (Endo and Arkin, 2001 ), praying mantis robots (Arkin et al., 1998 ) dog-like robots (Fujita et al., 2001 ) have been constructed. \u4f8b\u3048\u3070\u3001\u30bd\u30a6\u30d0\u30b0\b(\u4e38\u866b)\u306b\u89e6\u767a\u3055\u308c\u305f\u30ed\u30dc\u30c3\u30c8\uff08Endo and Arkin\u30012001\uff09\u3001\u30ab\u30de\u30ad\u30ea\u306e\u30ed\u30dc\u30c3\u30c8\uff0c\u72ac\u306e\u3088\u3046\u306a\u30ed\u30dc\u30c3\u30c8\uff08Arkin et al\u3002\u30011998\uff09\uff082001\u5e74\uff09\u304c\u4f5c\u3089\u308c\u3066\u304d\u305f\u3002 Some animals, and this is most prominent in humans, also have more general motivations that push them to explore, manipulate or probe their environment, fostering curiosity and engagement in playful and new activities. This kind of motivation, which is called intrinsic motivation by psychologists (Ryan and Deci, 2000 ), is paramount for sensorimotor and cognitive development throughout lifespan. \u4eba\u9593\u306e\u4e2d\u3067\u6700\u3082\u9855\u8457\u306a\u52d5\u7269\u3082\u3042\u308c\u3070\u3001\u5f7c\u3089\u306e\u74b0\u5883\u3092\u63a2\u7d22\u3057\u3001\u64cd\u4f5c\u3057\u305f\u308a\u63a2\u691c\u3057\u305f\u308a\u3001\u904a\u3073\u5fc3\u306e\u3042\u308b\u65b0\u3057\u3044\u6d3b\u52d5\u306e\u597d\u5947\u5fc3\u3084\u95a2\u4e0e\u3092\u4fc3\u3059\u3088\u3046\u306a\u4e00\u822c\u7684\u306a\u52d5\u6a5f\u3082\u3042\u308a\u307e\u3059\u3002 \u5fc3\u7406\u5b66\u8005\uff08Ryan and Deci\u30012000\uff09\u306b\u3088\u308b\u5185\u767a\u7684\u52d5\u6a5f\u3065\u3051\u3068\u547c\u3070\u308c\u308b\u3053\u306e\u7a2e\u306e\u30e2\u30c1\u30d9\u30fc\u30b7\u30e7\u30f3\u306f\u3001\u5bff\u547d\u3092\u901a\u3058\u3066\u611f\u899a\u904b\u52d5\u304a\u3088\u3073\u8a8d\u77e5\u767a\u9054\u306b\u6700\u3082\u91cd\u8981\u3067\u3042\u308b\u3002 There is a vast literature in psychology that explains why it is essential for cognitive growth and organization, and investigates the actual potential cognitive processes underlying intrinsic motivation (Berlyne, 1960 ; Csikszentmihalyi, 1991 ; Deci and Ryan, 1985 ; Ryan and Deci, 2000 ; White, 1959 ). This has gathered the interest of a growing number of researchers in developmental robotics in the recent years, and several computational models have been developed (see Barto et al., 2004 ; Oudeyer et al., 2007 for reviews). \u306a\u305c\u3001\u305d\u308c\u304c\u8a8d\u77e5\u306e\u6210\u9577\u3068\u7d44\u7e54\u306b\u3068\u3063\u3066\u4e0d\u53ef\u6b20\u306a\u306e\u304b\u3092\u8aac\u660e\u3057\u3001\u5185\u5728\u7684\u306a\u52d5\u6a5f\u3065\u3051\u306e\u6839\u5e95\u306b\u3042\u308b\u5b9f\u969b\u306e\u6f5c\u5728\u7684\u306a\u8a8d\u77e5\u904e\u7a0b\u3092\u8abf\u3079\u308b\u5fc3\u7406\u5b66\u306e\u5e83\u7bc4\u306a\u6587\u732e\u304c\u3042\u308b\uff08Berlyne\u30011960; Csikszentmihalyi\u30011991; Deci and Ryan\u30011985; Ryan and Deci\u3001 White\u30011959\uff09\u3002 \u3053\u308c\u306f\u8fd1\u5e74\u3001\u958b\u767a\u30ed\u30dc\u30c3\u30c8\u306e\u7814\u7a76\u8005\u6570\u304c\u5897\u52a0\u3057\u3066\u3044\u308b\u3053\u3068\u306e\u95a2\u5fc3\u3092\u96c6\u3081\u3066\u304a\u308a\u3001\u3044\u304f\u3064\u304b\u306e\u8a08\u7b97\u30e2\u30c7\u30eb\u304c\u958b\u767a\u3055\u308c\u3066\u3044\u308b\uff08Barto et al\u3002\u30012004; Oudeyer et al\u3002\u30012007\uff09\u3002 However, the very concept of intrinsic motivation has never really been consistently and critically discussed from a computational point of view. It has been used intuitively by many authors without asking for what it really means. Thus, the first objective and contribution of this paper is to present an overview of this concept in psychology followed by a critical reinterpretation in computational terms. \u3057\u304b\u3057\u3001\u672c\u8cea\u7684\u306a\u52d5\u6a5f\u3065\u3051\u3068\u3044\u3046\u6982\u5ff5\u306f\u3001\u8a08\u7b97\u4e0a\u3001\u4e00\u8cab\u3057\u3066\u6279\u5224\u7684\u306b\u8b70\u8ad6\u3055\u308c\u305f\u3053\u3068\u306f\u3042\u308a\u307e\u305b\u3093\u3002 \u305d\u308c\u306f\u3001\u5b9f\u969b\u306b\u4f55\u3092\u610f\u5473\u3059\u308b\u306e\u304b\u3092\u554f\u308f\u305a\u3001\u591a\u304f\u306e\u8457\u8005\u306b\u3088\u3063\u3066\u76f4\u611f\u7684\u306b\u4f7f\u7528\u3055\u308c\u3066\u304d\u307e\u3057\u305f\u3002 \u3057\u305f\u304c\u3063\u3066\u3001\u3053\u306e\u8ad6\u6587\u306e\u7b2c\u4e00\u306e\u76ee\u7684\u3068\u8ca2\u732e\u306f\u3001\u5fc3\u7406\u5b66\u306b\u304a\u3051\u308b\u3053\u306e\u6982\u5ff5\u306e\u6982\u8981\u3092\u63d0\u793a\u3057\u3001\u6b21\u306b\u8a08\u7b97\u4e0a\u306e\u91cd\u8981\u306a\u518d\u89e3\u91c8\u3092\u63d0\u793a\u3059\u308b\u3053\u3068\u3067\u3042\u308b\u3002 We show that the definitions provided in psychology are actually unsatisfying. As a consequence, we will set the ground for a systematic operational study of intrinsic motivation by presenting a typology of possible computational approaches, and discuss whether it is possible or useful to give a single general computational definition of intrinsic motivation. \u79c1\u305f\u3061\u306f\u3001\u5fc3\u7406\u5b66\u3067\u63d0\u4f9b\u3055\u308c\u308b\u5b9a\u7fa9\u304c\u5b9f\u969b\u306b\u306f\u6e80\u8db3\u3067\u304d\u306a\u3044\u3053\u3068\u3092\u793a\u3057\u3066\u3044\u307e\u3059\u3002 \u7d50\u679c\u3068\u3057\u3066\u3001\u53ef\u80fd\u306a\u8a08\u7b97\u624b\u6cd5\u306e\u985e\u578b\u3092\u63d0\u793a\u3057\u3001\u5185\u5728\u7684\u306a\u52d5\u6a5f\u3065\u3051\u306e\u5358\u4e00\u306e\u4e00\u822c\u7684\u306a\u8a08\u7b97\u4e0a\u306e\u5b9a\u7fa9\u3092\u4e0e\u3048\u308b\u3053\u3068\u304c\u53ef\u80fd\u3067\u3042\u308b\u304b\u6709\u7528\u3067\u3042\u308b\u304b\u3092\u8b70\u8ad6\u3059\u308b\u3053\u3068\u306b\u3088\u3063\u3066\u3001\u5185\u5728\u7684\u306a\u52d5\u6a5f\u3065\u3051\u306e\u4f53\u7cfb\u7684\u306a\u904b\u7528\u7814\u7a76\u306e\u6839\u62e0\u3092\u5b9a\u3081\u308b\u3002 The typology that we will present is partly based on existing computational models, but also presents new ways of conceptualizing intrinsic motivation. We will try to focus on how these models relate to each other and propose a classification into broad but distinct categories. \u6211\u3005\u304c\u63d0\u793a\u3059\u308b\u985e\u578b\u5b66\u306f\u3001\u65e2\u5b58\u306e\u8a08\u7b97\u30e2\u30c7\u30eb\u306b\u90e8\u5206\u7684\u306b\u57fa\u3065\u3044\u3066\u3044\u308b\u304c\u3001\u5185\u5728\u7684\u306a\u52d5\u6a5f\u3065\u3051\u3092\u6982\u5ff5\u5316\u3059\u308b\u65b0\u3057\u3044\u65b9\u6cd5\u3092\u63d0\u793a\u3059\u308b\u3002 \u3053\u308c\u3089\u306e\u30e2\u30c7\u30eb\u304c\u4e92\u3044\u306b\u3069\u306e\u3088\u3046\u306b\u95a2\u4fc2\u3057\u3066\u3044\u308b\u304b\u306b\u7126\u70b9\u3092\u5f53\u3066\u3066\u3001\u5e83\u7bc4\u3067\u306f\u3063\u304d\u308a\u3057\u305f\u30ab\u30c6\u30b4\u30ea\u30fc\u306b\u5206\u985e\u3059\u308b\u3053\u3068\u3092\u63d0\u6848\u3057\u307e\u3059\u3002","title":"Introduction"},{"location":"lecture/robo/what is intristic motivation/#_1","text":"","title":"(\u4e00\u4eba\u76ee\u306f\u3053\u3053\u307e\u3067)"},{"location":"lecture/robo/what is intristic motivation/#intrinsic-motivation-from-the-psychologists-point-of-view","text":"\u5fc3\u7406\u5b66\u8005\u306e\u8996\u70b9\u304b\u3089\u306e\u672c\u8cea\u7684\u306a\u52d5\u6a5f\u3065\u3051","title":"Intrinsic Motivation from the Psychologist\u2019s Point of View"},{"location":"lecture/robo/what is intristic motivation/#intrinsic-motivation-and-instrumentalization","text":"\u672c\u8cea\u7684\u306a\u52d5\u6a5f\u4ed8\u3051\u3068\u5668\u68b0\u5316(\u5f79\u306b\u7acb\u305f\u305b\u308b) According to Ryan and Deci (2000) (pp. 56), Intrinsic motivation is defined as the doing of an activity for its inherent satisfaction rather than for some separable consequence. When intrinsically motivated, a person is moved to act for the fun or challenge entailed rather than because of external products, pressures, or rewards. Ryan and Deci\uff082000\uff09\uff08pp\u300256\uff09\u306b\u3088\u308b\u3068\u3001 \u5185\u5728\u7684\u306a\u52d5\u6a5f\u3065\u3051\u3068\u306f\u3001\u5206\u96e2\u53ef\u80fd\u306a\u7d50\u679c\u304c\u751f\u3058\u308b\u306e\u3067\u306f\u306a\u304f\u3001\u5185\u5728\u3059\u308b\u6e80\u8db3\u306e\u305f\u3081\u306e\u6d3b\u52d5\u3092\u884c\u3046\u3053\u3068\u3067\u3059\u3002 \u672c\u8cea\u7684\u306b\u52d5\u6a5f\u4ed8\u3051\u3055\u308c\u308b\u3068\u3001\u5916\u90e8\u306e\u88fd\u54c1\u3001\u5727\u529b\u3001\u307e\u305f\u306f\u5831\u916c\u306e\u305f\u3081\u306b\u3067\u306f\u306a\u304f\u3001\u4eba\u304c\u697d\u3057\u304f\u3084\u308a\u305f\u3044\u884c\u52d5\u3092\u3059\u308b\u3088\u3046\u52d5\u304b\u3055\u308c\u307e\u3059\u3002 Intrinsic motivation is clearly visible in young infants, that consistently try to grasp, throw, bite, squash or shout at new objects they encounter. Even if less important as they grow, human adults are still often intrinsically motivated while they play crosswords, make paintings, do gardening or just read novels or watch movies. Yet, to get a clearer picture of intrinsic motivation, one needs to understand that it has been defined by contrast to extrinsic motivation: \u5185\u56e0\u6027\u306e\u52d5\u6a5f\u3065\u3051\u306f\u3001\u5e7c\u5150\u306b\u306f\u3063\u304d\u308a\u3068\u76ee\u306b\u898b\u3048\u307e\u3059\u3002\u5e7c\u5150\u306f\u3001\u906d\u9047\u3059\u308b\u65b0\u3057\u3044\u7269\u4f53\u3092\u5e38\u306b\u628a\u63e1\u3057\u3001\u6295\u3052\u305f\u308a\u3001\u565b\u3093\u3060\u308a\u3057\u305f\u308a\u3001\u53eb\u3076\u3088\u3046\u306b\u3057\u307e\u3059\u3002 \u5f7c\u3089\u304c\u6210\u9577\u3059\u308b\u306b\u3064\u308c\u3066\u305d\u308c\u307b\u3069\u91cd\u8981\u3067\u306f\u306a\u3044\u3068\u3057\u3066\u3082\u3001\u4eba\u9593\u306e\u5927\u4eba\u306f\u3001\u30af\u30ed\u30b9\u30ef\u30fc\u30c9\u3092\u6f14\u3058\u305f\u308a\u3001\u7d75\u3092\u63cf\u3044\u305f\u308a\u3001\u30ac\u30fc\u30c7\u30cb\u30f3\u30b0\u3092\u3057\u305f\u308a\u3001\u5c0f\u8aac\u3092\u8aad\u3093\u3060\u308a\u3001\u6620\u753b\u3092\u898b\u305f\u308a\u3057\u3066\u3044\u308b\u9593\u3001\u4f9d\u7136\u3068\u3057\u3066\u672c\u8cea\u7684\u306b\u52d5\u6a5f\u4ed8\u3051\u3089\u308c\u3066\u3044\u307e\u3059\u3002 \u3057\u304b\u3057\u3001\u5185\u5728\u7684\u306a\u30e2\u30c1\u30d9\u30fc\u30b7\u30e7\u30f3\u306e\u660e\u78ba\u306a\u30a4\u30e1\u30fc\u30b8\u3092\u5f97\u308b\u305f\u3081\u306b\u306f\u3001\u305d\u308c\u304c\u5916\u7684\u30e2\u30c1\u30d9\u30fc\u30b7\u30e7\u30f3\u3068\u306f\u5bfe\u7167\u7684\u306b\u5b9a\u7fa9\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u7406\u89e3\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002 Extrinsic motivation is a construct that pertains whenever an activity is done in order to attain some separable outcome. Extrinsic motivation thus contrasts with intrinsic motivation, which refers to doing an activity simply for the enjoyment of the activity itself, rather than its instrumental value. (Ryan and Deci, 2000 ) \u5185\u56e0\u6027\u306e\u52d5\u6a5f\u3065\u3051\u306f\u3001\u5206\u96e2\u53ef\u80fd\u306a\u7d50\u679c\u3092\u9054\u6210\u3059\u308b\u305f\u3081\u306b\u6d3b\u52d5\u304c\u884c\u308f\u308c\u308b\u305f\u3073\u306b\u95a2\u4fc2\u3059\u308b\u69cb\u6210\u7269\u3067\u3042\u308b\u3002 \u3057\u305f\u304c\u3063\u3066\u5185\u5728\u7684\u306a\u52d5\u6a5f\u3065\u3051\u306f\u5185\u5728\u7684\u306a\u52d5\u6a5f\u3065\u3051\u3068\u5bfe\u7167\u7684\u3067\u3042\u308a\u3001\u305d\u308c\u306f\u5358\u306b\bintrinsic motivation\u3088\u308a\u3082\u3080\u3057\u308d\u6d3b\u52d5\u305d\u306e\u3082\u306e\u3092\u697d\u3057\u3080\u305f\u3081\u306e\u6d3b\u52d5\u3092\u5358\u306b\u884c\u3046\u3053\u3068\u3092\u6307\u3059\u3002 \uff08Ryan and Deci\u30012000\uff09 We see that a central feature that differentiates intrinsic and extrinsic motivation is instrumentalization. We also see that the concepts of intrinsic and extrinsic motivations form a different distinction than the one between internal and external motivations. In the computational literature, \u201cintrinsic\u201d is sometimes used as a synonym to \u201cinternal\u201d, and \u201cextrinsic\u201d as a synonym to \u201cexternal\u201d. Yet, it is in fact a confusion. Indeed, there are extrinsic motivations that can be internal and vice versa. In fact, there are different kinds of instrumentalizations that can be classified as more or less self-determined (Ryan and Deci, 2000 ). Let us give examples to be more clear. \u5185\u5728\u6027\u3068\u5916\u56e0\u6027\u306e\u52d5\u6a5f\u3065\u3051\u3092\u533a\u5225\u3059\u308b\u4e2d\u5fc3\u7684\u306a\u7279\u5fb4\u306f\u6a5f\u5668\u5316\u3067\u3042\u308b\u3053\u3068\u304c\u308f\u304b\u308a\u307e\u3059\u3002 \u307e\u305f\u3001\u5185\u5728\u7684\u304a\u3088\u3073\u5916\u7684\u306a\u52d5\u6a5f\u306e\u6982\u5ff5\u306f\u3001\u5185\u7684\u304a\u3088\u3073\u5916\u7684\u306a\u52d5\u6a5f\u3068\u306f\u7570\u306a\u308b\u533a\u5225\u3092\u5f62\u6210\u3059\u308b\u3053\u3068\u304c\u308f\u304b\u308a\u307e\u3059\u3002 \u6587\u732e\u3067\u306f\u3001\u300c\u5185\u5728\u300d\u306f\u6642\u306b\u306f\u300c\u5185\u90e8\u300d\u306e\u540c\u7fa9\u8a9e\u3068\u3057\u3066\u4f7f\u7528\u3055\u308c\u3001\u300c\u5916\u7684\u300d\u306f\u300c\u5916\u90e8\u300d\u306e\u540c\u7fa9\u8a9e\u3068\u3057\u3066\u4f7f\u7528\u3055\u308c\u307e\u3059\u3002 \u3057\u304b\u3057\u3001\u305d\u308c\u306f\u5b9f\u969b\u306b\u306f\u6df7\u4e71\u3092\u62db\u304d\u307e\u3059\u3002 \u78ba\u304b\u306b\u3001\u5185\u90e8\u7684\u306a\u3053\u3068\u304c\u3067\u304d\u308b\u5916\u7684\u306a\u52d5\u6a5f\u304c\u3042\u308a\u3001\u305d\u306e\u9006\u3082\u3042\u308a\u307e\u3059\u3002 \u5b9f\u969b\u306b\u306f\u3001\u591a\u304b\u308c\u5c11\u306a\u304b\u308c\u81ea\u5df1\u6c7a\u5b9a\u3055\u308c\u305f\u3082\u306e\u3068\u3057\u3066\u5206\u985e\u3055\u308c\u5f97\u308b\u69d8\u3005\u306a\u6a5f\u5668\u5316\u304c\u3042\u308b\uff08Ryan and Deci\u30012000\uff09\u3002 \u3082\u3063\u3068\u660e\u78ba\u306b\u3059\u308b\u305f\u3081\u306b\u4f8b\u3092\u6319\u3052\u307e\u3057\u3087\u3046\u3002 For example, a child that does thoroughly his homework might be motivated by avoiding the sanctions of his parents if he would not do it. The cause for action is here clearly external, and the homework is not done for its own sake but for the separate outcome of not getting sanctions. Here the child is extrinsically and externally motivated. \u4f8b\u3048\u3070\u3001\u5f7c\u306e\u5bbf\u984c\u3092\u5fb9\u5e95\u7684\u306b\u884c\u3046\u5b50\u4f9b\u306f\u3001\u5f7c\u304c\u305d\u308c\u3092\u3057\u306a\u3044\u306a\u3089\u3001\u4e21\u89aa\u306e\u5236\u88c1\u3092\u907f\u3051\u308b\u3053\u3068\u306b\u3088\u3063\u3066\u52d5\u6a5f\u3065\u3051\u308b\u304b\u3082\u3057\u308c\u306a\u3044\u3002 \u884c\u52d5\u306e\u539f\u56e0\u306f\u3053\u3053\u3067\u306f\u660e\u3089\u304b\u306b\u5916\u7684\u3067\u3042\u308a\u3001\u5bbf\u984c\u306f\u5b50\u3069\u3082\u81ea\u8eab\u306e\u305f\u3081\u3067\u306f\u306a\u304f\u3001\u5236\u88c1\u3092\u53d7\u3051\u306a\u3044\u5225\u306e\u7d50\u679c\u306e\u305f\u3081\u306b\u884c\u308f\u308c\u308b\u3002 \u3053\u308c\u304c\u5b50\u4f9b\u306f\u5916\u90e8\u7684\u304b\u3064\u5916\u90e8\u7684\u306b\u52d5\u6a5f\u4ed8\u3051\u3089\u308c\u3066\u3044\u308b\u3002 On the other hand, it is possible that a child could do thoroughly his homework because he is persuaded that it will help him get the job he dreams of, later when he will be an adult. In this case, the cause for action is internally generated, and the homework is again not achieved for its own sake but because the child thinks it will lead to the separate outcome of getting a good job. \u4e00\u65b9\u3001\u5b50\u4f9b\u304c\u5927\u4eba\u306b\u306a\u308b\u3068\u304d\u306b\u3001\u5f7c\u304c\u5922\u898b\u308b\u4ed5\u4e8b\u3092\u5f97\u308b\u306e\u306b\u5f79\u7acb\u3064\u3068\u8aac\u5f97\u3055\u308c\u3066\u3044\u308b\u306e\u3067\u3001\u5b50\u4f9b\u304c\u5fb9\u5e95\u7684\u306b\u5bbf\u984c\u3092\u3059\u308b\u3053\u3068\u306f\u53ef\u80fd\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u3002 \u3053\u306e\u5834\u5408\u3001\u884c\u52d5\u306e\u539f\u56e0\u306f\u5185\u90e8\u7684\u306b\u751f\u6210\u3055\u308c\u3001\u5bbf\u984c\u306f\u81ea\u5206\u306e\u305f\u3081\u306b\u9054\u6210\u3055\u308c\u308b\u306e\u3067\u306f\u306a\u304f\u3001\u5b50\u3069\u3082\u304c\u826f\u3044\u4ed5\u4e8b\u3092\u5f97\u308b\u3068\u3044\u3046\u5225\u306e\u7d50\u679c\u306b\u3064\u306a\u304c\u308b\u3068\u8003\u3048\u3066\u3044\u308b\u304b\u3089\u3067\u3059\u3002 Finally, it is also possible that a child does thoroughly its homework for the fun of it, and because he experiences pleasure in the discovery of new knowledge or considers for example its math problem just as fun as playing a video game. In this case, its behavior is intrinsically (and internally) motivated. \u6700\u5f8c\u306b\u3001\u5b50\u4f9b\u304c\u305d\u308c\u3092\u697d\u3057\u3080\u305f\u3081\u306e\u5bbf\u984c\u3092\u5fb9\u5e95\u7684\u306b\u884c\u3046\u3053\u3068\u3084\u3001\u65b0\u3057\u3044\u77e5\u8b58\u306e\u767a\u898b\u306b\u559c\u3073\u3092\u611f\u3058\u305f\u308a\u3001\u30d3\u30c7\u30aa\u30b2\u30fc\u30e0\u3092\u30d7\u30ec\u30a4\u3059\u308b\u306e\u3068\u540c\u3058\u304f\u3089\u3044\u697d\u3057\u3044\u3068\u3044\u3046\u6570\u5b66\u7684\u554f\u984c\u3092\u8003\u616e\u3057\u305f\u308a\u3059\u308b\u3053\u3068\u3082\u3042\u308a\u307e\u3059\u3002 \u3053\u306e\u5834\u5408\u3001\u305d\u306e\u632f\u821e\u3044\u306f\u672c\u8cea\u7684\u306b\uff08\u305d\u3057\u3066\u5185\u90e8\u7684\u306b\uff09\u5185\u7684\u52d5\u6a5f\u3068\u8003\u3048\u3089\u308c\u307e\u3059\u3002 These different kinds of motivations can also sometimes be superposed or interleaved in the same global activity. For example, it is quite possible that a child doing his homework is partly extrinsically motivated by getting a high grade at the exam and partly intrinsically motivated by learning new interesting things. Also, for example, imagine a child that is intrinsically motivated by playing tennis but has to ride its bicycle to get to the tennis court (and does not like particularly riding bicycles). In this case, the riding of the bicycle is an internal and extrinsically motivated behavior that spins out of the intrinsically motivated behavior of playing tennis. \u3053\u308c\u3089\u306e\u7570\u306a\u308b\u7a2e\u985e\u306e\u52d5\u6a5f\u306f\u3001\u6642\u306b\u306f\u540c\u3058\u30b0\u30ed\u30fc\u30d0\u30eb\u306a\u6d3b\u52d5\u306b\u91cd\u306d\u5408\u308f\u3055\u308c\u305f\u308a\u3001\u30a4\u30f3\u30bf\u30fc\u30ea\u30fc\u30d6\u3055\u308c\u305f\u308a\u3059\u308b\u3053\u3068\u3082\u3042\u308a\u307e\u3059\u3002 \u4f8b\u3048\u3070\u3001\u5b50\u4f9b\u304c\u5bbf\u984c\u3092\u3057\u3066\u3044\u308b\u3053\u3068\u306f\u3001\u90e8\u5206\u7684\u306b\u306f\u3001\u8a66\u9a13\u3067\u7279\u5178\u3092\u5f97\u3066\u3001\u4e00\u90e8\u306f\u672c\u8cea\u7684\u306b\u65b0\u3057\u3044\u8208\u5473\u6df1\u3044\u3053\u3068\u3092\u5b66\u3076\u3053\u3068\u306b\u3088\u3063\u3066\u52d5\u6a5f\u3065\u3051\u3089\u308c\u3066\u3044\u308b\u53ef\u80fd\u6027\u304c\u3042\u308a\u307e\u3059\u3002 \u307e\u305f\u3001\u4f8b\u3048\u3070\u3001\u30c6\u30cb\u30b9\u3092\u3057\u3066\u672c\u8cea\u7684\u306b\u52d5\u6a5f\u4ed8\u3051\u3089\u308c\u3066\u3044\u308b\u304c\u3001\u81ea\u8ee2\u8eca\u306b\u4e57\u3063\u3066\u30c6\u30cb\u30b9\u30b3\u30fc\u30c8\u306b\u7740\u304f\uff08\u7279\u306b\u81ea\u8ee2\u8eca\u304c\u597d\u304d\u3067\u306f\u306a\u3044\uff09\u5b50\u4f9b\u3092\u60f3\u50cf\u3057\u3066\u307f\u3066\u304f\u3060\u3055\u3044\u3002 \u3053\u306e\u5834\u5408\u3001\u81ea\u8ee2\u8eca\u306e\u4e57\u8eca\u306f\u3001\u672c\u8cea\u7684\u306b\u52d5\u6a5f\u4ed8\u3051\u3055\u308c\u305f\u30c6\u30cb\u30b9\u306e\u632f\u308b\u821e\u3044\u304b\u3089\u98db\u3073\u51fa\u3059\u3001\u5185\u7684\u304b\u3064\u5185\u5728\u7684\u306b\u52d5\u6a5f\u4ed8\u3051\u3055\u308c\u305f\u884c\u52d5\u3067\u3042\u308b\u3002 question: \b\u5185\u7684\u3068\u5916\u7684\u306a\u52d5\u6a5f\u3065\u3051\u306f\u3069\u3046\u306a\u308b\uff1f","title":"Intrinsic Motivation and Instrumentalization"},{"location":"lecture/robo/what is intristic motivation/#what-makes-an-activity-intrinsically-motivating","text":"Given this broad distinction between intrinsic and extrinsic motivation, psychologists have tried to build theories about which features of activities make them intrinsically motivating for some people (and not all) at some times (the same activity might be intrinsically motivating for a person at a given time, but no more later on). They have studied how these motivations could be functionally implemented in an organism, humans in particular, and several theoretical directions have been presented. \u5fc3\u7406\u5b66\u8005\u306f\u5185\u5728\u6027\u3068\u5916\u56e0\u6027\u306e\u30e2\u30c1\u30d9\u30fc\u30b7\u30e7\u30f3\u306e\u9593\u306b\u3053\u306e\u5e83\u3044\u5dee\u7570\u304c\u3042\u308b\u3053\u3068\u3092\u8003\u616e\u3057\u3066\u3001\u3042\u308b\u7a2e\u306e\u4eba\u3005\uff08\u305d\u3057\u3066\u3059\u3079\u3066\u3067\u306f\u306a\u3044\uff09\u306b\u3068\u3063\u3066\u672c\u8cea\u7684\u306a\u52d5\u6a5f\u3068\u306a\u308b\u7406\u8ad6\u3092\u69cb\u7bc9\u3057\u3088\u3046\u3068\u8a66\u307f\u3066\u304d\u305f\uff08\u540c\u3058\u30a2\u30af\u30c6\u30a3\u30d3\u30c6\u30a3\u306f\u3001 but no more later on\uff09\u3002 \u5f7c\u3089\u306f\u3001\u3053\u308c\u3089\u306e\u52d5\u6a5f\u304c\u751f\u7269\u3001\u7279\u306b\u30d2\u30c8\u306b\u304a\u3044\u3066\u6a5f\u80fd\u7684\u306b\u3069\u306e\u3088\u3046\u306b\u5b9f\u65bd\u3055\u308c\u5f97\u308b\u304b\u3092\u7814\u7a76\u3057\u3001\u3044\u304f\u3064\u304b\u306e\u7406\u8ad6\u7684\u6307\u91dd\u304c\u63d0\u793a\u3055\u308c\u3066\u3044\u308b\u3002 Drives to manipulate, drives to explore. In the 1950s, psychologists started by trying to give an account of intrinsic motivation and exploratory activities on the basis of the theory of drives (Hull, 1943 ), which are specific tissue deficits like hunger or pain that the organisms try to reduce. For example, (Montgomery, 1954 ) proposed a drive for exploration and (Harlow, 1950 ) a drive to manipulate. This drive naming approach had many short-comings which were criticized in detail by White (1959) : drive: \u52d5\u6a5f\u3065\u3051\uff1f \u52d5\u6a5f\u3065\u3051\u306b\u3088\u3063\u3066\u64cd\u4f5c\u3057\u3001\u63a2\u7d22\u306e\u305f\u3081\u306b\u52d5\u6a5f\u3065\u3051\u3057\u307e\u3059\u3002 1950\u5e74\u4ee3\u3001\u5fc3\u7406\u5b66\u8005\u306f\u3001\u98e2\u3048\u3084\u75db\u307f\u306e\u3088\u3046\u306a\u7279\u5b9a\u306e\u7d44\u7e54\u6b20\u640d\u3067\u3042\u308a\u3001\u751f\u7269\u304c\u6e1b\u5c11\u3055\u305b\u3088\u3046\u3068\u3057\u3066\u3044\u308b\u3001\u52d5\u6a5f\u3065\u3051\u7406\u8ad6\uff08Hull\u30011943\uff09\u306b\u57fa\u3065\u3044\u3066\u3001\u5185\u5728\u7684\u306a\u52d5\u6a5f\u4ed8\u3051\u3068\u63a2\u7d22\u7684\u6d3b\u52d5\u306e\u8aac\u660e\u3092\u8a66\u307f\u308b\u3053\u3068\u304b\u3089\u59cb\u3081\u305f\u3002 \u4f8b\u3048\u3070\u3001\uff08Montgomery\u30011954\uff09\u306f\u3001\u63a2\u7d22\u306e\u305f\u3081\u306e\u52d5\u6a5f\u3065\u3051\u3092\u63d0\u6848\u3057\uff08Harlow\u30011950\uff09\u3001\u64cd\u4f5c\u3059\u308b\u305f\u3081\u306e\u52d5\u6a5f\u3065\u3051\u3092\u63d0\u6848\u3057\u305f\u3002 \u3053\u306e\u52d5\u6a5f\u3065\u3051\u3068\u547d\u540d\u3055\u308c\u305f\u30a2\u30d7\u30ed\u30fc\u30c1\u306b\u306f\u3001white\uff081959\uff09\u306b\u3088\u3063\u3066\u8a73\u7d30\u306b\u6279\u5224\u3055\u308c\u305f\u591a\u304f\u306e\u77ed\u6240\u304c\u3042\u308a\u307e\u3057\u305f\u3002 intrinsically motivated exploratory activities have a fundamentally different dynamics. Indeed, they are not homeostatic: the general tendency to explore is not a consummatory response to a stressful perturbation of the organism\u2019s body. \u672c\u8cea\u7684\u306b\u52d5\u6a5f\u4ed8\u3051\u3055\u308c\u305f\u63a2\u7d22\u7684\u6d3b\u52d5\u306f\u3001\u6839\u672c\u7684\u306b\u7570\u306a\u308b\u52d5\u529b\u5b66\u3092\u6709\u3059\u308b\u3002 \u78ba\u304b\u306b\u3001\u5f7c\u3089\u306f\u6052\u5e38\u6027\u3067\u306f\u3042\u308a\u307e\u305b\u3093\u3002\u4e00\u822c\u7684\u306a\u63a2\u7d22\u306e\u50be\u5411\u306f\u3001\u751f\u7269\u4f53\u306e\u30b9\u30c8\u30ec\u30b9\u306b\u6e80\u3061\u305f\u6442\u52d5\u306b\u5bfe\u3059\u308b\u5b8c\u7d50\u7684\u306a\u53cd\u5fdc\u3067\u306f\u3042\u308a\u307e\u305b\u3093\u3002 Reduction of cognitive dissonance. Some researchers then proposed another conceptualization. Festinger\u2019s theory of cognitive dissonance (Festinger, 1957 ) asserted that organisms are motivated to reduce dissonance, which is the incompatibility between internal cognitive structures and the situations currently perceived. dissonance: \u4e0d\u4e00\u81f4\uff0c\u4e0d\u548c \u8a8d\u77e5\u4e0d\u8abf\u548c\u306e\u8efd\u6e1b\u3002 \u3044\u304f\u3064\u304b\u306e\u7814\u7a76\u8005\u306f\u5225\u306e\u6982\u5ff5\u5316\u3092\u63d0\u6848\u3057\u305f\u3002 Festinger\u306e\u8a8d\u77e5\u7684\u4e0d\u8abf\u548c\u306e\u7406\u8ad6\uff08Festinger\u30011957\uff09\u306f\u3001\u751f\u7269\u304c\u5185\u7684\u8a8d\u77e5\u69cb\u9020\u3068\u73fe\u5728\u8a8d\u8b58\u3055\u308c\u3066\u3044\u308b\u72b6\u6cc1\u3068\u306e\u9593\u306e\u4e0d\u9069\u5408\u3067\u3042\u308b\u4e0d\u5354\u548c\u3092\u6e1b\u3089\u3059\u3088\u3046\u52d5\u6a5f\u4ed8\u3051\u3089\u308c\u3066\u3044\u308b\u3068\u4e3b\u5f35\u3057\u305f\u3002 Fifteen years later a related view was articulated by Kagan stating that a primary motivation for humans is the reduction of uncertainty in the sense of the \u201cincompatibility between (two or more) cognitive structures, between cognitive structure and experience, or between structures and behavior\u201d (Kagan, 1972 ). 15\u5e74\u5f8c\u3001\u4eba\u9593\u306e\u4e3b\u306a\u52d5\u6a5f\u306f\u3001\u300c\uff082\u3064\u4ee5\u4e0a\u306e\uff09\u8a8d\u77e5\u69cb\u9020\u306e\u9593\u3001\u8a8d\u77e5\u69cb\u9020\u3068\u7d4c\u9a13\u306e\u9593\u3001\u307e\u305f\u306f\u69cb\u9020\u3068\u884c\u52d5\u3068\u306e\u9593\u306e\u4e0d\u9069\u5408\u6027\u300d\u306e\u610f\u5473\u306b\u304a\u3051\u308b\u4e0d\u78ba\u5b9f\u6027\u306e\u6e1b\u5c11\u3067\u3042\u308b\u3068\u3044\u3046Kagan\u306e\u95a2\u9023\u3059\u308b\u8ad6\u6587\u304c\u3042\u308b\uff08Kagan\u30011972\uff09\u3002 However, these theories were criticized on the basis that much human behavior is also intended to increase uncertainty, and not only to reduce it. Human seem to look for some forms of optimality between completely uncertain and completely certain situations. \u3057\u304b\u3057\u3001\u3053\u308c\u3089\u306e\u7406\u8ad6\u306f\u3001\u591a\u304f\u306e\u4eba\u9593\u306e\u884c\u52d5\u304c\u4e0d\u78ba\u5b9f\u6027\u3092\u9ad8\u3081\u3088\u3046\u3068\u3057\u3066\u3044\u308b\u3060\u3051\u3067\u306a\u304f\u3001\u305d\u308c\u3092\u6e1b\u3089\u3059\u3053\u3068\u3092\u610f\u56f3\u3057\u3066\u3044\u308b\u3068\u3044\u3046\u6279\u5224\u3092\u53d7\u3051\u3066\u3044\u308b\u3002 \u4eba\u9593\u306f\u3001\u5b8c\u5168\u306b\u4e0d\u78ba\u5b9f\u306a\u72b6\u6cc1\u3068\u5b8c\u5168\u306b\u4e00\u5b9a\u306e\u72b6\u6cc1\u3068\u306e\u9593\u3067\u3001\u3042\u308b\u7a2e\u306e\u5f62\u614b\u306e\u6700\u9069\u6027\u3092\u63a2\u3059\u3088\u3046\u306b\u898b\u3048\u308b\u3002 Optimal incongruity. In 1965, Hunt developed the idea that children and adult look for optimal incongruity (Hunt, 1965 ). He regarded children as information-processing systems and stated that interesting stimuli were those where there was a discrepancy between the perceived and standard levels of the stimuli. \u6700\u9069\u306a\u9055\u548c\u611f\u3002(\u5168\u7136\u9055\u3046\u3063\u3066\u306e\u3082\u5acc\u3060\u3057\uff0c\u5408\u3044\u307e\u304f\u3063\u3066\u308b\u306e\u3082\u5acc\u3060\u3068\u3044\u3046\u6700\u9069\u306a\u3068\u3053\u308d\u304c\u3042\u308b\u611f\u3058) 1965\u5e74\u3001\u30cf\u30f3\u30c8\u306f\u5b50\u4f9b\u3068\u5927\u4eba\u304c\u6700\u9069\u306a\u9055\u548c\u611f\u3092\u899a\u3048\u308b\u3068\u3044\u3046\u8003\u3048\u3092\u767a\u9054\u3055\u305b\u305f\uff08Hunt\u30011965\uff09\u3002 \u5f7c\u306f\u5b50\u4f9b\u3092\u60c5\u5831\u51e6\u7406\u30b7\u30b9\u30c6\u30e0\u3068\u307f\u306a\u3057\u3001\u8208\u5473\u6df1\u3044\u523a\u6fc0\u306f\u3001\u523a\u6fc0\u306e\u77e5\u899a\u30ec\u30d9\u30eb\u3068\u6a19\u6e96\u30ec\u30d9\u30eb\u3068\u306e\u9593\u306b\u76f8\u9055\u304c\u3042\u3063\u305f\u523a\u6fc0\u3067\u3042\u308b\u3068\u8ff0\u3079\u305f\u3002 For, Dember and Earl, the incongruity or discrepancy in intrinsically-motivated behaviors was between a person\u2019s expectations and the properties of the stimulus (Dember and Earl, 1957 ). Dember\u3068Earl\u306b\u3068\u3063\u3066\u3001\u5185\u7684\u52d5\u6a5f\u4ed8\u3051\u3055\u308c\u305f\u884c\u52d5\u306e\u4e0d\u4e00\u81f4\u307e\u305f\u306f\u4e0d\u4e00\u81f4\u306f\u3001\u4eba\u306e\u671f\u5f85\u3068\u523a\u6fc0\u306e\u7279\u6027\u3068\u306e\u9593\u306b\u3042\u3063\u305f\uff08Dember and Earl\u30011957\uff09\u3002 Berlyne developed similar notions as he observed that the most rewarding situations were those with an intermediate level of novelty, between already familiar and completely new situations (Berlyne, 1960 ). Berlyne\u306f\u3001\u540c\u69d8\u306e\u6982\u5ff5\u3092\u767a\u5c55\u3055\u305b\uff0c\u6700\u3082\u5831\u916c\u306e\u9ad8\u3044\u72b6\u6cc1\u306f\u3001\u3059\u3067\u306b\u3088\u304f\u77e5\u3089\u308c\u3066\u3044\u308b\u72b6\u6cc1\u3068\u5b8c\u5168\u306b\u65b0\u3057\u3044\u72b6\u6cc1\u306e\u9593\u3067\u3001\u4e2d\u9593\u30ec\u30d9\u30eb\u306e\u65b0\u898f\u6027\u3092\u6301\u3064\u3082\u306e\u3067\u3042\u308b\u3068\u6c17\u3065\u3044\u305f\uff08Berlyne\u30011960\uff09\u3002 Motivation for effectance, personal causation, competence and self-determination. Eventually, a last group of researchers preferred the concept of challenge to the notion of optimal incongruity. These researchers stated that what was driving human behavior was a motivation for effectance (White, 1959 ), personal causation (De Charms, 1968 ), competence and self-determination (Deci and Ryan, 1985 ). \u5b9f\u52b9\u6027\u3001\u500b\u4eba\u7684\u56e0\u679c\u95a2\u4fc2\u3001\u80fd\u529b\u3068\u81ea\u5df1\u6c7a\u5b9a\u306e\u52d5\u6a5f\u4ed8\u3051\u3002 \u7d50\u5c40\u3001\u6700\u5f8c\u306e\u30b0\u30eb\u30fc\u30d7\u306e\u7814\u7a76\u8005\u306f\u3001\u6700\u9069\u306a\u9055\u548c\u611f\u3068\u3044\u3046\u6982\u5ff5\u3088\u308a\u3082\uff0c\u30c1\u30e3\u30ec\u30f3\u30b8\u306e\u6982\u5ff5\u3092\u512a\u5148\u3055\u305b\u307e\u3057\u305f\u3002 \u3053\u308c\u3089\u306e\u7814\u7a76\u8005\u306f\u3001\u4eba\u9593\u306e\u884c\u52d5\u3092\u52d5\u304b\u3059\u3082\u306e\u306f\u3001\u52b9\u679c\u306e\u30e2\u30c1\u30d9\u30fc\u30b7\u30e7\u30f3\uff08White\u30011959\uff09\u3001\u500b\u4eba\u7684\u56e0\u679c\u95a2\u4fc2\uff08De Charms\u30011968\uff09\u3001\u80fd\u529b\u3068\u81ea\u5df1\u6c7a\u5b9a\uff08Deci and Ryan\u30011985\uff09\u3067\u3042\u308b\u3068\u8ff0\u3079\u305f\u3002 Basically, these approaches argue that what motivates people is the degree of control they can have on other people, external objects and themselves, or in other words, the amount of effective interaction. In an analogous manner, the concept of optimal challenge has been put forward, such as for example in the theory of \u201cFlow\u201d (Csikszentmihalyi, 1991 ). \u57fa\u672c\u7684\u306b\u3001\u3053\u308c\u3089\u306e\u30a2\u30d7\u30ed\u30fc\u30c1\u306f\u3001\u4eba\u3005\u3092\u52d5\u6a5f\u3065\u3051\u308b\u3082\u306e\u306f\u3001\u4ed6\u4eba\u3001\u5916\u90e8\u306e\u7269\u4f53\u304a\u3088\u3073\u81ea\u5206\u81ea\u8eab\u306b\u4e0e\u3048\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u5236\u5fa1\u306e\u7a0b\u5ea6\u3001\u3059\u306a\u308f\u3061\u6709\u52b9\u306a\u76f8\u4e92\u4f5c\u7528\u306e\u91cf\u3067\u3042\u308b\u3068\u4e3b\u5f35\u3057\u3066\u3044\u308b\u3002 \u540c\u69d8\u306e\u65b9\u6cd5\u3067\u3001\u4f8b\u3048\u3070\u300c\u30d5\u30ed\u30fc\u300d\u7406\u8ad6\uff08Csikszentmihalyi\u30011991\uff09\u306e\u3088\u3046\u306a\u6700\u9069\u306a\u6311\u6226\u306e\u6982\u5ff5\u304c\u63d0\u5531\u3055\u308c\u3066\u3044\u308b\u3002","title":"What Makes an Activity Intrinsically Motivating?"},{"location":"lecture/robo/what is intristic motivation/#motivation-in-computational-systems-extrinsic-vs-intrinsic-and-external-vs-internal","text":"After having made a broad review of intrinsic motivation in psychology, we will here start to take a computational viewpoint. To begin with, we will describe how motivations in general are conceived and used in computer and robotic architectures. We will then present a set of important distinctive dimensions, among which the intrinsic-extrinsic distinction, that are useful to organize the space of possible motivation systems. \u5fc3\u7406\u5b66\u306b\u304a\u3051\u308b\u672c\u8cea\u7684\u306a\u52d5\u6a5f\u3065\u3051\u3092\u5e45\u5e83\u304f\u898b\u76f4\u3057\u305f\u5f8c\u3001\u6211\u3005\u306f\u3053\u3053\u3067\u8a08\u7b97\u4e0a\u306e\u8996\u70b9\u3092\u53d6\u308a\u59cb\u3081\u308b\u3002 \u306f\u3058\u3081\u306b\u3001\u79c1\u305f\u3061\u306f\u3001\u4e00\u822c\u7684\u306a\u52d5\u6a5f\u304c\u30b3\u30f3\u30d4\u30e5\u30fc\u30bf\u30fc\u3084\u30ed\u30dc\u30c3\u30c8\u306e\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u30fc\u3067\u3069\u306e\u3088\u3046\u306b\u69cb\u60f3\u3055\u308c\u4f7f\u7528\u3055\u308c\u3066\u3044\u308b\u304b\u3092\u8aac\u660e\u3057\u307e\u3059\u3002 \u53ef\u80fd\u6027\u306e\u3042\u308b\u30e2\u30c1\u30d9\u30fc\u30b7\u30e7\u30f3\u30b7\u30b9\u30c6\u30e0\u306e\u7a7a\u9593\u3092\u6574\u7406\u3059\u308b\u306e\u306b\u6709\u7528\u306a\u5185\u5728\u7684 - \u5185\u5728\u7684\u306a\u533a\u5225\u306e\u4e2d\u3067\u91cd\u8981\u306a\u7279\u5fb4\u7684\u306a\u6b21\u5143\u306e\u30bb\u30c3\u30c8\u3092\u63d0\u793a\u3059\u308b\u3002 Motivational variables and drives. While motivation is sometimes implemented in an implicit manner in simple robot architectures, such as phototaxis in Braitenberg vehicles (Braitenberg, 1984 ), it is now rather common to implement it directly and explicitly in the form of a module that tracks the value of a number of internal \u201cmotivational\u201d variables and sends signals to the rest of the architecture (Arkin, 2005 ; Breazeal, 2002 ; Huang and Weng, 2004 ; Konidaris and Barto, 2006 ). \u610f\u6b32\u7684\u306a\u5909\u6570\u3068\u30c9\u30e9\u30a4\u30d6\u3002 \u52d5\u6a5f\u3065\u3051\u306f\u3001Braitenberg\u8eca\u4e21\u306e\u5149\u8d70\u6027\u306e\u3088\u3046\u306a\u7c21\u5358\u306a\u30ed\u30dc\u30c3\u30c8\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u3067\u306f\u6697\u9ed9\u7684\u306b\u5b9f\u88c5\u3055\u308c\u308b\u3053\u3068\u304c\u3042\u308b\u304c\uff08Braitenberg\u30011984\uff09\u3001\u6570\u5024\u306e\u5024\u3092\u8ffd\u8de1\u3059\u308b\u30e2\u30b8\u30e5\u30fc\u30eb\u306e\u5f62\u3067\u76f4\u63a5\u7684\u304b\u3064\u660e\u793a\u7684\u306b\u5b9f\u88c5\u3059\u308b\u3053\u3068\u306f\u304b\u306a\u308a\u4e00\u822c\u7684\u3067\u3042\u308b \uff08Arkin\u30012005; Breazeal\u30012002; Huang and Weng\u30012004; Konidaris and Barto\u30012006\uff09\u3002 For example, one often encounters an energy level variable, associated with a zone of comfort (i.e., a range of values), and when this variable gets out of this zone, the system sends signals to the rest of the architecture, and to the action selection module in particular, so that the robot finds a charging station as soon as possible. This homeostatic system can also be implemented as a Hullian drive (Hull, 1943 ; Konidaris and Barto, 2006 ), energy level being a variable ranging from 0 (totally unsatisfied) to 1 (satiated), and constantly sending its value to the action selection system in order to maintain it as close to 1 as possible. \u305f\u3068\u3048\u3070\u3001\u5feb\u9069\u5ea6\u306e\u30be\u30fc\u30f3\uff08\u5024\u306e\u7bc4\u56f2\uff09\u306b\u95a2\u9023\u4ed8\u3051\u3089\u308c\u305f\u30a8\u30cd\u30eb\u30ae\u30fc\u30ec\u30d9\u30eb\u306e\u5909\u6570\u306b\u983b\u7e41\u306b\u906d\u9047\u3057\u3001\u3053\u306e\u5909\u6570\u304c\u3053\u306e\u30be\u30fc\u30f3\u304b\u3089\u629c\u3051\u51fa\u3059\u3068\u3001\u30b7\u30b9\u30c6\u30e0\u306f\u6b8b\u308a\u306e\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u306b\u4fe1\u53f7\u3092\u9001\u4fe1\u3057\u3001 \u7279\u306b\u30ed\u30dc\u30c3\u30c8\u304c\u5145\u96fb\u30b9\u30c6\u30fc\u30b7\u30e7\u30f3\u3092\u3067\u304d\u308b\u3060\u3051\u65e9\u304f\u898b\u3064\u3051\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u3002 \u3053\u306e\u30db\u30e1\u30aa\u30b9\u30bf\u30b7\u30b9\u30fb\u30b7\u30b9\u30c6\u30e0\u306f\u30010\uff08\u5b8c\u5168\u306b\u6e80\u305f\u3055\u308c\u3066\u3044\u306a\u3044\uff09\u304b\u30891\uff08\u98fd\u548c\u72b6\u614b\uff09\u306e\u7bc4\u56f2\u306e\u5909\u6570\u3067\u3042\u308a\u3001\u5e38\u306b\u305d\u306e\u5024\u3092\u30a2\u30af\u30b7\u30e7\u30f3\u9078\u629e\u306b\u9001\u308b\u3001Hullian\u30c9\u30e9\u30a4\u30d6\uff08Hull\u30011943; Konidaris and Barto\u30012006\uff09 \u30b7\u30b9\u30c6\u30e0\u3092\u53ef\u80fd\u306a\u9650\u308a1\u306b\u8fd1\u3065\u3051\u308b\u3088\u3046\u306b\u3057\u3066\u304f\u3060\u3055\u3044\u3002 Computational Reinforcement Learning and rewards. It is often the case in robotic systems that the action strategy that allows to keep motivational variables as satiated as possible is neither fixed nor initially hand-coded, but rather should be learnt. The standard framework in which this happens is \u201ccomputational reinforcement learning (CRL)\u201d (Sutton and Barto, 1998 ). This framework has introduced many algorithms targeted at finding strategies to maximize \u201crewards\u201d, which is the pivotal concept of CRL. Very importantly, the meaning of the term \u201creward\u201d is used in a specific technical manner in CRL and is different from the meaning of the term \u201creward\u201d in psychology, and in particular in the theory of operant conditioning (Skinner, 1953 ). Nevertheless, these two meanings overlap and this has produced a number of confusions in the literature. In CRL, a \u201creward\u201d is technically only a numerical quantity that is measured continuously and used to drive the action selection mechanism so that the cumulated value of this quantity in the future is maximized. CRL theory is completely agnostic about what/how/where this value is generated. Coming back to robots implementing ethologically inspired motivation system, this value could be for example the value of the robot\u2019s internal level of energy. But, and this is how CRL is often used in the computational literature, this value could also be set directly by a human engineer or by an external program built by a human engineer. For example, a number of experiments in which engineers try to build robots that can walk forward have used CRL algorithms with a reward being a value coming from an external system (e.g., camera on the ceiling) observing how fast (or not) the robot moves (the value being the speed). It is in these experiments that the term \u201creward\u201d overlaps with the term \u201creward\u201d used in the operant conditioning literature, and where it denotes the getting of an external object/event/property such as money, food or a high grade at school. But one has to keep in mind that in a robot using CRL, a reward can be completely internally defined and be analogous to the very release of a neurotransmitter. Rewards as a common currency for multiple motivations. One of the nice features of the reward concept in CRL is that, being a numerical quantity, it can act as a \u201ccommon currency\u201d among several coexisting motivations in a single architecture (McFarland and Bosser, 1994 ). Indeed, in a typical organism, natural or artificial, different and possibly conflicting motives can try to push actions in certain directions: for example, one may have a drive for energy level maintenance co-existing with a drive for physical integrity maintenance, a drive for sleeping, and a drive pushing towards the search for social partners. In order to arbitrate between the possibly conflicting actions entailed by all these motivations, one uses the possibility to numerically compare the expected rewards associated with each of them. Moreover, one often sees architectures in which a (possibly adaptive) numerical weight is associated to each of these rewards (Konidaris and Barto, 2006 ). Internal vs. external motivations. Given this architectural framework for implementing motivations in a robot, one can investigate a first kind of distinction between internal and external motivations. This difference relates to autonomy and lies in the functional location of the mechanism that computes/generates the reward. If the reward, i.e., the numerical quantity that the system has to maximize, comes from the outside of the autonomous system, then it is called external. This is the above mentioned example of the walking robot driven by a reward coming from a human or a system with a camera mounted on the ceiling. If the reward is computed and generated internally by the autonomous system, then it is called internal. This is the above mentioned example of the reward associated to the satiation of an energy maintenance drive. This difference is summarized on Figure 1 . Yet, this difference can be sometimes subtle in the case of robots. Computers allow us to do manipulations that are impossible with humans. For example, an engineer could very well build an autonomous machine that is capable of monitoring by itself whether it is walking forward or not and at what speed, and could incorporate in the robot\u2019s internal architecture a motivation to go forward as fast as possible. In practice, this will produce more or less the same behavior that with the walking detection system mounted on the ceiling, but technically we have here an internal reward (which is nevertheless extrinsic as we will see). Of course, this kind of manipulation is not possible with humans, and it is much more difficult to find this kind of \u201climit\u201d example in humans.","title":"Motivation in Computational Systems: Extrinsic vs. Intrinsic and External vs. Internal"},{"location":"procon/\u30e1\u30e2/","text":"\u30e1\u30e2 \u73fe\u72b6 \u30b5\u30a4\u30ba\u6570 char 8bit 2byte int 32bit 8byte long long 64biy 16bite 1byte 8bit 1kb 1024byte 2 10 byte \u7d04 10 3 byte 1mb 1024kb 2 20 byte \u7d04 10 6 byte 1gb 1024mb 2 30 byte \u7d04 10 9 byte ABC 061 D\u554f\u984c \u8ca0\u306e\u30b3\u30b9\u30c8\u304c\u3042\u308b\u6700\u77ed\u7d4c\u8def\u554f\u984c\u306b\u843d\u3068\u3057\u8fbc\u3081\u308b\u306e\u3067\uff0c\u30d9\u30eb\u30de\u30f3\u30d5\u30a9\u30fc\u30c9\u6cd5\u3092\u4f7f\u3046 \u8ca0\u306e\u9589\u8def\u304c\u751f\u307e\u308c\u305f\u6642\uff0c\u7121\u9650\u306b\u30b9\u30b3\u30a2\u304c\u4e0a\u6607\u3059\u308b\u30d1\u30bf\u30fc\u30f3\u3068\uff0c\u30b9\u30b3\u30a2\u4e0a\u6607\u306b\u95a2\u4fc2\u306a\u3044\u30d1\u30bf\u30fc\u30f3\u3092\u898b\u6975\u3081\u308b ABC072 C\u554f\u984c \u30d0\u30b1\u30c4\u30bd\u30fc\u30c8\u307f\u305f\u3044\u306a\u306e\u3092\u3084\u308b\u3060\u3051 D\u554f\u984c \u30b9\u30ef\u30c3\u30d7\u3059\u308b\u3060\u3051 ABC073 A\u554f\u984c python\u306einput()\u306f\u3059\u3079\u3066\u6587\u5b57\u5217\u3068\u3057\u3066\u5165\u529b\u3055\u308c\u308b \u6570\u5b57\u3068\u3057\u3066\u4f7f\u3046\u306a\u3089\u30ad\u30e3\u30b9\u30c8\u3059\u308b B\u554f\u984c \u8907\u6570\u306e\u5909\u6570\u306b\u683c\u7d0d A,B,C = map(int,input().split()) n\u500b\u5206\u914d\u5217\u306b\u5165\u308c\u308b\u5834\u5408 \u5185\u5305\u8868\u8a18\u3067 a = [input() for i in range(N)] autopep8\u5165\u308c\u305f\uff0c\u30b7\u30e7\u30fc\u30c8\u30ab\u30c3\u30c8\u306f ctrl + alt + s C\u554f\u984c python\u306e\u96c6\u5408set\u3064\u304b\u3063\u305f python s = set() # \u5ba3\u8a00 s.add(a) # \u8ffd\u52a0 s.remove(a) # \u524a\u9664 len(s) # \u500b\u6570 D\u554f\u984c \u30ef\u30fc\u30b7\u30e3\u30eb\u30d5\u30ed\u30a4\u30c9\u6cd5\u3067\u5168\u8ddd\u96e2\u3092\u6c42\u3081\u308b bitDP\u3067\u5de1\u56de\u30bb\u30fc\u30eb\u30b9\u30de\u30f3\u554f\u984c\u3092\u89e3\u304f \u521d\u671f\u5316\u4f4d\u7f6e\u3092\u30de\u30b8\u3067\u6c17\u3092\u4ed8\u3051\u308b INF\b\u306e\u5024\u3092\u9069\u5f53\u306b\u5165\u308c\u308b\u3068\u30d0\u30b0\u308b ABC080 C\u554f\u984c \u3072\u3068\u3064\u306a\u304c\u308a\u306ea2\u306fa1\u3068\u7b49\u4fa1\u3067\u3042\u308b\u3053\u3068\u304c\u308f\u304b\u308b \u9593\u9055\u3063\u305f\u8a08\u7b97\u5f0f\u3067\u89e3\u3051\u3066\u3057\u307e\u3063\u305f 'if(a + (a2>0) <= a4 + 1)' \u3053\u308c\u304c\u6b63\u3057\u3044 'if(a + (a2%2) <= a4 + 1)' \u3053\u308c\u304c\u9593\u9055\u3044 a2\u304c\u5076\u6570\u500b\u3060\u3063\u305f\u3068\u3057\u3066\u4f55\u3082\u306a\u3044\u72b6\u614b\u3068\u7b49\u4fa1\u306b\u306f\u306a\u3089\u306a\u3044 ABC081 sort(v.begin(),v.end(),std :: greater < Type > ()); int * pos; pos = lower_bound(a,a + 200001 , 1 ); cin\u3067\u3082\u9ad8\u901f\u3067\u5165\u529b\u304c\u884c\u3048\u308b\u65b9\u6cd5 cin.tie( 0 ); ios :: sync_with_stdio(false); ABC083 C\u554f\u984c \u304b\u306a\u308a\u7c21\u5358\uff0c\u305f\u3060\u3057\u554f\u984c\u6587\u3092\u8aad\u307f\u89e3\u304f\u306e\u306b\u82e6\u52b4\u3057\u305f\uff0e X\u4ee5\u4e0aY\u4ee5\u4e0b\u306e\u6574\u6570\u304b\u3089\u306a\u308b\u3082\u3063\u3068\u3082\u9577\u3044\u5358\u8abf\u5897\u52a0\u6570\u5217\u3092\u4f5c\u308a\uff0c\u6700\u9577\u306e\u9577\u3055\u3092\u6c42\u3081\u3088\u3068\u3044\u3046\u554f\u984c\uff0e \u6761\u4ef6\u3068\u3057\u3066\uff0c a[i+1]>a[i] \u304b\u3064 a[i+1]=k*a[i] \u3064\u307e\u308aa[i+1]\u306fa[i]\u306e\u500d\u6570\u3067\u3042\u308b\uff0e \u6700\u5c0f\u306ea[i]\u306f a[i] = x * 2 ^ i \u306a\u306e\u3067\uff0c y > x * 2 ^ i \u3092\u6e80\u305f\u3059\u6700\u5927\u306ei\u304c\u7b54\u3048\uff0e ABC084 C\u554f\u984c \u6771\u897f\u306b\u4e00\u76f4\u7dda\u306b\u99c5\u304cN\u500b\u3042\u308a\uff0c\u897f\u304b\u3089\u9806\u756a\u306b\u756a\u53f7\u304c\u3064\u3044\u3066\u3044\u308b\uff0e \u3042\u308b\u99c5\u304b\u3089\u6b21\u306e\u99c5\u3078\u884c\u304f\u624b\u6bb5\u306f\u99c5\u306b f[i] \u6642\u9593\u3054\u3068\u306b\u6765\u308b\u96fb\u8eca\u306b\u4e57\u308b\u4e8b \u897f\u304b\u3089\u6771\u306b\u3082\u3063\u3068\u3082\u65e9\u304f\u305f\u3069\u308a\u7740\u304f\u6642\u9593\u304c\u77e5\u308a\u305f\u3044\uff0e \u99c5\u306b\u304f\u308b\u96fb\u8eca\u306f c[i] \u304c\u6b21\u306e\u99c5\u307e\u3067\u306e\u79fb\u52d5\u6642\u9593\uff0c s[i] \u304c\u96fb\u8eca\u304c\u306f\u3058\u3081\u306b\u52d5\u304f\u6642\u9593\uff0c f[i] \u304c\u6b21\u306e\u96fb\u8eca\u304c\u6765\u308b\u307e\u3067\u306e\u6642\u9593 \u8003\u5bdf \u30b4\u30fc\u30eb\u304b\u3089\u6700\u77ed\u6642\u9593\u3092\u8abf\u3079\u3066\u3044\u3063\u305f\u65b9\u304c\u826f\u3055\u305d\u3046? \u5b9f\u88c5\u304c\u308f\u304b\u3089\u306a\u304f\u306a\u3063\u305f\u306e\u3067\u611a\u76f4\u306a\u65b9\u6cd5\u3067\u3068\u308a\u3042\u3048\u305a\u3068\u304f\uff0e \u6642\u523bt\u3092d\u306e\u76ee\u76db\u308a\u3067\u306b\u5408\u308f\u305b\u308b\u65b9\u6cd5\u306f if(t%d!=0) t = t+d-(t%d); ABC085 B \u96c6\u5408\u3092\u6271\u3046\u306b\u306fset set<int> S S.insert(a) \u3067a\u3092\u633f\u5165 S.size() \u3067\u8981\u7d20\u6570\u3092\u51fa\u3059 D pair\u306e\u9006\u9806\u30bd\u30fc\u30c8 sort(a.rbegin(),a.rend()) \u5207\u308a\u4e0a\u3052\u95a2\u6570 ceil(a) \u6574\u6570\u540c\u58eb\u306e\u5272\u308a\u7b97\u306e\u5207\u308a\u4e0a\u3052\u306f\u3053\u308c\u3067\u3082\u304ak (a+b-1)/b ABC087 C\u554f\u984c 5\u7a2e\u985e\u306e\u4eba\u9593\u304c\u5927\u91cf\u306b\u3044\u308b\u4e2d\u3067\u7a2e\u985e\u304c\u88ab\u3089\u306a\u30443\u4eba\u306e\u9078\u3073\u65b9\u306f\u4f55\u901a\u308a\u304b\uff0e \u6587\u5b57\u5217\u4e2d\u304b\u3089\u6587\u5b57\u306e\u4f4d\u7f6e\u3092\u691c\u7d22\u3059\u308b\u305f\u3081\u306b\u306f str.find(\"A\") \u306a\u3069\u3068\u3044\u3046\u3088\u3046\u306b\u4f7f\u3046 \u5927\u304d\u3044\u6570\u5b57\u3092\u6271\u3046\u3068\u304d\u306f\uff0c\u8a08\u7b97\u9014\u4e2d\u3067\u3082\u30ad\u30e3\u30b9\u30c8\u3057\u3066\u304a\u304b\u306a\u3044\u3068\u3044\u3051\u306a\u3044\uff0e D = (long long)A * B * C 5\u7a2e\u985e\u304b\u30893\u7a2e\u985e\u306e\u9078\u3073\u65b9 REP(i, 1 , 4 ){ REP(j,i + 1 , 5 ){ REP(k,j + 1 , 6 ){ printf( \"%d,%d,%d \\n \" ,i,j,k); //\u3053\u3053\u3067i,j,k\u306f1~5\u3092\u88ab\u3089\u305a\u306b\u53d6\u308b } } }","title":"\u30e1\u30e2"},{"location":"procon/\u30e1\u30e2/#_1","text":"\u73fe\u72b6","title":"\u30e1\u30e2"},{"location":"procon/\u30e1\u30e2/#_2","text":"char 8bit 2byte int 32bit 8byte long long 64biy 16bite 1byte 8bit 1kb 1024byte 2 10 byte \u7d04 10 3 byte 1mb 1024kb 2 20 byte \u7d04 10 6 byte 1gb 1024mb 2 30 byte \u7d04 10 9 byte","title":"\u30b5\u30a4\u30ba\u6570"},{"location":"procon/\u30e1\u30e2/#abc-061","text":"D\u554f\u984c \u8ca0\u306e\u30b3\u30b9\u30c8\u304c\u3042\u308b\u6700\u77ed\u7d4c\u8def\u554f\u984c\u306b\u843d\u3068\u3057\u8fbc\u3081\u308b\u306e\u3067\uff0c\u30d9\u30eb\u30de\u30f3\u30d5\u30a9\u30fc\u30c9\u6cd5\u3092\u4f7f\u3046 \u8ca0\u306e\u9589\u8def\u304c\u751f\u307e\u308c\u305f\u6642\uff0c\u7121\u9650\u306b\u30b9\u30b3\u30a2\u304c\u4e0a\u6607\u3059\u308b\u30d1\u30bf\u30fc\u30f3\u3068\uff0c\u30b9\u30b3\u30a2\u4e0a\u6607\u306b\u95a2\u4fc2\u306a\u3044\u30d1\u30bf\u30fc\u30f3\u3092\u898b\u6975\u3081\u308b","title":"ABC 061"},{"location":"procon/\u30e1\u30e2/#abc072","text":"C\u554f\u984c \u30d0\u30b1\u30c4\u30bd\u30fc\u30c8\u307f\u305f\u3044\u306a\u306e\u3092\u3084\u308b\u3060\u3051 D\u554f\u984c \u30b9\u30ef\u30c3\u30d7\u3059\u308b\u3060\u3051","title":"ABC072"},{"location":"procon/\u30e1\u30e2/#abc073","text":"A\u554f\u984c python\u306einput()\u306f\u3059\u3079\u3066\u6587\u5b57\u5217\u3068\u3057\u3066\u5165\u529b\u3055\u308c\u308b \u6570\u5b57\u3068\u3057\u3066\u4f7f\u3046\u306a\u3089\u30ad\u30e3\u30b9\u30c8\u3059\u308b B\u554f\u984c \u8907\u6570\u306e\u5909\u6570\u306b\u683c\u7d0d A,B,C = map(int,input().split()) n\u500b\u5206\u914d\u5217\u306b\u5165\u308c\u308b\u5834\u5408 \u5185\u5305\u8868\u8a18\u3067 a = [input() for i in range(N)] autopep8\u5165\u308c\u305f\uff0c\u30b7\u30e7\u30fc\u30c8\u30ab\u30c3\u30c8\u306f ctrl + alt + s C\u554f\u984c python\u306e\u96c6\u5408set\u3064\u304b\u3063\u305f python s = set() # \u5ba3\u8a00 s.add(a) # \u8ffd\u52a0 s.remove(a) # \u524a\u9664 len(s) # \u500b\u6570 D\u554f\u984c \u30ef\u30fc\u30b7\u30e3\u30eb\u30d5\u30ed\u30a4\u30c9\u6cd5\u3067\u5168\u8ddd\u96e2\u3092\u6c42\u3081\u308b bitDP\u3067\u5de1\u56de\u30bb\u30fc\u30eb\u30b9\u30de\u30f3\u554f\u984c\u3092\u89e3\u304f \u521d\u671f\u5316\u4f4d\u7f6e\u3092\u30de\u30b8\u3067\u6c17\u3092\u4ed8\u3051\u308b INF\b\u306e\u5024\u3092\u9069\u5f53\u306b\u5165\u308c\u308b\u3068\u30d0\u30b0\u308b","title":"ABC073"},{"location":"procon/\u30e1\u30e2/#abc080","text":"C\u554f\u984c \u3072\u3068\u3064\u306a\u304c\u308a\u306ea2\u306fa1\u3068\u7b49\u4fa1\u3067\u3042\u308b\u3053\u3068\u304c\u308f\u304b\u308b \u9593\u9055\u3063\u305f\u8a08\u7b97\u5f0f\u3067\u89e3\u3051\u3066\u3057\u307e\u3063\u305f 'if(a + (a2>0) <= a4 + 1)' \u3053\u308c\u304c\u6b63\u3057\u3044 'if(a + (a2%2) <= a4 + 1)' \u3053\u308c\u304c\u9593\u9055\u3044 a2\u304c\u5076\u6570\u500b\u3060\u3063\u305f\u3068\u3057\u3066\u4f55\u3082\u306a\u3044\u72b6\u614b\u3068\u7b49\u4fa1\u306b\u306f\u306a\u3089\u306a\u3044","title":"ABC080"},{"location":"procon/\u30e1\u30e2/#abc081","text":"sort(v.begin(),v.end(),std :: greater < Type > ()); int * pos; pos = lower_bound(a,a + 200001 , 1 ); cin\u3067\u3082\u9ad8\u901f\u3067\u5165\u529b\u304c\u884c\u3048\u308b\u65b9\u6cd5 cin.tie( 0 ); ios :: sync_with_stdio(false);","title":"ABC081"},{"location":"procon/\u30e1\u30e2/#abc083","text":"C\u554f\u984c \u304b\u306a\u308a\u7c21\u5358\uff0c\u305f\u3060\u3057\u554f\u984c\u6587\u3092\u8aad\u307f\u89e3\u304f\u306e\u306b\u82e6\u52b4\u3057\u305f\uff0e X\u4ee5\u4e0aY\u4ee5\u4e0b\u306e\u6574\u6570\u304b\u3089\u306a\u308b\u3082\u3063\u3068\u3082\u9577\u3044\u5358\u8abf\u5897\u52a0\u6570\u5217\u3092\u4f5c\u308a\uff0c\u6700\u9577\u306e\u9577\u3055\u3092\u6c42\u3081\u3088\u3068\u3044\u3046\u554f\u984c\uff0e \u6761\u4ef6\u3068\u3057\u3066\uff0c a[i+1]>a[i] \u304b\u3064 a[i+1]=k*a[i] \u3064\u307e\u308aa[i+1]\u306fa[i]\u306e\u500d\u6570\u3067\u3042\u308b\uff0e \u6700\u5c0f\u306ea[i]\u306f a[i] = x * 2 ^ i \u306a\u306e\u3067\uff0c y > x * 2 ^ i \u3092\u6e80\u305f\u3059\u6700\u5927\u306ei\u304c\u7b54\u3048\uff0e","title":"ABC083"},{"location":"procon/\u30e1\u30e2/#abc084","text":"C\u554f\u984c \u6771\u897f\u306b\u4e00\u76f4\u7dda\u306b\u99c5\u304cN\u500b\u3042\u308a\uff0c\u897f\u304b\u3089\u9806\u756a\u306b\u756a\u53f7\u304c\u3064\u3044\u3066\u3044\u308b\uff0e \u3042\u308b\u99c5\u304b\u3089\u6b21\u306e\u99c5\u3078\u884c\u304f\u624b\u6bb5\u306f\u99c5\u306b f[i] \u6642\u9593\u3054\u3068\u306b\u6765\u308b\u96fb\u8eca\u306b\u4e57\u308b\u4e8b \u897f\u304b\u3089\u6771\u306b\u3082\u3063\u3068\u3082\u65e9\u304f\u305f\u3069\u308a\u7740\u304f\u6642\u9593\u304c\u77e5\u308a\u305f\u3044\uff0e \u99c5\u306b\u304f\u308b\u96fb\u8eca\u306f c[i] \u304c\u6b21\u306e\u99c5\u307e\u3067\u306e\u79fb\u52d5\u6642\u9593\uff0c s[i] \u304c\u96fb\u8eca\u304c\u306f\u3058\u3081\u306b\u52d5\u304f\u6642\u9593\uff0c f[i] \u304c\u6b21\u306e\u96fb\u8eca\u304c\u6765\u308b\u307e\u3067\u306e\u6642\u9593 \u8003\u5bdf \u30b4\u30fc\u30eb\u304b\u3089\u6700\u77ed\u6642\u9593\u3092\u8abf\u3079\u3066\u3044\u3063\u305f\u65b9\u304c\u826f\u3055\u305d\u3046? \u5b9f\u88c5\u304c\u308f\u304b\u3089\u306a\u304f\u306a\u3063\u305f\u306e\u3067\u611a\u76f4\u306a\u65b9\u6cd5\u3067\u3068\u308a\u3042\u3048\u305a\u3068\u304f\uff0e \u6642\u523bt\u3092d\u306e\u76ee\u76db\u308a\u3067\u306b\u5408\u308f\u305b\u308b\u65b9\u6cd5\u306f if(t%d!=0) t = t+d-(t%d);","title":"ABC084"},{"location":"procon/\u30e1\u30e2/#abc085","text":"B \u96c6\u5408\u3092\u6271\u3046\u306b\u306fset set<int> S S.insert(a) \u3067a\u3092\u633f\u5165 S.size() \u3067\u8981\u7d20\u6570\u3092\u51fa\u3059 D pair\u306e\u9006\u9806\u30bd\u30fc\u30c8 sort(a.rbegin(),a.rend()) \u5207\u308a\u4e0a\u3052\u95a2\u6570 ceil(a) \u6574\u6570\u540c\u58eb\u306e\u5272\u308a\u7b97\u306e\u5207\u308a\u4e0a\u3052\u306f\u3053\u308c\u3067\u3082\u304ak (a+b-1)/b","title":"ABC085"},{"location":"procon/\u30e1\u30e2/#abc087","text":"C\u554f\u984c 5\u7a2e\u985e\u306e\u4eba\u9593\u304c\u5927\u91cf\u306b\u3044\u308b\u4e2d\u3067\u7a2e\u985e\u304c\u88ab\u3089\u306a\u30443\u4eba\u306e\u9078\u3073\u65b9\u306f\u4f55\u901a\u308a\u304b\uff0e \u6587\u5b57\u5217\u4e2d\u304b\u3089\u6587\u5b57\u306e\u4f4d\u7f6e\u3092\u691c\u7d22\u3059\u308b\u305f\u3081\u306b\u306f str.find(\"A\") \u306a\u3069\u3068\u3044\u3046\u3088\u3046\u306b\u4f7f\u3046 \u5927\u304d\u3044\u6570\u5b57\u3092\u6271\u3046\u3068\u304d\u306f\uff0c\u8a08\u7b97\u9014\u4e2d\u3067\u3082\u30ad\u30e3\u30b9\u30c8\u3057\u3066\u304a\u304b\u306a\u3044\u3068\u3044\u3051\u306a\u3044\uff0e D = (long long)A * B * C 5\u7a2e\u985e\u304b\u30893\u7a2e\u985e\u306e\u9078\u3073\u65b9 REP(i, 1 , 4 ){ REP(j,i + 1 , 5 ){ REP(k,j + 1 , 6 ){ printf( \"%d,%d,%d \\n \" ,i,j,k); //\u3053\u3053\u3067i,j,k\u306f1~5\u3092\u88ab\u3089\u305a\u306b\u53d6\u308b } } }","title":"ABC087"},{"location":"tech/mkdocs/","text":"mkdocs\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb&\u8a2d\u5b9a mkdocs\u306e\u521d\u671f\u8a2d\u5b9a \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb pip install mkdocs pip install mkdocs-material pip install fontawesome_markdown \u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u4f5c\u6210 mkdocs new project \u3092\u4f5c\u6210\u5f8c project.yml \u3092\u7de8\u96c6 \u8a2d\u5b9a project.yml extra_css: - 'https://fonts.googleapis.com/earlyaccess/roundedmplus1c.css' #\u30d5\u30a9\u30f3\u30c8\u306e\u8a2d\u5b9a - 'css/custom.css' #custom.css\u3092\u4f7f\u3046\u3088\u3046\u306b - \"https://maxcdn.bootstrapcdn.com/font-awesome/4.6.1/css/font-awesome.min.css\" #\u30d6\u30fc\u30c8\u30b9\u30c8\u30e9\u30c3\u30d7\u306e\u8a2d\u5b9a markdown_extensions: - admonition - footnotes - fontawesome_markdown - def_list - codehilite: linenums: true use_pygments: true noclasses: true pygments_style: monokai \u30d3\u30eb\u30c9 mkdocs build \u30ed\u30fc\u30ab\u30eb\u3067\u898b\u308b(\u81ea\u52d5\u30d3\u30eb\u30c9) mkdocs serve","title":"mkdocs\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb&\u8a2d\u5b9a"},{"location":"tech/mkdocs/#mkdocs","text":"","title":"mkdocs\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb&amp;\u8a2d\u5b9a"},{"location":"tech/mkdocs/#mkdocs_1","text":"","title":"mkdocs\u306e\u521d\u671f\u8a2d\u5b9a"},{"location":"tech/mkdocs/#_1","text":"pip install mkdocs pip install mkdocs-material pip install fontawesome_markdown","title":"\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb"},{"location":"tech/mkdocs/#_2","text":"mkdocs new project \u3092\u4f5c\u6210\u5f8c project.yml \u3092\u7de8\u96c6","title":"\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u4f5c\u6210"},{"location":"tech/mkdocs/#_3","text":"project.yml extra_css: - 'https://fonts.googleapis.com/earlyaccess/roundedmplus1c.css' #\u30d5\u30a9\u30f3\u30c8\u306e\u8a2d\u5b9a - 'css/custom.css' #custom.css\u3092\u4f7f\u3046\u3088\u3046\u306b - \"https://maxcdn.bootstrapcdn.com/font-awesome/4.6.1/css/font-awesome.min.css\" #\u30d6\u30fc\u30c8\u30b9\u30c8\u30e9\u30c3\u30d7\u306e\u8a2d\u5b9a markdown_extensions: - admonition - footnotes - fontawesome_markdown - def_list - codehilite: linenums: true use_pygments: true noclasses: true pygments_style: monokai","title":"\u8a2d\u5b9a"},{"location":"tech/mkdocs/#_4","text":"mkdocs build","title":"\u30d3\u30eb\u30c9"},{"location":"tech/mkdocs/#_5","text":"mkdocs serve","title":"\u30ed\u30fc\u30ab\u30eb\u3067\u898b\u308b(\u81ea\u52d5\u30d3\u30eb\u30c9)"},{"location":"tech/\u30e1\u30e2\u30ea/","text":"\u30e1\u30e2\u30ea \u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u7684\u306a\u8a71 How computer memory works \u4efb\u610f\u306e(random\u306b)\u30a2\u30af\u30bb\u30b9\u3067\u304d\u308b\u30e1\u30e2\u30ea random access memory (RAM) \u63ee\u767a\u6027\u30e1\u30e2\u30ea DRAM : \u30b3\u30f3\u30c7\u30f3\u30b5\u306e\u5145\u96fb\u653e\u96fb\u3092\u884c\u3044\u8a18\u9332\u3059\u308b \u3053\u3063\u3061\u306e\u304c\u9045\u3044 SRAM \uff1a\u9ad8\u901f\u306a\u5185\u90e8\u30e1\u30e2\u30ea \u30ea\u30d5\u30ec\u30c3\u30b7\u30e5\u306e\u5fc5\u8981\u304c\u306a\u3044 \u9ad8\u4fa1\uff06\u3067\u304b\u3044 \u4e0d\u63ee\u767a\u6027\u30e1\u30e2\u30ea HDD\u3068\u304bSSD\u3068\u304b \u30e1\u30e2\u30ea\u306e\u9818\u57df text\u9818\u57df \u30d7\u30ed\u30b0\u30e9\u30e0\u3092\u7f6e\u304f data\u9818\u57df \u521d\u671f\u5316\u3055\u308c\u305f\u30b0\u30ed\u30fc\u30d0\u30eb\u5909\u6570\u306a\u3069\u3092\u7f6e\u304f bss\u9818\u57df \u521d\u671f\u5316\u3055\u308c\u3066\u306a\u3044\u30b0\u30ed\u30fc\u30d0\u30eb\u5909\u6570\u306a\u3069\u3092\u7f6e\u304f stack\u9818\u57df \u95a2\u6570\u306e\u5f15\u6570\u3084\u30ed\u30fc\u30ab\u30eb\u5909\u6570\u3092\u7f6e\u304f heap\u9818\u57df \u30d7\u30ed\u30b0\u30e9\u30e0\u306e\u30c7\u30fc\u30bf\u3092\u7f6e\u304f(\u52d5\u7684\u306b\u78ba\u4fdd\u3057\u305f\u3082\u306e) Stack\u3068\u95a2\u6570 Stack\u306f\u95a2\u6570\u547c\u3073\u51fa\u3057\u306e\u305f\u3081\u306b\u4f7f\u308f\u308c\u308b heap\u9818\u57df \u30d2\u30fc\u30d7\u306f\u4efb\u610f\u306e\u30b5\u30a4\u30ba\u306e\u30c7\u30fc\u30bf\u3092\u4efb\u610f\u306e\u9806\u756a\u306b\u78ba\u4fdd\u30fb\u89e3\u653e\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u308b new\u3057\u305f\u3068\u304d\u306f\u3053\u3061\u3089\u306e\u65b9\u6cd5\u3067\u78ba\u4fdd\u3055\u308c\u308b\uff0e vector\u306e\u914d\u5217\u3082\u3053\u3053\u306b\u5165\u308b StackOverflow\u6642\u306e\u30a8\u30e9\u30fc python Fatal Python error: Cannot recover from stack overflow. Current thread 0x00007f434f650700 (most recent call first): File \"***.py\" , line 4 in ... File \"***.py\" , line 7 in ... File \"***.py\" , line 7 in ...","title":"\u30e1\u30e2\u30ea"},{"location":"tech/\u30e1\u30e2\u30ea/#_1","text":"","title":"\u30e1\u30e2\u30ea"},{"location":"tech/\u30e1\u30e2\u30ea/#_2","text":"How computer memory works \u4efb\u610f\u306e(random\u306b)\u30a2\u30af\u30bb\u30b9\u3067\u304d\u308b\u30e1\u30e2\u30ea random access memory (RAM)","title":"\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u7684\u306a\u8a71"},{"location":"tech/\u30e1\u30e2\u30ea/#_3","text":"DRAM : \u30b3\u30f3\u30c7\u30f3\u30b5\u306e\u5145\u96fb\u653e\u96fb\u3092\u884c\u3044\u8a18\u9332\u3059\u308b \u3053\u3063\u3061\u306e\u304c\u9045\u3044 SRAM \uff1a\u9ad8\u901f\u306a\u5185\u90e8\u30e1\u30e2\u30ea \u30ea\u30d5\u30ec\u30c3\u30b7\u30e5\u306e\u5fc5\u8981\u304c\u306a\u3044 \u9ad8\u4fa1\uff06\u3067\u304b\u3044","title":"\u63ee\u767a\u6027\u30e1\u30e2\u30ea"},{"location":"tech/\u30e1\u30e2\u30ea/#_4","text":"HDD\u3068\u304bSSD\u3068\u304b","title":"\u4e0d\u63ee\u767a\u6027\u30e1\u30e2\u30ea"},{"location":"tech/\u30e1\u30e2\u30ea/#_5","text":"text\u9818\u57df \u30d7\u30ed\u30b0\u30e9\u30e0\u3092\u7f6e\u304f data\u9818\u57df \u521d\u671f\u5316\u3055\u308c\u305f\u30b0\u30ed\u30fc\u30d0\u30eb\u5909\u6570\u306a\u3069\u3092\u7f6e\u304f bss\u9818\u57df \u521d\u671f\u5316\u3055\u308c\u3066\u306a\u3044\u30b0\u30ed\u30fc\u30d0\u30eb\u5909\u6570\u306a\u3069\u3092\u7f6e\u304f stack\u9818\u57df \u95a2\u6570\u306e\u5f15\u6570\u3084\u30ed\u30fc\u30ab\u30eb\u5909\u6570\u3092\u7f6e\u304f heap\u9818\u57df \u30d7\u30ed\u30b0\u30e9\u30e0\u306e\u30c7\u30fc\u30bf\u3092\u7f6e\u304f(\u52d5\u7684\u306b\u78ba\u4fdd\u3057\u305f\u3082\u306e)","title":"\u30e1\u30e2\u30ea\u306e\u9818\u57df"},{"location":"tech/\u30e1\u30e2\u30ea/#stack","text":"Stack\u306f\u95a2\u6570\u547c\u3073\u51fa\u3057\u306e\u305f\u3081\u306b\u4f7f\u308f\u308c\u308b","title":"Stack\u3068\u95a2\u6570"},{"location":"tech/\u30e1\u30e2\u30ea/#heap","text":"\u30d2\u30fc\u30d7\u306f\u4efb\u610f\u306e\u30b5\u30a4\u30ba\u306e\u30c7\u30fc\u30bf\u3092\u4efb\u610f\u306e\u9806\u756a\u306b\u78ba\u4fdd\u30fb\u89e3\u653e\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u308b new\u3057\u305f\u3068\u304d\u306f\u3053\u3061\u3089\u306e\u65b9\u6cd5\u3067\u78ba\u4fdd\u3055\u308c\u308b\uff0e vector\u306e\u914d\u5217\u3082\u3053\u3053\u306b\u5165\u308b","title":"heap\u9818\u57df"},{"location":"tech/\u30e1\u30e2\u30ea/#stackoverflow","text":"","title":"StackOverflow\u6642\u306e\u30a8\u30e9\u30fc"},{"location":"tech/\u30e1\u30e2\u30ea/#python","text":"Fatal Python error: Cannot recover from stack overflow. Current thread 0x00007f434f650700 (most recent call first): File \"***.py\" , line 4 in ... File \"***.py\" , line 7 in ... File \"***.py\" , line 7 in ...","title":"python"},{"location":"wiki/attention/","text":"Attention\u3068\u3044\u3046\u6280\u6cd5\u306b\u3064\u3044\u3066 Deep Learning \u3067\u4f7f\u308f\u308c\u3066\u308b attention \u3063\u3066\u3084\u3064\u3092\u8abf\u3079\u3066\u307f\u305f \u30a8\u30f3\u30b3\u30fc\u30c0\u30fc\u30fb\u30c7\u30b3\u30fc\u30c0\u30fc\u30e2\u30c7\u30eb\u306b\u304a\u3051\u308battention\u306e\u8aac\u660e \u5165\u529b\u60c5\u5831\u5168\u4f53\u3067\u306f\u306a\u304f\uff0c\u305d\u306e\u4e00\u90e8\u306e\u307f\u3092\u7279\u306b\u30d5\u30a9\u30fc\u30ab\u30b9\u3057\u305f\u30d9\u30af\u30c8\u30eb\u3092\u30c7\u30b3\u30fc\u30c0\u30fc\u3067\u4f7f\u7528\u3059\u308b\u4ed5\u7d44\u307f\uff0e","title":"Attention\u3068\u3044\u3046\u6280\u6cd5\u306b\u3064\u3044\u3066"},{"location":"wiki/attention/#attention","text":"","title":"Attention\u3068\u3044\u3046\u6280\u6cd5\u306b\u3064\u3044\u3066"},{"location":"wiki/attention/#deep-learning-attention","text":"\u30a8\u30f3\u30b3\u30fc\u30c0\u30fc\u30fb\u30c7\u30b3\u30fc\u30c0\u30fc\u30e2\u30c7\u30eb\u306b\u304a\u3051\u308battention\u306e\u8aac\u660e \u5165\u529b\u60c5\u5831\u5168\u4f53\u3067\u306f\u306a\u304f\uff0c\u305d\u306e\u4e00\u90e8\u306e\u307f\u3092\u7279\u306b\u30d5\u30a9\u30fc\u30ab\u30b9\u3057\u305f\u30d9\u30af\u30c8\u30eb\u3092\u30c7\u30b3\u30fc\u30c0\u30fc\u3067\u4f7f\u7528\u3059\u308b\u4ed5\u7d44\u307f\uff0e","title":"Deep Learning \u3067\u4f7f\u308f\u308c\u3066\u308b attention \u3063\u3066\u3084\u3064\u3092\u8abf\u3079\u3066\u307f\u305f"},{"location":"wiki/clion/","text":"Clion \u30b7\u30e7\u30fc\u30c8\u30ab\u30c3\u30c8 \u30b3\u30de\u30f3\u30c9\u691c\u7d22 : Cmd + A \u30b3\u30d4\u30fc\u5c65\u6b74 : Cmd + V \u30de\u30eb\u30c1\u30ab\u30fc\u30bd\u30eb : C + g RUN : C + r \u30d5\u30a9\u30fc\u30de\u30c3\u30c8 : Cmd + Alt + l \u578b\u3092\u8868\u793a : Cmd + C + p \u30d5\u30a1\u30a4\u30eb\u8868\u793a : Cmd + e \u56f2\u3080 : Cmd Alt t","title":"Clion \u30b7\u30e7\u30fc\u30c8\u30ab\u30c3\u30c8"},{"location":"wiki/clion/#clion","text":"\u30b3\u30de\u30f3\u30c9\u691c\u7d22 : Cmd + A \u30b3\u30d4\u30fc\u5c65\u6b74 : Cmd + V \u30de\u30eb\u30c1\u30ab\u30fc\u30bd\u30eb : C + g RUN : C + r \u30d5\u30a9\u30fc\u30de\u30c3\u30c8 : Cmd + Alt + l \u578b\u3092\u8868\u793a : Cmd + C + p \u30d5\u30a1\u30a4\u30eb\u8868\u793a : Cmd + e \u56f2\u3080 : Cmd Alt t","title":"Clion \u30b7\u30e7\u30fc\u30c8\u30ab\u30c3\u30c8"},{"location":"wiki/emacs/","text":"emacs\u306e\u30bb\u30c3\u30c6\u30a3\u30f3\u30b0\u95a2\u9023 \u30d1\u30c3\u30b1\u30fc\u30b8\u60c5\u5831\u3092\u6700\u65b0\u306b\u66f4\u65b0 M-x package-refresh-contents \u30d1\u30c3\u30b1\u30fc\u30b8\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb M-x package-list-packages emacs\u306e\u521d\u671f\u5316\u30d5\u30a1\u30a4\u30eb\u306f~.emacs\u3001~/.emacs.el\u3001~/.emacs.d/init.el\u306e\u9806\u306b\u691c\u7d22\u3055\u308c\u308b \u3057\u304b\u3057\uff0c\u8907\u6570\u30d5\u30a1\u30a4\u30eb\u304c\u3042\u3063\u3066\u3082\u4e00\u3064\u3057\u304b\u691c\u7d22\u3055\u308c\u306a\u3044\u306e\u3067\u4e00\u756a\u6700\u5f8c\u306e\u3092\u4f7f\u3046\u3088\u3046\u306b \u5165\u308c\u3066\u308b\u30d1\u30c3\u30b1\u30fc\u30b8 guide-key fzf markdown-guide \u53c2\u8003 https://qiita.com/yuzo_nishikawa/items/748f84de7a963e29412f","title":"emacs\u306e\u30bb\u30c3\u30c6\u30a3\u30f3\u30b0\u95a2\u9023"},{"location":"wiki/emacs/#emacs","text":"\u30d1\u30c3\u30b1\u30fc\u30b8\u60c5\u5831\u3092\u6700\u65b0\u306b\u66f4\u65b0 M-x package-refresh-contents \u30d1\u30c3\u30b1\u30fc\u30b8\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb M-x package-list-packages emacs\u306e\u521d\u671f\u5316\u30d5\u30a1\u30a4\u30eb\u306f~.emacs\u3001~/.emacs.el\u3001~/.emacs.d/init.el\u306e\u9806\u306b\u691c\u7d22\u3055\u308c\u308b \u3057\u304b\u3057\uff0c\u8907\u6570\u30d5\u30a1\u30a4\u30eb\u304c\u3042\u3063\u3066\u3082\u4e00\u3064\u3057\u304b\u691c\u7d22\u3055\u308c\u306a\u3044\u306e\u3067\u4e00\u756a\u6700\u5f8c\u306e\u3092\u4f7f\u3046\u3088\u3046\u306b \u5165\u308c\u3066\u308b\u30d1\u30c3\u30b1\u30fc\u30b8 guide-key fzf markdown-guide \u53c2\u8003 https://qiita.com/yuzo_nishikawa/items/748f84de7a963e29412f","title":"emacs\u306e\u30bb\u30c3\u30c6\u30a3\u30f3\u30b0\u95a2\u9023"},{"location":"wiki/fzf/","text":"fzf\u306e\u4f7f\u3044\u65b9 \u30b7\u30e7\u30fc\u30c8\u30ab\u30c3\u30c8 C-t : Paste the selected files C-r : Paste the selected command from history Alt-c : cd into the selected directory \u30b3\u30de\u30f3\u30c9 fzf --preview 'cat {}'","title":"fzf\u306e\u4f7f\u3044\u65b9"},{"location":"wiki/fzf/#fzf","text":"","title":"fzf\u306e\u4f7f\u3044\u65b9"},{"location":"wiki/fzf/#_1","text":"C-t : Paste the selected files C-r : Paste the selected command from history Alt-c : cd into the selected directory","title":"\u30b7\u30e7\u30fc\u30c8\u30ab\u30c3\u30c8"},{"location":"wiki/fzf/#_2","text":"fzf --preview 'cat {}'","title":"\u30b3\u30de\u30f3\u30c9"},{"location":"wiki/git/","text":"git\u64cd\u4f5c\u306e\u57fa\u672c git add . git commit -m \"comment\" git push -u origin master origin\u3068\u3044\u3046\u30ea\u30e2\u30fc\u30c8\u30ea\u30dd\u30b8\u30c8\u30ea\u306emaster\u3068\u3044\u3046\u30d6\u30e9\u30f3\u30c1\u306b\u30d7\u30c3\u30b7\u30e5\u3059\u308b\u3068\u3044\u3046\u610f\u5473","title":"git\u64cd\u4f5c\u306e\u57fa\u672c"},{"location":"wiki/git/#git","text":"git add . git commit -m \"comment\" git push -u origin master origin\u3068\u3044\u3046\u30ea\u30e2\u30fc\u30c8\u30ea\u30dd\u30b8\u30c8\u30ea\u306emaster\u3068\u3044\u3046\u30d6\u30e9\u30f3\u30c1\u306b\u30d7\u30c3\u30b7\u30e5\u3059\u308b\u3068\u3044\u3046\u610f\u5473","title":"git\u64cd\u4f5c\u306e\u57fa\u672c"},{"location":"wiki/latex/","text":"latex\u30b3\u30de\u30f3\u30c9 platex hogehoge.tex dvipdfmx hogehoge.dvi","title":"latex\u30b3\u30de\u30f3\u30c9"},{"location":"wiki/latex/#latex","text":"platex hogehoge.tex dvipdfmx hogehoge.dvi","title":"latex\u30b3\u30de\u30f3\u30c9"},{"location":"wiki/terminal/","text":"\u30bf\u30fc\u30df\u30ca\u30eb\u306e\u8a2d\u5b9a\u3068\u304b emacs \u30b7\u30e7\u30fc\u30c8\u30ab\u30c3\u30c8 \u53f3\u306e\u30d0\u30c3\u30d5\u30a1\u306b\u79fb\u52d5 C + x + \u2190 \u5de6\u306e\u30d0\u30c3\u30d5\u30a1\u306b\u79fb\u52d5 C + x + \u2192 (define-key global-map [?\\s-x] 'kill-region) (define-key global-map [?\\s-c] 'kill-ring-save) (define-key global-map [?\\s-v] 'yank)","title":"\u30bf\u30fc\u30df\u30ca\u30eb\u306e\u8a2d\u5b9a\u3068\u304b"},{"location":"wiki/terminal/#_1","text":"emacs \u30b7\u30e7\u30fc\u30c8\u30ab\u30c3\u30c8 \u53f3\u306e\u30d0\u30c3\u30d5\u30a1\u306b\u79fb\u52d5 C + x + \u2190 \u5de6\u306e\u30d0\u30c3\u30d5\u30a1\u306b\u79fb\u52d5 C + x + \u2192 (define-key global-map [?\\s-x] 'kill-region) (define-key global-map [?\\s-c] 'kill-ring-save) (define-key global-map [?\\s-v] 'yank)","title":"\u30bf\u30fc\u30df\u30ca\u30eb\u306e\u8a2d\u5b9a\u3068\u304b"},{"location":"wiki/zsh/","text":"zsh\u306e\u8a2d\u5b9a zplug\u3092\u5165\u308c\u308b brew install zplug ### zplug export ZPLUG_HOME = /usr/local/opt/zplug source $ZPLUG_HOME /init.zsh zplug 'zsh-users/zsh-completions' zplug 'zsh-users/zaw' zplug 'zsh-users/zsh-syntax-highlighting' , defer:2 zplug \"b4b4r07/enhancd\" , use:init.sh zplug check || zplug install fzy\u3092\u5165\u308c\u308b brew install fzy emacs\u3092\u6700\u65b0\u306b\u3059\u308b brew tap d12frosted/emacs-plus brew install emacs-plus brew linkapps emacs-plus #Spacemacs\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb cd ~ mv .emacs.d .emacs.d.bak mv .emacs .emacs.bak git clone https://github.com/syl20bnr/spacemacs ~/.emacs.d \u8d77\u52d5 emacs --insecure","title":"zsh\u306e\u8a2d\u5b9a"},{"location":"wiki/zsh/#zsh","text":"","title":"zsh\u306e\u8a2d\u5b9a"},{"location":"wiki/zsh/#zplug","text":"brew install zplug ### zplug export ZPLUG_HOME = /usr/local/opt/zplug source $ZPLUG_HOME /init.zsh zplug 'zsh-users/zsh-completions' zplug 'zsh-users/zaw' zplug 'zsh-users/zsh-syntax-highlighting' , defer:2 zplug \"b4b4r07/enhancd\" , use:init.sh zplug check || zplug install","title":"zplug\u3092\u5165\u308c\u308b"},{"location":"wiki/zsh/#fzy","text":"brew install fzy","title":"fzy\u3092\u5165\u308c\u308b"},{"location":"wiki/zsh/#emacs","text":"brew tap d12frosted/emacs-plus brew install emacs-plus brew linkapps emacs-plus #Spacemacs\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb cd ~ mv .emacs.d .emacs.d.bak mv .emacs .emacs.bak git clone https://github.com/syl20bnr/spacemacs ~/.emacs.d \u8d77\u52d5 emacs --insecure","title":"emacs\u3092\u6700\u65b0\u306b\u3059\u308b"},{"location":"wiki/deeplearning/","text":"","title":"Index"},{"location":"wiki/deeplearning/LSTM/","text":"LSTM (Long short-term memory) \u6642\u7cfb\u5217\u30c7\u30fc\u30bf(sequental data)\u306b\u5bfe\u3059\u308b\u30e2\u30c7\u30eb\uff0c\u3042\u308b\u3044\u306f\u69cb\u9020(archtecuture)\u306e\u4e00\u7a2e \u9577\u671f\u4f9d\u5b58\u3092\u5b66\u7fd2\u53ef\u80fd\u3067\u3042\u308b","title":"LSTM (Long short-term memory)"},{"location":"wiki/deeplearning/LSTM/#lstm-long-short-term-memory","text":"\u6642\u7cfb\u5217\u30c7\u30fc\u30bf(sequental data)\u306b\u5bfe\u3059\u308b\u30e2\u30c7\u30eb\uff0c\u3042\u308b\u3044\u306f\u69cb\u9020(archtecuture)\u306e\u4e00\u7a2e \u9577\u671f\u4f9d\u5b58\u3092\u5b66\u7fd2\u53ef\u80fd\u3067\u3042\u308b","title":"LSTM (Long short-term memory)"},{"location":"wiki/deeplearning/RNN/","text":"RNN (Recurrent Neural Network) \u601d\u8003\u306e\u6301\u7d9a\u6027\u3092\u8868\u73fe\u3057\u3066\u3044\u308b \u4eba\u9593\u306f\u6bce\u79d2\u30bc\u30ed\u304b\u3089\u601d\u8003\u3092\u958b\u59cb\u3059\u308b\u3053\u3068\u306f\u3042\u308a\u307e\u305b\u3093\u3002\u3053\u306e\u30a8\u30c3\u30bb\u30a4\u3092\u8aad\u3093\u3067\u3044\u308b\u9593\u3001\u3042\u306a\u305f\u306f\u524d\u306e\u5358\u8a9e\u306e\u7406\u89e3\u306b\u57fa\u3065\u3044\u3066\u3001\u5404\u5358\u8a9e\u3092\u7406\u89e3\u3057\u307e\u3059\u3002\u3059\u3079\u3066\u3092\u6368\u3066\u3066\u3001\u307e\u305f\u30bc\u30ed\u304b\u3089\u601d\u8003\u3092\u958b\u59cb\u3057\u3066\u306f\u3044\u307e\u305b\u3093\u3002\u3042\u306a\u305f\u306e\u601d\u8003\u306f\u6301\u7d9a\u6027\u3092\u6301\u3063\u3066\u3044\u307e\u3059\u3002 \u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u5206\u91ce\u3067\u9ad8\u3044\u6210\u679c\u3092\u4e0a\u3052\u3066\u3044\u308b\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0 Recurrent\u306f\u518d\u5e30\u306e\u610f\u5473\uff0c\u5185\u90e8\u306b\u30eb\u30fc\u30d7\u3092\u6301\u3064 RNN\u30d9\u30fc\u30b9\u306e\u8a00\u8a9e\u30e2\u30c7\u30eb \u8a00\u8a9e\u30e2\u30c7\u30eb\u3068\u306f \u6587\u7ae0\u304c\u5b9f\u969b\u306b\u3069\u308c\u3050\u3089\u3044\u306e\u78ba\u7387\u3067\u73fe\u308c\u308b\u306e\u304b\u3092\u30b9\u30b3\u30a2\u30ea\u30f3\u30b0\u3059\u308b\u3053\u3068 \u3053\u306e\u30b9\u30b3\u30a2\u306f\u30bb\u30de\u30f3\u30c6\u30a3\u30af\u30b9\u7684\u306b\u6b63\u3057\u3044\u304b\u3069\u3046\u304b\u306e\u5224\u65ad\u57fa\u6e96\u306b\u306a\u308b \u65b0\u305f\u306a\u30c6\u30ad\u30b9\u30c8\u3092\u751f\u6210\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u308b Andrej Karpathy\u306e\u30d6\u30ed\u30b0\u3067\u306f\u5358\u8a9e\u30ec\u30d9\u30eb\u306eRNN\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092\u89e3\u8aac\u30fb\u958b\u767a\u3057\u3066\u3044\u308b \u5229\u70b9 \u6587\u7ae0\u306a\u3069\u306e\u9023\u7d9a\u7684\u306a\u60c5\u5831\u3092\u5229\u7528\u3067\u304d\u308b * \u6b20\u70b9 RNN\u5358\u4f53\u3067\u306f\u9577\u671f\u9593\u306e\u8a18\u61b6\u304c\u3067\u304d\u306a\u3044\u2192 LSTM","title":"RNN (Recurrent Neural Network)"},{"location":"wiki/deeplearning/RNN/#rnn-recurrent-neural-network","text":"\u601d\u8003\u306e\u6301\u7d9a\u6027\u3092\u8868\u73fe\u3057\u3066\u3044\u308b \u4eba\u9593\u306f\u6bce\u79d2\u30bc\u30ed\u304b\u3089\u601d\u8003\u3092\u958b\u59cb\u3059\u308b\u3053\u3068\u306f\u3042\u308a\u307e\u305b\u3093\u3002\u3053\u306e\u30a8\u30c3\u30bb\u30a4\u3092\u8aad\u3093\u3067\u3044\u308b\u9593\u3001\u3042\u306a\u305f\u306f\u524d\u306e\u5358\u8a9e\u306e\u7406\u89e3\u306b\u57fa\u3065\u3044\u3066\u3001\u5404\u5358\u8a9e\u3092\u7406\u89e3\u3057\u307e\u3059\u3002\u3059\u3079\u3066\u3092\u6368\u3066\u3066\u3001\u307e\u305f\u30bc\u30ed\u304b\u3089\u601d\u8003\u3092\u958b\u59cb\u3057\u3066\u306f\u3044\u307e\u305b\u3093\u3002\u3042\u306a\u305f\u306e\u601d\u8003\u306f\u6301\u7d9a\u6027\u3092\u6301\u3063\u3066\u3044\u307e\u3059\u3002 \u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u5206\u91ce\u3067\u9ad8\u3044\u6210\u679c\u3092\u4e0a\u3052\u3066\u3044\u308b\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0 Recurrent\u306f\u518d\u5e30\u306e\u610f\u5473\uff0c\u5185\u90e8\u306b\u30eb\u30fc\u30d7\u3092\u6301\u3064","title":"RNN (Recurrent Neural Network)"},{"location":"wiki/deeplearning/RNN/#rnn","text":"\u8a00\u8a9e\u30e2\u30c7\u30eb\u3068\u306f \u6587\u7ae0\u304c\u5b9f\u969b\u306b\u3069\u308c\u3050\u3089\u3044\u306e\u78ba\u7387\u3067\u73fe\u308c\u308b\u306e\u304b\u3092\u30b9\u30b3\u30a2\u30ea\u30f3\u30b0\u3059\u308b\u3053\u3068 \u3053\u306e\u30b9\u30b3\u30a2\u306f\u30bb\u30de\u30f3\u30c6\u30a3\u30af\u30b9\u7684\u306b\u6b63\u3057\u3044\u304b\u3069\u3046\u304b\u306e\u5224\u65ad\u57fa\u6e96\u306b\u306a\u308b \u65b0\u305f\u306a\u30c6\u30ad\u30b9\u30c8\u3092\u751f\u6210\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u308b Andrej Karpathy\u306e\u30d6\u30ed\u30b0\u3067\u306f\u5358\u8a9e\u30ec\u30d9\u30eb\u306eRNN\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092\u89e3\u8aac\u30fb\u958b\u767a\u3057\u3066\u3044\u308b","title":"RNN\u30d9\u30fc\u30b9\u306e\u8a00\u8a9e\u30e2\u30c7\u30eb"},{"location":"wiki/deeplearning/RNN/#_1","text":"\u6587\u7ae0\u306a\u3069\u306e\u9023\u7d9a\u7684\u306a\u60c5\u5831\u3092\u5229\u7528\u3067\u304d\u308b *","title":"\u5229\u70b9"},{"location":"wiki/deeplearning/RNN/#_2","text":"RNN\u5358\u4f53\u3067\u306f\u9577\u671f\u9593\u306e\u8a18\u61b6\u304c\u3067\u304d\u306a\u3044\u2192 LSTM","title":"\u6b20\u70b9"},{"location":"wiki/deeplearning/adversarial attack/","text":"Adversarial Attack \u5206\u985e\u5668\u306b\u5bfe\u3059\u308b\u8106\u5f31\u6027\u653b\u6483 \u5206\u985e\u5668\u304c\u6b63\u3057\u304f\u5206\u985e\u3067\u304d\u3066\u3044\u305f\u753b\u50cf\u306b\u3001\u4eba\u306e\u76ee\u3067\u306f\u5224\u5225\u3067\u304d\u306a\u3044\u7a0b\u5ea6\u306e\u30ce\u30a4\u30ba\u3092\u306e\u305b\u308b\u3053\u3068\u3067\u3001\u4f5c\u70ba\u7684\u306b\u5206\u985e\u5668\u306e\u5224\u65ad\u3092\u8aa4\u3089\u305b\u308b","title":"Adversarial Attack"},{"location":"wiki/deeplearning/adversarial attack/#adversarial-attack","text":"\u5206\u985e\u5668\u306b\u5bfe\u3059\u308b\u8106\u5f31\u6027\u653b\u6483 \u5206\u985e\u5668\u304c\u6b63\u3057\u304f\u5206\u985e\u3067\u304d\u3066\u3044\u305f\u753b\u50cf\u306b\u3001\u4eba\u306e\u76ee\u3067\u306f\u5224\u5225\u3067\u304d\u306a\u3044\u7a0b\u5ea6\u306e\u30ce\u30a4\u30ba\u3092\u306e\u305b\u308b\u3053\u3068\u3067\u3001\u4f5c\u70ba\u7684\u306b\u5206\u985e\u5668\u306e\u5224\u65ad\u3092\u8aa4\u3089\u305b\u308b","title":"Adversarial Attack"},{"location":"wiki/deeplearning/attention/","text":"Attention \u30a8\u30f3\u30b3\u30fc\u30c0\u30fc\u30fb\u30c7\u30b3\u30fc\u30c0\u30fc\u30e2\u30c7\u30eb\u306b\u304a\u3051\u308b\u306b\u304a\u3044\u3066\u5165\u529b\u60c5\u5831\u5168\u4f53\u3067\u306f\u306a\u304f\u3001\u305d\u306e\u4e00\u90e8\u306e\u307f\u3092\u7279\u306b\u30d5\u30a9\u30fc\u30ab\u30b9\u3057\u305f\u30d9\u30af\u30c8\u30eb\u3092\u30c7\u30b3\u30fc\u30c0\u30fc\u3067\u4f7f\u7528\u3059\u308b\u4ed5\u7d44\u307f \u901a\u5e38\u306e\u30a8\u30f3\u30b3\u30fc\u30c0\u30fb\u30c7\u30b3\u30fc\u30c0\u30e2\u30c7\u30eb\u3067\u306f\u3001\u30a8\u30f3\u30b3\u30fc\u30c0\u306e\u51fa\u529b\u306f\u4e00\u3064\u3057\u304b\u30c7\u30b3\u30fc\u30c0\u3067\u306f\u4f7f\u7528\u3055\u308c\u306a\u3044\uff0e \u901a\u5e38\u5165\u529b\u6587\u306e\u60c5\u5831\u3092\u7279\u5b9a\u306e\u30b5\u30a4\u30ba\u306e\u30d9\u30af\u30c8\u30eb\u306b\u307e\u3068\u3081\u308b\u5fc5\u8981\u304c\u3042\u308a\uff0c\u9577\u6587\u306b\u306a\u308c\u3070\u306a\u308b\u307b\u3069\u5143\u306e\u60c5\u5831\u306e\u5727\u7e2e\u7cbe\u5ea6\u304c\u60aa\u304f\u306a\u308b\uff0e attention \u3092\u7528\u3044\u305f\u30e2\u30c7\u30eb\u3067\u306f\uff0c\u30a8\u30f3\u30b3\u30fc\u30c0\u30fc\u306e\u96a0\u308c\u5c64\u306e\u3046\u3061\uff0c\u7279\u5b9a\u306e\u5165\u529b\u5358\u8a9e\u3084\u305d\u306e\u5468\u8fba\u306e\u5358\u8a9e\u306b\u30d5\u30a9\u30fc\u30ab\u30b9\u3057\u305f\u30d9\u30af\u30c8\u30eb\u3092\u30c7\u30b3\u30fc\u30c0\u3067\u7528\u3044\u308b\uff0e\u3053\u308c\u306b\u3088\u308a\uff0c\u30c7\u30b3\u30fc\u30c0\u306e\u3042\u308b\u6642\u70b9\u3067\u5fc5\u8981\u306a\u60c5\u5831\u306b\u30d5\u30a9\u30fc\u30ab\u30b9\u3057\u3066\u4f7f\u7528\u3059\u308b\u3053\u3068\u304c\u3067\u304d\uff0c\u5165\u529b\u6587\u306e\u9577\u3055\u306b\u95a2\u4fc2\u306a\u304f\u30c7\u30b3\u30fc\u30c9\u3092\u52b9\u7387\u3088\u304f\u884c\u3046\u3053\u3068\u304c\u3067\u304d\u308b\uff0e","title":"Attention"},{"location":"wiki/deeplearning/attention/#attention","text":"\u30a8\u30f3\u30b3\u30fc\u30c0\u30fc\u30fb\u30c7\u30b3\u30fc\u30c0\u30fc\u30e2\u30c7\u30eb\u306b\u304a\u3051\u308b\u306b\u304a\u3044\u3066\u5165\u529b\u60c5\u5831\u5168\u4f53\u3067\u306f\u306a\u304f\u3001\u305d\u306e\u4e00\u90e8\u306e\u307f\u3092\u7279\u306b\u30d5\u30a9\u30fc\u30ab\u30b9\u3057\u305f\u30d9\u30af\u30c8\u30eb\u3092\u30c7\u30b3\u30fc\u30c0\u30fc\u3067\u4f7f\u7528\u3059\u308b\u4ed5\u7d44\u307f \u901a\u5e38\u306e\u30a8\u30f3\u30b3\u30fc\u30c0\u30fb\u30c7\u30b3\u30fc\u30c0\u30e2\u30c7\u30eb\u3067\u306f\u3001\u30a8\u30f3\u30b3\u30fc\u30c0\u306e\u51fa\u529b\u306f\u4e00\u3064\u3057\u304b\u30c7\u30b3\u30fc\u30c0\u3067\u306f\u4f7f\u7528\u3055\u308c\u306a\u3044\uff0e \u901a\u5e38\u5165\u529b\u6587\u306e\u60c5\u5831\u3092\u7279\u5b9a\u306e\u30b5\u30a4\u30ba\u306e\u30d9\u30af\u30c8\u30eb\u306b\u307e\u3068\u3081\u308b\u5fc5\u8981\u304c\u3042\u308a\uff0c\u9577\u6587\u306b\u306a\u308c\u3070\u306a\u308b\u307b\u3069\u5143\u306e\u60c5\u5831\u306e\u5727\u7e2e\u7cbe\u5ea6\u304c\u60aa\u304f\u306a\u308b\uff0e attention \u3092\u7528\u3044\u305f\u30e2\u30c7\u30eb\u3067\u306f\uff0c\u30a8\u30f3\u30b3\u30fc\u30c0\u30fc\u306e\u96a0\u308c\u5c64\u306e\u3046\u3061\uff0c\u7279\u5b9a\u306e\u5165\u529b\u5358\u8a9e\u3084\u305d\u306e\u5468\u8fba\u306e\u5358\u8a9e\u306b\u30d5\u30a9\u30fc\u30ab\u30b9\u3057\u305f\u30d9\u30af\u30c8\u30eb\u3092\u30c7\u30b3\u30fc\u30c0\u3067\u7528\u3044\u308b\uff0e\u3053\u308c\u306b\u3088\u308a\uff0c\u30c7\u30b3\u30fc\u30c0\u306e\u3042\u308b\u6642\u70b9\u3067\u5fc5\u8981\u306a\u60c5\u5831\u306b\u30d5\u30a9\u30fc\u30ab\u30b9\u3057\u3066\u4f7f\u7528\u3059\u308b\u3053\u3068\u304c\u3067\u304d\uff0c\u5165\u529b\u6587\u306e\u9577\u3055\u306b\u95a2\u4fc2\u306a\u304f\u30c7\u30b3\u30fc\u30c9\u3092\u52b9\u7387\u3088\u304f\u884c\u3046\u3053\u3068\u304c\u3067\u304d\u308b\uff0e","title":"Attention"},{"location":"wiki/heroku/heroku/","text":"\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb brew install heroku-toolbelt \u30ed\u30b0\u30a4\u30f3 heroku login \u6982\u8981 \u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u3092\u7a3c\u50cd\u3055\u305b\u308b\u305f\u3081\u306e\u30d7\u30e9\u30c3\u30c8\u30d5\u30a9\u30fc\u30e0 PaaS (Platform as a Service) heroku.com \u5fc5\u8981\u77e5\u8b58 Ruby on Rails / Ruby Unix \u30b3\u30de\u30f3\u30c9 git posgreSQL \u74b0\u5883 \u30ed\u30fc\u30ab\u30eb\u958b\u767a\u74b0\u5883\u3092\u4f5c\u308b PaaS\u3068\u306f\u4f55\u304b\uff1f \u4eca\u307e\u3067\u306f\u30ed\u30fc\u30ab\u30eb\u958b\u767a\u74b0\u5883\u5185\u3067\u4f5c\u308a\uff0c\u30b5\u30fc\u30d0\u306b\u9001\u308b \u30b5\u30fc\u30d0\u5185\u3082\u540c\u3058\u74b0\u5883\u3092\u4f5c\u308b\u5fc5\u8981\u304c\u3042\u308b\uff0c\u307e\u305f\uff0c\u30bb\u30ad\u30e5\u30ea\u30c6\u30a3\u3084\u5206\u6563\u7528\u306e\u30b7\u30b9\u30c6\u30e0\u3082\u5fc5\u8981\u3060\u3063\u305f PaaS\u3067\u306f\uff0c\u30bd\u30fc\u30b9\u30b3\u30fc\u30c9\u306b\u3044\u304f\u3064\u304b\u306e\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb\u3092\u3064\u3051\u308b\u3060\u3051\u3067Heroku\u3067\u74b0\u5883\u304c\u81ea\u52d5\u3067\u4f5c\u3089\u308c\u308b\u3088\u3046\u306b\u306a\u3063\u305f\uff0e Dependencies : \u4f9d\u5b58\u95a2\u4fc2 Procfile : \u5b9f\u969b\u306bheroku\u4e0a\u3067\u52d5\u304b\u3057\u305f\u3044\u30b3\u30de\u30f3\u30c9\u4e00\u89a7 git\u306e\u30b3\u30de\u30f3\u30c9\u4e00\u767a\u3067heroku\u306b\u9001\u308b\u3053\u3068\u304c\u3067\u304d\u308b Slug : \u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u3092\u3055\u3063\u3068\u7a3c\u50cd\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u308b Dyno : \u4e00\u6642\u7684\u306a\u30c7\u30fc\u30bf Addon : \u30e1\u30fc\u30eb\u3084\u5206\u6790\u30c4\u30fc\u30eb\u30a2\u30c9\u30aa\u30f3 JS\u3084CSS\u3084\u753b\u50cf\u306f\u5225\u30b5\u30fc\u30d3\u30b9\u3092\u5229\u7528\u3059\u308b\u65b9\u304c\u826f\u3044","title":"\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb"},{"location":"wiki/heroku/heroku/#_1","text":"brew install heroku-toolbelt \u30ed\u30b0\u30a4\u30f3 heroku login","title":"\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb"},{"location":"wiki/heroku/heroku/#_2","text":"\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u3092\u7a3c\u50cd\u3055\u305b\u308b\u305f\u3081\u306e\u30d7\u30e9\u30c3\u30c8\u30d5\u30a9\u30fc\u30e0 PaaS (Platform as a Service) heroku.com","title":"\u6982\u8981"},{"location":"wiki/heroku/heroku/#_3","text":"Ruby on Rails / Ruby Unix \u30b3\u30de\u30f3\u30c9 git posgreSQL","title":"\u5fc5\u8981\u77e5\u8b58"},{"location":"wiki/heroku/heroku/#_4","text":"\u30ed\u30fc\u30ab\u30eb\u958b\u767a\u74b0\u5883\u3092\u4f5c\u308b","title":"\u74b0\u5883"},{"location":"wiki/heroku/heroku/#paas","text":"\u4eca\u307e\u3067\u306f\u30ed\u30fc\u30ab\u30eb\u958b\u767a\u74b0\u5883\u5185\u3067\u4f5c\u308a\uff0c\u30b5\u30fc\u30d0\u306b\u9001\u308b \u30b5\u30fc\u30d0\u5185\u3082\u540c\u3058\u74b0\u5883\u3092\u4f5c\u308b\u5fc5\u8981\u304c\u3042\u308b\uff0c\u307e\u305f\uff0c\u30bb\u30ad\u30e5\u30ea\u30c6\u30a3\u3084\u5206\u6563\u7528\u306e\u30b7\u30b9\u30c6\u30e0\u3082\u5fc5\u8981\u3060\u3063\u305f PaaS\u3067\u306f\uff0c\u30bd\u30fc\u30b9\u30b3\u30fc\u30c9\u306b\u3044\u304f\u3064\u304b\u306e\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb\u3092\u3064\u3051\u308b\u3060\u3051\u3067Heroku\u3067\u74b0\u5883\u304c\u81ea\u52d5\u3067\u4f5c\u3089\u308c\u308b\u3088\u3046\u306b\u306a\u3063\u305f\uff0e Dependencies : \u4f9d\u5b58\u95a2\u4fc2 Procfile : \u5b9f\u969b\u306bheroku\u4e0a\u3067\u52d5\u304b\u3057\u305f\u3044\u30b3\u30de\u30f3\u30c9\u4e00\u89a7 git\u306e\u30b3\u30de\u30f3\u30c9\u4e00\u767a\u3067heroku\u306b\u9001\u308b\u3053\u3068\u304c\u3067\u304d\u308b Slug : \u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u3092\u3055\u3063\u3068\u7a3c\u50cd\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u308b Dyno : \u4e00\u6642\u7684\u306a\u30c7\u30fc\u30bf Addon : \u30e1\u30fc\u30eb\u3084\u5206\u6790\u30c4\u30fc\u30eb\u30a2\u30c9\u30aa\u30f3 JS\u3084CSS\u3084\u753b\u50cf\u306f\u5225\u30b5\u30fc\u30d3\u30b9\u3092\u5229\u7528\u3059\u308b\u65b9\u304c\u826f\u3044","title":"PaaS\u3068\u306f\u4f55\u304b\uff1f"},{"location":"wiki/heroku/postgresql/","text":"postgresql \u57fa\u672c\u52d5\u4f5c \u8d77\u52d5 : pg_ctl start \u7d42\u4e86 : pg_ctl stop DB\u8868\u793a: psql -l \u30ed\u30b0\u30a4\u30f3: psql posgres \u30ed\u30b0\u30a2\u30a6\u30c8: psql \\q \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb ~/.zshrc\u306b export PGDATA=/usr/local/var/postgres","title":"postgresql"},{"location":"wiki/heroku/postgresql/#postgresql","text":"","title":"postgresql"},{"location":"wiki/heroku/postgresql/#_1","text":"\u8d77\u52d5 : pg_ctl start \u7d42\u4e86 : pg_ctl stop DB\u8868\u793a: psql -l \u30ed\u30b0\u30a4\u30f3: psql posgres \u30ed\u30b0\u30a2\u30a6\u30c8: psql \\q","title":"\u57fa\u672c\u52d5\u4f5c"},{"location":"wiki/heroku/postgresql/#_2","text":"~/.zshrc\u306b export PGDATA=/usr/local/var/postgres","title":"\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb"},{"location":"wiki/javascript/array/","text":"javascript \u914d\u5217\u57fa\u672c \u914d\u5217 \u521d\u671f\u5316 : let arr = new Array(4) \u4ee3\u5165 : let arr = [1,2,3,4] \u53c2\u7167 : arr[3] \u8ffd\u52a0 : arr.push(5) \u524a\u9664 : arr.pop() # \u6700\u5f8c\u306e\u5024\u306e\u307f\u3092\u524a\u9664 \u524a\u9664 : arr.shift() # \u6700\u521d\u306e\u8981\u7d20\u306e\u307f\u3092\u524a\u9664\u3059\u308b \u7d50\u5408 : arr.concat(arr2) \u30b5\u30a4\u30ba : arr.length","title":"javascript \u914d\u5217\u57fa\u672c"},{"location":"wiki/javascript/array/#javascript","text":"","title":"javascript \u914d\u5217\u57fa\u672c"},{"location":"wiki/javascript/array/#_1","text":"\u521d\u671f\u5316 : let arr = new Array(4) \u4ee3\u5165 : let arr = [1,2,3,4] \u53c2\u7167 : arr[3] \u8ffd\u52a0 : arr.push(5) \u524a\u9664 : arr.pop() # \u6700\u5f8c\u306e\u5024\u306e\u307f\u3092\u524a\u9664 \u524a\u9664 : arr.shift() # \u6700\u521d\u306e\u8981\u7d20\u306e\u307f\u3092\u524a\u9664\u3059\u308b \u7d50\u5408 : arr.concat(arr2) \u30b5\u30a4\u30ba : arr.length","title":"\u914d\u5217"},{"location":"wiki/javascript/css/","text":"css\u57fa\u790e id\u8981\u7d20\u3078\u306e\u8ffd\u52a0 #myid rect{ fill : aqua; } svg { width : 10px; height : 20px; border : 1px solid black; }","title":"css\u57fa\u790e"},{"location":"wiki/javascript/css/#css","text":"","title":"css\u57fa\u790e"},{"location":"wiki/javascript/css/#id","text":"#myid rect{ fill : aqua; } svg { width : 10px; height : 20px; border : 1px solid black; }","title":"id\u8981\u7d20\u3078\u306e\u8ffd\u52a0"},{"location":"wiki/javascript/d3js/","text":"d3js\u57fa\u672c d3.select(\"body\").append(\"p\").text(\"new text\"); # body\u8981\u7d20\u306bp\u8981\u7d20\u306e\u30c6\u30ad\u30b9\u30c8\u3092\u633f\u5165 # id myGraph\u306b\u68d2\u30b0\u30e9\u30d5\u3092\u633f\u5165\u3059\u308b\u4f8b for ( let i = 0 ; i <= dataset . length ; i ++ ){ d3 . select ( \"#myGraph\" ) . append ( \"rect\" ) . attr ( \"x\" , 0 ) . attr ( \"y\" , 25 * i ) . attr ( \"width\" , dataset [ i ]) . attr ( \"height\" , \"20px\" ); } d3 . select ( \"#myGraph\" ) . selectAll ( \"rect\" ) . data ( dataset ) . enter () . append ( \"rect\" ) . attr ( \"width\" , function ( d , i ){ return ( d + \"px\" ; }) . attr ( \"height\" , \"25px\" ) . attr ( \"x\" , 0 ) . attr ( \"y\" , function ( d , i ){ return i * 25 ; })","title":"d3js\u57fa\u672c"},{"location":"wiki/javascript/d3js/#d3js","text":"d3.select(\"body\").append(\"p\").text(\"new text\"); # body\u8981\u7d20\u306bp\u8981\u7d20\u306e\u30c6\u30ad\u30b9\u30c8\u3092\u633f\u5165 # id myGraph\u306b\u68d2\u30b0\u30e9\u30d5\u3092\u633f\u5165\u3059\u308b\u4f8b for ( let i = 0 ; i <= dataset . length ; i ++ ){ d3 . select ( \"#myGraph\" ) . append ( \"rect\" ) . attr ( \"x\" , 0 ) . attr ( \"y\" , 25 * i ) . attr ( \"width\" , dataset [ i ]) . attr ( \"height\" , \"20px\" ); } d3 . select ( \"#myGraph\" ) . selectAll ( \"rect\" ) . data ( dataset ) . enter () . append ( \"rect\" ) . attr ( \"width\" , function ( d , i ){ return ( d + \"px\" ; }) . attr ( \"height\" , \"25px\" ) . attr ( \"x\" , 0 ) . attr ( \"y\" , function ( d , i ){ return i * 25 ; })","title":"d3js\u57fa\u672c"},{"location":"wiki/javascript/html/","text":"HTML\u306e\u57fa\u790e css\u8aad\u307f\u8fbc\u307f : js \u8aad\u307f\u8fbc\u307f : \u5916\u90e8js\u8aad\u307f\u8fbc\u307f : \u30c6\u30f3\u30d7\u30ec <!DOCTYPE html> < html lang = \"ja\" > < head > < meta charset = \"UTF-8\" > < title > Title </ title > < link rel = \"stylesheet\" type = \"text/css\" href = \"style.css\" > < script src = \"http://d3js.org/d3.v3.min.js\" charset = \"utf-8\" ></ script > </ head > < body > < script type = \"text/javascript\" src = \"main.js\" ></ script > </ body > </ html >","title":"HTML\u306e\u57fa\u790e"},{"location":"wiki/javascript/html/#html","text":"css\u8aad\u307f\u8fbc\u307f : js \u8aad\u307f\u8fbc\u307f : \u5916\u90e8js\u8aad\u307f\u8fbc\u307f :","title":"HTML\u306e\u57fa\u790e"},{"location":"wiki/javascript/html/#_1","text":"<!DOCTYPE html> < html lang = \"ja\" > < head > < meta charset = \"UTF-8\" > < title > Title </ title > < link rel = \"stylesheet\" type = \"text/css\" href = \"style.css\" > < script src = \"http://d3js.org/d3.v3.min.js\" charset = \"utf-8\" ></ script > </ head > < body > < script type = \"text/javascript\" src = \"main.js\" ></ script > </ body > </ html >","title":"\u30c6\u30f3\u30d7\u30ec"},{"location":"wiki/javascript/method_chain/","text":"javascript \u30e1\u30bd\u30c3\u30c9\u30c1\u30a7\u30fc\u30f3 \u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u306b\u5bfe\u3057\u3066\u30e1\u30bd\u30c3\u30c9\u3092\u8aad\u307f\u51fa\u3059\u969b\u306b\u9023\u7d9a\u3057\u3066\u6307\u793a\u3092\u4e0e\u3048\u308b\u3053\u3068\u304c\u3067\u304d\u308b\uff0e","title":"javascript \u30e1\u30bd\u30c3\u30c9\u30c1\u30a7\u30fc\u30f3"},{"location":"wiki/javascript/method_chain/#javascript","text":"\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u306b\u5bfe\u3057\u3066\u30e1\u30bd\u30c3\u30c9\u3092\u8aad\u307f\u51fa\u3059\u969b\u306b\u9023\u7d9a\u3057\u3066\u6307\u793a\u3092\u4e0e\u3048\u308b\u3053\u3068\u304c\u3067\u304d\u308b\uff0e","title":"javascript \u30e1\u30bd\u30c3\u30c9\u30c1\u30a7\u30fc\u30f3"},{"location":"wiki/javascript/string/","text":"javascript \u6587\u5b57\u5217 \u5909\u63db \u5909\u63db : num.toString(10) # \u6570\u5b57\u304b\u3089\u6587\u5b57\u5217\u3078 \u7c21\u6613 : ''+num; \u5909\u63db : parseInt(str,10) # \u6587\u5b57\u5217\u304b\u3089\u6570\u5b57\u3078 \u7c21\u6613 : str|0","title":"javascript \u6587\u5b57\u5217"},{"location":"wiki/javascript/string/#javascript","text":"","title":"javascript \u6587\u5b57\u5217"},{"location":"wiki/javascript/string/#_1","text":"\u5909\u63db : num.toString(10) # \u6570\u5b57\u304b\u3089\u6587\u5b57\u5217\u3078 \u7c21\u6613 : ''+num; \u5909\u63db : parseInt(str,10) # \u6587\u5b57\u5217\u304b\u3089\u6570\u5b57\u3078 \u7c21\u6613 : str|0","title":"\u5909\u63db"},{"location":"wiki/procon/next_permutation/","text":"next_permutation do{ }while(next_permutation(v.begin(),v.end()));","title":"next_permutation"},{"location":"wiki/procon/next_permutation/#next_permutation","text":"do{ }while(next_permutation(v.begin(),v.end()));","title":"next_permutation"},{"location":"wiki/procon/pair/","text":"pair\u306e\u30bd\u30fc\u30c8 sort(c.begin(),c.end(),[](auto &l, auto &r) { if(l.fi == r.fi) return l.se < r.se; else return l.fi > r.fi; });","title":"pair\u306e\u30bd\u30fc\u30c8"},{"location":"wiki/procon/pair/#pair","text":"sort(c.begin(),c.end(),[](auto &l, auto &r) { if(l.fi == r.fi) return l.se < r.se; else return l.fi > r.fi; });","title":"pair\u306e\u30bd\u30fc\u30c8"},{"location":"wiki/procon/sort/","text":"\u69cb\u9020\u4f53\u306e\u305d\u30fc\u3068 struct Amida{ int to_n; //\u3069\u306e\u5217\u306b\u5411\u304b\u3046\u304b int to_l; //\u3069\u306e\u7e26\u4f4d\u7f6e\u306b\u5411\u304b\u3046\u304b int from_l; //\u3069\u306e\u7e26\u4f4d\u7f6e\u304b\u3089\u52d5\u304f\u304b bool operator < ( const Amida & right ) const { return from_l < right.from_l; } }; //\u95a2\u6570\u306e\u5834\u5408 static bool LessWeight ( const VEC & l, const VEC & r){ return (l.w == r.w) ? l.v > r.v : r.w < r.w ; } pair\u306e\u30bd\u30fc\u30c8 sort(c.begin(),c.end(),[]( auto & l, auto & r) { if (l.fi == r.fi) return l.se < r.se; else return l.fi > r.fi; });","title":"\u69cb\u9020\u4f53\u306e\u305d\u30fc\u3068"},{"location":"wiki/procon/sort/#_1","text":"struct Amida{ int to_n; //\u3069\u306e\u5217\u306b\u5411\u304b\u3046\u304b int to_l; //\u3069\u306e\u7e26\u4f4d\u7f6e\u306b\u5411\u304b\u3046\u304b int from_l; //\u3069\u306e\u7e26\u4f4d\u7f6e\u304b\u3089\u52d5\u304f\u304b bool operator < ( const Amida & right ) const { return from_l < right.from_l; } }; //\u95a2\u6570\u306e\u5834\u5408 static bool LessWeight ( const VEC & l, const VEC & r){ return (l.w == r.w) ? l.v > r.v : r.w < r.w ; }","title":"\u69cb\u9020\u4f53\u306e\u305d\u30fc\u3068"},{"location":"wiki/procon/sort/#pair","text":"sort(c.begin(),c.end(),[]( auto & l, auto & r) { if (l.fi == r.fi) return l.se < r.se; else return l.fi > r.fi; });","title":"pair\u306e\u30bd\u30fc\u30c8"},{"location":"wiki/python/anaconda/","text":"\u4eee\u60f3\u74b0\u5883\u3092\u4f5c\u308b conda create -n py27 python=2.7 anaconda \u4eee\u60f3\u74b0\u5883\u306e\u78ba\u8a8d conda info -e \u4eee\u60f3\u74b0\u5883\u306e\u524a\u9664 conda remove -n py27 -all \u4eee\u60f3\u74b0\u5883\u3092Active\u306b\u3059\u308b source activate py27 \u81ea\u524d\u306e\u74b0\u5883\u3060\u3068 activate gpy27","title":"Anaconda"},{"location":"wiki/python/anaconda/#_1","text":"conda create -n py27 python=2.7 anaconda","title":"\u4eee\u60f3\u74b0\u5883\u3092\u4f5c\u308b"},{"location":"wiki/python/anaconda/#_2","text":"conda info -e","title":"\u4eee\u60f3\u74b0\u5883\u306e\u78ba\u8a8d"},{"location":"wiki/python/anaconda/#_3","text":"conda remove -n py27 -all","title":"\u4eee\u60f3\u74b0\u5883\u306e\u524a\u9664"},{"location":"wiki/python/anaconda/#active","text":"source activate py27 \u81ea\u524d\u306e\u74b0\u5883\u3060\u3068 activate gpy27","title":"\u4eee\u60f3\u74b0\u5883\u3092Active\u306b\u3059\u308b"},{"location":"wiki/python/env/","text":"python\u306e\u74b0\u5883\u69cb\u7bc9\u95a2\u9023 python \u306e\u5834\u6240\u3092\u8abf\u3079\u308b import sys sys . executable jupyter kernel\u306e\u5834\u6240\u3092\u8abf\u3079\u308b $ jupyter kernelspec list py35_tf /home/user/.local/share/jupyter/kernels/py35_tf cd /home/user/.local/share/jupyter/kernels/py35_tf cat kernel.json /Users/sakai/.pyenv/versions/anaconda3-5.1.0/envs/py3.5/bin/python pandas \u306e0.23.3\u3068numpy\u306e1.15.0\u306f\u885d\u7a81\u3059\u308b\u306e\u3067numpy 1.14.5\u3092\u4f7f\u308f\u306a\u304f\u3066\u306f\u3044\u3051\u306a\u3044","title":"python\u306e\u74b0\u5883\u69cb\u7bc9\u95a2\u9023"},{"location":"wiki/python/env/#python","text":"","title":"python\u306e\u74b0\u5883\u69cb\u7bc9\u95a2\u9023"},{"location":"wiki/python/env/#python_1","text":"import sys sys . executable jupyter kernel\u306e\u5834\u6240\u3092\u8abf\u3079\u308b $ jupyter kernelspec list py35_tf /home/user/.local/share/jupyter/kernels/py35_tf cd /home/user/.local/share/jupyter/kernels/py35_tf cat kernel.json /Users/sakai/.pyenv/versions/anaconda3-5.1.0/envs/py3.5/bin/python pandas \u306e0.23.3\u3068numpy\u306e1.15.0\u306f\u885d\u7a81\u3059\u308b\u306e\u3067numpy 1.14.5\u3092\u4f7f\u308f\u306a\u304f\u3066\u306f\u3044\u3051\u306a\u3044","title":"python \u306e\u5834\u6240\u3092\u8abf\u3079\u308b"},{"location":"wiki/python/pandas_concat/","text":"pandas concatenate \u9023\u7d50 pd . concat([df1,df2]) \u3000\u3000\u3000\u3000 # \u7e26\u65b9\u5411\u306b\u9023\u7d50 pd . concat([df1,df2],axis = 1 ) # \u6a2a\u65b9\u5411\u306b\u9023\u7d50 # \u7279\u5b9a\u30e9\u30d9\u30eb\u306e\u307f\u6b8b\u3057\u3066\u9023\u7d50\u3057\u305f\u3044\u3068\u304d pd . concat([df1,df2],join = 'inner) # \u5171\u901a\u30e9\u30d9\u30eb\u306e\u307f\u6b8b\u3057\u3066\u9023\u7d50 pd . concat([df1,df2],axis = 1 ,join_axes = [df1 . index]) # df1\u306eindex\u306e\u307f\u3067\u9023\u7d50 # index\u306e\u5024\u304c\u91cd\u8907\u3057\u3066\u3057\u307e\u3044\u554f\u984c\u306a\u3068\u304d pd . concat([df1,df2],axis = 1 ,keys = [ 'X' , 'Y' ]) # \u9023\u7d50\u5143\u306e\u30c7\u30fc\u30bf\u3054\u3068\u306b\u30e9\u30d9\u30eb\u3092\u8ffd\u52a0\u4ed8\u4e0e pd . concat([df1,df2],ignore_index = True) # \u30e9\u30d9\u30eb\u306e\u756a\u53f7\u3092\u632f\u308a\u76f4\u3057 # \u30c7\u30fc\u30bf\u306b\u4e00\u884c\u8ffd\u52a0\u3057\u305f\u3044 df1 . append(pd . Series([ 1 , 2 , 3 , 4 , 5 ],indexs),ignore_index = True) :","title":"pandas concatenate \u9023\u7d50"},{"location":"wiki/python/pandas_concat/#pandas-concatenate","text":"pd . concat([df1,df2]) \u3000\u3000\u3000\u3000 # \u7e26\u65b9\u5411\u306b\u9023\u7d50 pd . concat([df1,df2],axis = 1 ) # \u6a2a\u65b9\u5411\u306b\u9023\u7d50 # \u7279\u5b9a\u30e9\u30d9\u30eb\u306e\u307f\u6b8b\u3057\u3066\u9023\u7d50\u3057\u305f\u3044\u3068\u304d pd . concat([df1,df2],join = 'inner) # \u5171\u901a\u30e9\u30d9\u30eb\u306e\u307f\u6b8b\u3057\u3066\u9023\u7d50 pd . concat([df1,df2],axis = 1 ,join_axes = [df1 . index]) # df1\u306eindex\u306e\u307f\u3067\u9023\u7d50 # index\u306e\u5024\u304c\u91cd\u8907\u3057\u3066\u3057\u307e\u3044\u554f\u984c\u306a\u3068\u304d pd . concat([df1,df2],axis = 1 ,keys = [ 'X' , 'Y' ]) # \u9023\u7d50\u5143\u306e\u30c7\u30fc\u30bf\u3054\u3068\u306b\u30e9\u30d9\u30eb\u3092\u8ffd\u52a0\u4ed8\u4e0e pd . concat([df1,df2],ignore_index = True) # \u30e9\u30d9\u30eb\u306e\u756a\u53f7\u3092\u632f\u308a\u76f4\u3057 # \u30c7\u30fc\u30bf\u306b\u4e00\u884c\u8ffd\u52a0\u3057\u305f\u3044 df1 . append(pd . Series([ 1 , 2 , 3 , 4 , 5 ],indexs),ignore_index = True) :","title":"pandas concatenate \u9023\u7d50"},{"location":"wiki/python/pandas_csv/","text":"pandas\u306e\u30c7\u30fc\u30bf\u8aad\u307f\u8fbc\u307f\u65b9\u6cd5 import pandas as pd pd . read_csv( \"URL\" ) pd . read_table( \"URL\" ) # tsv\u30d5\u30a1\u30a4\u30eb\u306e\u8aad\u307f\u8fbc\u307f","title":"Pandas csv"},{"location":"wiki/python/pandas_csv/#pandas","text":"import pandas as pd pd . read_csv( \"URL\" ) pd . read_table( \"URL\" ) # tsv\u30d5\u30a1\u30a4\u30eb\u306e\u8aad\u307f\u8fbc\u307f","title":"pandas\u306e\u30c7\u30fc\u30bf\u8aad\u307f\u8fbc\u307f\u65b9\u6cd5"},{"location":"wiki/python/pandas_split/","text":"pandas \u3067\u306e \u8981\u7d20\u5206\u5272","title":"Pandas split"},{"location":"wiki/python/pandas_split/#pandas","text":"","title":"pandas \u3067\u306e \u8981\u7d20\u5206\u5272"},{"location":"wiki/python/pandas_unique/","text":"pandas\u306e\uff0c\u7a2e\u985e\u6570\u6570\u3048\u4e0a\u3052 df . name . unique()","title":"Pandas unique"},{"location":"wiki/python/pandas_unique/#pandas","text":"df . name . unique()","title":"pandas\u306e\uff0c\u7a2e\u985e\u6570\u6570\u3048\u4e0a\u3052"},{"location":"wiki/python/python_set/","text":"python\u306eset ```python s = set([]) s = set({'dog' : 'inu' , 'cat' : 'neko' , 'bird' : 'tori'}) len(s) #\u8981\u7d20\u6570 s.add(1) #\u8ffd\u52a0 s.remove(2) # \u524a\u9664 s.clear() # \u5168\u524a\u9664 s1 & s2 # \u7a4d\u96c6\u5408 s1 | s2 # \u548c\u96c6\u5408 s1 - s2 # \u5dee\u96c6\u5408 s1.issubset(s2) #s1\u304cs2\u306e\u90e8\u5206\u96c6\u5408\u3067\u3042\u308b\u304b\uff1f","title":"python\u306eset"},{"location":"wiki/python/python_set/#pythonset","text":"```python s = set([]) s = set({'dog' : 'inu' , 'cat' : 'neko' , 'bird' : 'tori'}) len(s) #\u8981\u7d20\u6570 s.add(1) #\u8ffd\u52a0 s.remove(2) # \u524a\u9664 s.clear() # \u5168\u524a\u9664 s1 & s2 # \u7a4d\u96c6\u5408 s1 | s2 # \u548c\u96c6\u5408 s1 - s2 # \u5dee\u96c6\u5408 s1.issubset(s2) #s1\u304cs2\u306e\u90e8\u5206\u96c6\u5408\u3067\u3042\u308b\u304b\uff1f","title":"python\u306eset"},{"location":"wiki/scoop/","text":"windows\u3067\u306e\u30d1\u30c3\u30b1\u30fc\u30b8\u7ba1\u7406\u30b7\u30b9\u30c6\u30e0Scoop \u30d1\u30c3\u30b1\u30fc\u30b8\u30de\u30cd\u30fc\u30b8\u30e3 \u5165\u308c\u305f\u3082\u306e\u306f C:\\Users\\\u540d\u524d\\scoop\\shims \u306b\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3055\u308c\u308b python2\u3068python3\u306e\u5207\u308a\u66ff\u3048\u65b9 scoop search python \u3067\u30d0\u30fc\u30b8\u30e7\u30f3\u78ba\u8a8d scoop install python27 \u3068\u304b\u3067\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb scoop reset python3 \u3068\u304b\u3067\u30d0\u30fc\u30b8\u30e7\u30f3\u5909\u66f4 Scoop\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb Powershell3\u4ee5\u4e0a\u306e\u74b0\u5883\u3067 iex (new-object net.webclient).downloadstring('https://get.scoop.sh') \u30a8\u30e9\u30fc\u304c\u51fa\u308b\u306a\u3089ExecutionPolicy\u3092\u5f31\u3081\u308b Set-ExecutionPolicy RemoteSigned -scope CurrentUser","title":"windows\u3067\u306e\u30d1\u30c3\u30b1\u30fc\u30b8\u7ba1\u7406\u30b7\u30b9\u30c6\u30e0Scoop"},{"location":"wiki/scoop/#windowsscoop","text":"\u30d1\u30c3\u30b1\u30fc\u30b8\u30de\u30cd\u30fc\u30b8\u30e3 \u5165\u308c\u305f\u3082\u306e\u306f C:\\Users\\\u540d\u524d\\scoop\\shims \u306b\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3055\u308c\u308b python2\u3068python3\u306e\u5207\u308a\u66ff\u3048\u65b9 scoop search python \u3067\u30d0\u30fc\u30b8\u30e7\u30f3\u78ba\u8a8d scoop install python27 \u3068\u304b\u3067\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb scoop reset python3 \u3068\u304b\u3067\u30d0\u30fc\u30b8\u30e7\u30f3\u5909\u66f4","title":"windows\u3067\u306e\u30d1\u30c3\u30b1\u30fc\u30b8\u7ba1\u7406\u30b7\u30b9\u30c6\u30e0Scoop"},{"location":"wiki/scoop/#scoop","text":"Powershell3\u4ee5\u4e0a\u306e\u74b0\u5883\u3067 iex (new-object net.webclient).downloadstring('https://get.scoop.sh') \u30a8\u30e9\u30fc\u304c\u51fa\u308b\u306a\u3089ExecutionPolicy\u3092\u5f31\u3081\u308b Set-ExecutionPolicy RemoteSigned -scope CurrentUser","title":"Scoop\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb"},{"location":"wiki/term/\u30b9\u30bf\u30c3\u30af/","text":"\u30b9\u30bf\u30c3\u30af\u306b\u3064\u3044\u3066 \u30b9\u30bf\u30c3\u30af\u306f\u7a4d\u307f\u91cd\u306d\u308b\u3068\u3044\u3046\u610f\u5473 \u30b9\u30bf\u30c3\u30af\u30e1\u30e2\u30ea \u30bf\u30b9\u30af\u3084\u95a2\u6570\u5185\u3060\u3051\u3067\u4f7f\u308f\u308c\u308b\u5909\u6570\u3084\u30a2\u30c9\u30ec\u30b9\u60c5\u5831\u306a\u3069\u3092\u7f6e\u3044\u3066\u304a\u304f\u30e1\u30e2\u30ea\u9818\u57df \u30bf\u30b9\u30af\u3092\u7d42\u4e86\u3059\u308b\uff0c\u307e\u305f\u306f\u95a2\u6570\u304b\u3089\u629c\u3051\u308b\u3068\uff0c\u30b9\u30bf\u30c3\u30af\u306e\u5185\u5bb9\u3082\u7834\u68c4\u3055\u308c\u308b \u30b0\u30ed\u30fc\u30d0\u30eb\u5909\u6570\u306f\uff0c\u4ed6\u306e\u30bf\u30b9\u30af\u3084\u95a2\u6570\u304b\u3089\u30a2\u30af\u30bb\u30b9\u3067\u304d\u308b\u3088\u3046\u306b\u30b9\u30bf\u30c3\u30af\u30e1\u30e2\u30ea\u3068\u306f\u5225\u306e\u9818\u57df\u306b\u914d\u7f6e\u3055\u308c\u308b\uff0e \u30b9\u30bf\u30c3\u30af\u30dd\u30a4\u30f3\u30bf\u30ec\u30b8\u30b9\u30bf(SP\u30ec\u30b8\u30b9\u30bf) \u30d5\u30ec\u30fc\u30e0\u30dd\u30a4\u30f3\u30bf\u30ec\u30b8\u30b9\u30bf(FP\u30ec\u30b8\u30b9\u30bf)","title":"\u30b9\u30bf\u30c3\u30af\u306b\u3064\u3044\u3066"},{"location":"wiki/term/\u30b9\u30bf\u30c3\u30af/#_1","text":"\u30b9\u30bf\u30c3\u30af\u306f\u7a4d\u307f\u91cd\u306d\u308b\u3068\u3044\u3046\u610f\u5473","title":"\u30b9\u30bf\u30c3\u30af\u306b\u3064\u3044\u3066"},{"location":"wiki/term/\u30b9\u30bf\u30c3\u30af/#_2","text":"\u30bf\u30b9\u30af\u3084\u95a2\u6570\u5185\u3060\u3051\u3067\u4f7f\u308f\u308c\u308b\u5909\u6570\u3084\u30a2\u30c9\u30ec\u30b9\u60c5\u5831\u306a\u3069\u3092\u7f6e\u3044\u3066\u304a\u304f\u30e1\u30e2\u30ea\u9818\u57df \u30bf\u30b9\u30af\u3092\u7d42\u4e86\u3059\u308b\uff0c\u307e\u305f\u306f\u95a2\u6570\u304b\u3089\u629c\u3051\u308b\u3068\uff0c\u30b9\u30bf\u30c3\u30af\u306e\u5185\u5bb9\u3082\u7834\u68c4\u3055\u308c\u308b \u30b0\u30ed\u30fc\u30d0\u30eb\u5909\u6570\u306f\uff0c\u4ed6\u306e\u30bf\u30b9\u30af\u3084\u95a2\u6570\u304b\u3089\u30a2\u30af\u30bb\u30b9\u3067\u304d\u308b\u3088\u3046\u306b\u30b9\u30bf\u30c3\u30af\u30e1\u30e2\u30ea\u3068\u306f\u5225\u306e\u9818\u57df\u306b\u914d\u7f6e\u3055\u308c\u308b\uff0e","title":"\u30b9\u30bf\u30c3\u30af\u30e1\u30e2\u30ea"},{"location":"wiki/term/\u30b9\u30bf\u30c3\u30af/#sp","text":"","title":"\u30b9\u30bf\u30c3\u30af\u30dd\u30a4\u30f3\u30bf\u30ec\u30b8\u30b9\u30bf(SP\u30ec\u30b8\u30b9\u30bf)"},{"location":"wiki/term/\u30b9\u30bf\u30c3\u30af/#fp","text":"","title":"\u30d5\u30ec\u30fc\u30e0\u30dd\u30a4\u30f3\u30bf\u30ec\u30b8\u30b9\u30bf(FP\u30ec\u30b8\u30b9\u30bf)"},{"location":"wiki/term/\u30e1\u30e2\u30ea/","text":"\u30e1\u30e2\u30ea\u306b\u3064\u3044\u3066 \u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u7684\u306a\u8a71 How computer memory works \u4efb\u610f\u306e(random\u306b)\u30a2\u30af\u30bb\u30b9\u3067\u304d\u308b\u30e1\u30e2\u30ea random access memory (RAM) \u63ee\u767a\u6027\u30e1\u30e2\u30ea DRAM : \u30b3\u30f3\u30c7\u30f3\u30b5\u306e\u5145\u96fb\u653e\u96fb\u3092\u884c\u3044\u8a18\u9332\u3059\u308b \u3053\u3063\u3061\u306e\u304c\u9045\u3044 SRAM \uff1a\u9ad8\u901f\u306a\u5185\u90e8\u30e1\u30e2\u30ea \u30ea\u30d5\u30ec\u30c3\u30b7\u30e5\u306e\u5fc5\u8981\u304c\u306a\u3044 \u9ad8\u4fa1\uff06\u3067\u304b\u3044 \u4e0d\u63ee\u767a\u6027\u30e1\u30e2\u30ea \u30e1\u30e2\u30ea\u306e\u9818\u57df text\u9818\u57df \u30d7\u30ed\u30b0\u30e9\u30e0\u3092\u7f6e\u304f data\u9818\u57df \u521d\u671f\u5316\u3055\u308c\u305f\u30b0\u30ed\u30fc\u30d0\u30eb\u5909\u6570\u306a\u3069\u3092\u7f6e\u304f bss\u9818\u57df \u521d\u671f\u5316\u3055\u308c\u3066\u306a\u3044\u30b0\u30ed\u30fc\u30d0\u30eb\u5909\u6570\u306a\u3069\u3092\u7f6e\u304f stack\u9818\u57df \u95a2\u6570\u306e\u5f15\u6570\u3084\u30ed\u30fc\u30ab\u30eb\u5909\u6570\u3092\u7f6e\u304f heap\u9818\u57df \u30d7\u30ed\u30b0\u30e9\u30e0\u306e\u30c7\u30fc\u30bf\u3092\u7f6e\u304f(\u52d5\u7684\u306b\u78ba\u4fdd\u3057\u305f\u3082\u306e) Stack\u3068\u95a2\u6570 Stack\u306f\u95a2\u6570\u547c\u3073\u51fa\u3057\u306e\u305f\u3081\u306b\u4f7f\u308f\u308c\u308b heap\u9818\u57df \u30d2\u30fc\u30d7\u306f\u4efb\u610f\u306e\u30b5\u30a4\u30ba\u306e\u30c7\u30fc\u30bf\u3092\u4efb\u610f\u306e\u9806\u756a\u306b\u78ba\u4fdd\u30fb\u89e3\u653e\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u308b new\u3057\u305f\u3068\u304d\u306f\u3053\u3061\u3089\u306e\u65b9\u6cd5\u3067\u78ba\u4fdd\u3055\u308c\u308b\uff0e vector\u306e\u914d\u5217\u3082\u3053\u3053\u306b\u5165\u308b StackOverflow\u6642\u306e\u30a8\u30e9\u30fc python Fatal Python error: Cannot recover from stack overflow. Current thread 0x00007f434f650700 (most recent call first): File \"***.py\", line 4 in ... File \"***.py\", line 7 in ... File \"***.py\", line 7 in ...","title":"\u30e1\u30e2\u30ea\u306b\u3064\u3044\u3066"},{"location":"wiki/term/\u30e1\u30e2\u30ea/#_1","text":"","title":"\u30e1\u30e2\u30ea\u306b\u3064\u3044\u3066"},{"location":"wiki/term/\u30e1\u30e2\u30ea/#_2","text":"How computer memory works \u4efb\u610f\u306e(random\u306b)\u30a2\u30af\u30bb\u30b9\u3067\u304d\u308b\u30e1\u30e2\u30ea random access memory (RAM)","title":"\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u7684\u306a\u8a71"},{"location":"wiki/term/\u30e1\u30e2\u30ea/#_3","text":"DRAM : \u30b3\u30f3\u30c7\u30f3\u30b5\u306e\u5145\u96fb\u653e\u96fb\u3092\u884c\u3044\u8a18\u9332\u3059\u308b \u3053\u3063\u3061\u306e\u304c\u9045\u3044 SRAM \uff1a\u9ad8\u901f\u306a\u5185\u90e8\u30e1\u30e2\u30ea \u30ea\u30d5\u30ec\u30c3\u30b7\u30e5\u306e\u5fc5\u8981\u304c\u306a\u3044 \u9ad8\u4fa1\uff06\u3067\u304b\u3044","title":"\u63ee\u767a\u6027\u30e1\u30e2\u30ea"},{"location":"wiki/term/\u30e1\u30e2\u30ea/#_4","text":"","title":"\u4e0d\u63ee\u767a\u6027\u30e1\u30e2\u30ea"},{"location":"wiki/term/\u30e1\u30e2\u30ea/#_5","text":"text\u9818\u57df \u30d7\u30ed\u30b0\u30e9\u30e0\u3092\u7f6e\u304f data\u9818\u57df \u521d\u671f\u5316\u3055\u308c\u305f\u30b0\u30ed\u30fc\u30d0\u30eb\u5909\u6570\u306a\u3069\u3092\u7f6e\u304f bss\u9818\u57df \u521d\u671f\u5316\u3055\u308c\u3066\u306a\u3044\u30b0\u30ed\u30fc\u30d0\u30eb\u5909\u6570\u306a\u3069\u3092\u7f6e\u304f stack\u9818\u57df \u95a2\u6570\u306e\u5f15\u6570\u3084\u30ed\u30fc\u30ab\u30eb\u5909\u6570\u3092\u7f6e\u304f heap\u9818\u57df \u30d7\u30ed\u30b0\u30e9\u30e0\u306e\u30c7\u30fc\u30bf\u3092\u7f6e\u304f(\u52d5\u7684\u306b\u78ba\u4fdd\u3057\u305f\u3082\u306e)","title":"\u30e1\u30e2\u30ea\u306e\u9818\u57df"},{"location":"wiki/term/\u30e1\u30e2\u30ea/#stack","text":"Stack\u306f\u95a2\u6570\u547c\u3073\u51fa\u3057\u306e\u305f\u3081\u306b\u4f7f\u308f\u308c\u308b","title":"Stack\u3068\u95a2\u6570"},{"location":"wiki/term/\u30e1\u30e2\u30ea/#heap","text":"\u30d2\u30fc\u30d7\u306f\u4efb\u610f\u306e\u30b5\u30a4\u30ba\u306e\u30c7\u30fc\u30bf\u3092\u4efb\u610f\u306e\u9806\u756a\u306b\u78ba\u4fdd\u30fb\u89e3\u653e\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u308b new\u3057\u305f\u3068\u304d\u306f\u3053\u3061\u3089\u306e\u65b9\u6cd5\u3067\u78ba\u4fdd\u3055\u308c\u308b\uff0e vector\u306e\u914d\u5217\u3082\u3053\u3053\u306b\u5165\u308b","title":"heap\u9818\u57df"},{"location":"wiki/term/\u30e1\u30e2\u30ea/#stackoverflow","text":"","title":"StackOverflow\u6642\u306e\u30a8\u30e9\u30fc"},{"location":"wiki/term/\u30e1\u30e2\u30ea/#python","text":"Fatal Python error: Cannot recover from stack overflow. Current thread 0x00007f434f650700 (most recent call first): File \"***.py\", line 4 in ... File \"***.py\", line 7 in ... File \"***.py\", line 7 in ...","title":"python"}]}